{
  "id": "lecture_01",
  "title": "Advanced DBMS and Database System Architectures",
  "text": "This lecture introduces advanced database management systems and focuses on database system architectures. A database system architecture refers to the overall design and structure of a database system, describing how data is stored, managed, accessed, and how different components of the system interact with each other. A well-designed architecture ensures key data properties such as integrity, efficiency, security, and scalability.\n\nThe lecture explains different types of database architectures. Tiered architectures describe how responsibilities are divided between system layers, including one-tier systems where the database and application reside on the same machine, two-tier systems where clients communicate directly with a database server, and three-tier systems where presentation, application logic, and data storage are separated. Physical database architectures are also discussed, including centralized architectures where all data is stored on a single machine, client-server architectures where multiple clients connect to a central server, distributed database systems where data is stored across multiple physical locations, and parallel database systems that use multiple processors and disks to improve performance.\n\nThe concept of a database schema is introduced as the blueprint that defines how data is organized in a database. A schema specifies elements such as tables, fields, data types, and relationships, but does not include the actual data itself. The lecture presents the three-schema architecture, which separates the database into external, conceptual, and internal schemas. The external schema represents user-specific views of the data, the conceptual schema defines the logical structure of the entire database including tables, relationships, and integrity constraints, and the internal schema describes the physical storage structures such as indexes, partitioning strategies, and storage formats. This separation improves data abstraction, data independence, and system flexibility.\n\nThe lecture defines a Database Management System (DBMS) as software that allows applications to store, retrieve, update, and analyze information in a database according to a defined data model. A general-purpose DBMS supports core operations such as database definition, creation, querying, updating, and administration. The internal components of a DBMS are discussed in detail. The database engine is responsible for data processing and manipulation. The database schema component defines the logical structure of the database. The query processor interprets and executes user queries by translating high-level SQL statements into efficient execution plans. The storage manager manages physical storage, including buffer management, file organization, access methods, and index management. The transaction manager ensures data integrity and consistency by managing transactions, enforcing concurrency control, recovery mechanisms, and ACID properties.\n\nQuery processing is presented as a multi-stage process that transforms high-level SQL queries into executable operations. This process includes parsing and lexical analysis to validate syntax, generating an abstract syntax tree, query optimization to minimize execution cost in terms of I/O, CPU, and memory usage, creating an optimized query plan, and executing the plan using physical operators within the execution engine.\n\nThe lecture then examines the historical evolution of database system architectures. Early file-based systems in the 1960s and 1970s relied on flat files, where each application maintained its own data, leading to data redundancy, inconsistency, and lack of standardized access methods. Hierarchical and network data models emerged in the 1970s and 1980s, such as IBM's Information Management System (IMS) and CODASYL network models, which provided better data organization but required manual navigation of complex pointer structures by programmers.\n\nThe relational model emerged in the 1980s and 1990s, introducing a mathematical foundation based on set theory and first-order predicate logic. This model allowed users to focus on what data they wanted rather than how to retrieve it, leading to the development of SQL and modern relational database systems such as Oracle, DB2, and SQL Server. The relational model emphasized data abstraction, storing data in simple structures, accessing data through declarative languages, and leaving physical storage decisions to the DBMS implementation.\n\nAs applications became more complex in the 2000s, object-oriented and object-relational database systems were developed to support complex data types. PostgreSQL is highlighted as an example that supports traditional relational operations along with complex types such as arrays, JSON, and user-defined data types. During the same period, the internet boom exposed limitations of traditional databases, leading to the development of custom middleware and new architectural approaches.\n\nThe lecture discusses the rise of data warehouses and OLAP systems optimized for analytical workloads, typically using distributed, shared-nothing architectures and columnar storage models. NoSQL systems emerged to support high availability and scalability, using schema-less designs, non-relational data models such as key-value, document, and graph models, and often relaxing ACID guarantees. NewSQL systems later combined the scalability of NoSQL with the transactional guarantees of traditional relational systems.\n\nHybrid systems are introduced as systems capable of handling both transactional and analytical workloads, commonly referred to as Hybrid Transactional and Analytical Processing (HTAP) systems. Cloud database systems are discussed as databases designed specifically for cloud environments, often offered as database-as-a-service solutions. The lecture also covers shared-disk engines, stream processing systems, graph databases, time-series databases, and modern lakehouse architectures that combine features of data lakes and data warehouses.\n\nThe lecture introduces data models as collections of concepts used to describe data in a database. Common data models include the relational model, key-value model, document model, graph model, wide-column model, and vector data model. The document data model is explained using JSON as a modern representation for hierarchical data. The vector data model is highlighted as a modern model used in machine learning applications, where high-dimensional vectors represent semantic meaning and nearest-neighbor search techniques are used for efficient retrieval.\n\nFinally, the lecture compares Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) workloads. OLTP systems handle real-time business transactions requiring fast response times, high concurrency, and strict consistency, typically using row-oriented storage and B-tree indexes. OLAP systems support complex analytical queries over large historical datasets, using columnar storage, bitmap indexes, materialized views, and parallel processing. The lecture concludes by discussing hybrid approaches that integrate OLTP and OLAP capabilities within the same system or through data replication strategies."
}
