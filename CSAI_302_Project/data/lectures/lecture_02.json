{
  "id": "lecture_02",
  "title": "Storage Management",
  "text": "This lecture focuses on storage management in database management systems, explaining how data is stored, accessed, and managed efficiently across different levels of the storage hierarchy. The storage hierarchy is introduced, ranging from CPU registers and caches, to main memory (DRAM), solid-state drives, hard disk drives, and tertiary or network storage. These storage layers differ in speed, capacity, cost, volatility, and access patterns, with faster storage being smaller and more expensive, and slower storage being larger and cheaper.\n\nThe lecture explains disk-based database architectures, where the DBMS assumes that the primary storage location of the database resides on non-volatile disk. The DBMS is responsible for managing the movement of data between disk and main memory. This functionality exists at the lowest layer of the DBMS, hiding physical storage details from higher system layers. The primary goals include mapping logical pages to disk locations, loading pages from disk into memory, and writing modified pages back to disk while ensuring data durability.\n\nKey storage management terminologies are introduced, including blocks, which are the unit of transfer for disk I/O operations, and pages, which are block-sized chunks of memory used by the DBMS. Databases are stored as files on disk, where each file contains multiple pages and each page contains multiple records. Pages are managed on disk by the disk space manager and in memory by the buffer manager, while higher DBMS layers operate exclusively on in-memory pages.\n\nThe lecture discusses system design goals related to storage, emphasizing that databases often exceed available main memory and that disk I/O operations are expensive. Random disk access is significantly slower than sequential access, making it important for DBMSs to optimize access patterns to maximize sequential reads and writes. This leads to careful design of storage layouts and buffer management strategies.\n\nThe storage manager is described as the DBMS component responsible for maintaining database files and organizing them as collections of pages. It tracks page reads and writes, manages available disk space, and typically avoids maintaining multiple copies of the same page on disk. Database pages are fixed-size blocks that may contain tuples, metadata, indexes, or log records. Most systems avoid mixing page types, and each page is assigned a unique page identifier that is mapped to its physical location using an indirection layer.\n\nDifferent notions of pages are explained, including hardware pages, operating system pages, and database pages, each with potentially different sizes. Database page sizes commonly range from 4KB to 16KB, though some systems use larger pages for read-heavy workloads. The lecture introduces page storage architectures, including heap file organization, tree-based organization, sorted or sequential file organization, and hashing-based file organization.\n\nHeap file organization is examined in detail. Heap files store records in no particular order across pages and require metadata to track free space and page locations. A heap file typically contains a special header page that maintains pointers to data pages and free space. Page directories are used to efficiently locate pages with available space and to track metadata such as free bytes, page types, and empty pages. This structure allows the DBMS to efficiently insert, delete, and locate records.\n\nThe lecture then explains page layout and how data is organized within a page. Each page contains a header with metadata such as page size, checksums, transaction visibility information, compression metadata, schema information, and data summaries. Many systems require pages to be self-contained, meaning that both data and its associated metadata reside on the same physical storage unit.\n\nTuple-oriented storage is discussed as a common row-oriented storage model. Tuples are sequences of bytes interpreted by the DBMS according to schema definitions. Each tuple includes a header containing metadata such as visibility information and null-value indicators. For handling variable-length attributes and tuple deletions efficiently, the lecture introduces slotted page layouts. In this approach, a slot array maps logical slot numbers to tuple offsets within the page, allowing tuples to move without changing external references.\n\nRecord identifiers are introduced as unique identifiers assigned by the DBMS to each tuple, typically consisting of a file identifier, page identifier, and slot number. These identifiers represent physical locations and should not be relied upon by applications as meaningful values. Different DBMSs implement record identifiers differently, such as ROWID or CTID.\n\nThe lecture also discusses tuple layout and physical denormalization. While tuples are typically stored in the order defined by the table schema, DBMSs may physically denormalize data by pre-joining related tuples and storing them together to reduce I/O for common workloads. Although this can improve read performance, it may increase the cost of updates. Physical denormalization is not a new concept and has been used in both relational and NoSQL systems.\n\nBeyond data pages, DBMSs require additional memory pools for operations such as sorting, joins, query caching, logging, and metadata storage. The buffer pool is introduced as a central memory structure that caches disk pages in memory. It is organized as an array of fixed-size frames, each holding a copy of a disk page. The buffer pool uses a write-back caching strategy, meaning that modified pages are not immediately written to disk.\n\nThe buffer pool maintains metadata using a page table, which tracks pages currently in memory, and additional metadata such as dirty flags, pin counts, and access history. The lecture distinguishes between the page directory, which maps page identifiers to disk locations and is stored persistently, and the page table, which maps page identifiers to in-memory frames and exists only in memory.\n\nThe process of handling page requests is explained in detail. When a requested page is not in the buffer pool, the DBMS selects an unpinned frame for replacement, writes the page back to disk if it is dirty, loads the requested page into memory, pins it, and returns its address. Prefetching techniques may be used when access patterns are predictable, such as during sequential scans. When the buffer pool is full, the DBMS must evict pages using a replacement policy.\n\nSeveral buffer replacement policies are discussed. Least Recently Used (LRU) replaces the page that has not been accessed for the longest time and works well for workloads with temporal locality but performs poorly for repeated sequential scans. Improved policies such as LRU-K track the history of the last K accesses to better balance recency and frequency. Approximate implementations, such as MySQLâ€™s two-list approach with old and young regions, reduce overhead while approximating LRU-K behavior.\n\nAdvanced buffer management techniques are also presented. Localization strategies assign a subset of buffer pool pages to individual queries to prevent cache pollution. Priority hints allow the DBMS to provide contextual information about page importance during query execution. Dirty page handling strategies distinguish between fast eviction of clean pages and slow eviction requiring disk writes. Background writing mechanisms periodically flush dirty pages to disk while ensuring correctness with respect to logging and recovery protocols.\n\nFinally, the lecture introduces buffer pool optimizations, including the use of multiple buffer pools to reduce contention, prefetching to improve I/O performance, scan sharing to reuse data across queries, and buffer pool bypass techniques that avoid polluting the cache with pages unlikely to be reused."
}
