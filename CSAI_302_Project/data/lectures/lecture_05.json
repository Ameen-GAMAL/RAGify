{
  "id": "lecture_05",
  "title": "Query Processing and Optimization (Part 1)",
  "text": "This lecture introduces the fundamentals of query processing and query optimization in database management systems. Query execution is described as the process by which a DBMS evaluates a SQL query by translating it into a query plan composed of operators arranged as a directed acyclic graph. Each operator consumes tuples from its child operators and produces output tuples for its parent. Query plans can be decomposed into pipelines, which are sequences of operators where tuples flow continuously without intermediate storage. Certain operators act as pipeline breakers because they cannot produce output until all input tuples have been consumed, such as the build phase of joins, subqueries, and ORDER BY operations.\n\nThe lecture introduces the concept of a processing model, which defines how a DBMS executes a query plan and how data is transferred between operators. Processing models represent trade-offs between OLTP and OLAP workloads. Each processing model defines both control flow, which determines how operators are invoked, and data flow, which determines how results are passed between operators. Operator output may consist of complete tuples, as in row-oriented storage models, or subsets of columns, as in column-oriented storage models.\n\nThree primary processing models are discussed: the iterator model, the vectorized or batch model, and the materialization model. The iterator model, also known as the Volcano or pipeline model, is the most widely used execution model in DBMSs. In this model, each operator implements Open, Next, and Close functions. On each invocation of Next, an operator returns either a single tuple or an end-of-file marker. Operators repeatedly call Next on their children to retrieve tuples, process them, and emit results. This model enables pipelining, where each tuple is processed through as many operators as possible before the next tuple is retrieved. The iterator model is easy to implement and debug and provides fine-grained output control, making it suitable for a wide range of workloads.\n\nThe vectorized or batch processing model extends the iterator model by having operators emit batches of tuples rather than single tuples. Each operator processes multiple tuples at a time using tight loops over arrays of values, often organized by column with associated null bitmaps. The size of each batch may vary depending on hardware characteristics or query properties. Vectorized execution significantly reduces the number of function calls per operator and allows modern CPUs to exploit instruction-level parallelism and vectorization. This model is particularly well-suited for OLAP workloads because it improves cache utilization, reduces control overhead, and keeps the instruction cache hot. Operator fusion techniques may be used to combine multiple operators into a single loop, further reducing intermediate materialization.\n\nThe materialization model is described as an alternative execution approach in which each operator processes all of its input tuples at once and produces its entire output before passing it to the next operator. In this model, intermediate results are fully materialized, and the DBMS may push down hints such as LIMIT clauses to avoid unnecessary processing. Materialization can emit either full tuples or single columns. This model is more suitable for OLTP workloads where queries access only a small number of tuples, resulting in lower coordination overhead and fewer function calls. However, it is not ideal for OLAP workloads because large intermediate results require substantial memory allocation.\n\nThe lecture then discusses plan processing direction. In top-to-bottom or pull-based execution, processing begins at the root of the query plan, and parent operators pull tuples from their children using function calls. In bottom-to-top or push-based execution, leaf operators push tuples upward to their parents. Push-based execution allows operators to be fused within a single loop, reducing intermediate staging and improving performance.\n\nAccess methods are introduced as the mechanisms used by the DBMS to retrieve data from tables. Access methods are not defined in relational algebra and represent physical execution choices. Three main access methods are discussed: sequential scans, index scans, and multi-index scans. A sequential scan iterates over every page in a table, retrieves each page from the buffer pool, and examines every tuple to determine whether it satisfies the query predicate. The DBMS maintains an internal cursor to track progress through the table.\n\nSeveral optimizations for sequential scans are presented, including data encoding and compression, prefetching, scan sharing, buffer bypass, clustering and sorting, late materialization, result caching, and parallelization. Data skipping techniques are discussed in detail. Approximate query processing executes queries over sampled data to produce approximate results. Zone maps provide a lossless optimization by storing precomputed minimum and maximum values for columns within each page. The DBMS consults these zone maps to determine whether a page can be skipped entirely based on query predicates, trading off page size against filtering effectiveness.\n\nIndex scans allow the DBMS to retrieve only the tuples that satisfy query predicates by traversing an index. The choice of index depends on the attributes referenced in the query, the value distribution of those attributes, predicate composition, and whether the index is unique or non-unique. Examples illustrate how different index choices can lead to drastically different performance outcomes depending on selectivity.\n\nThe lecture introduces multi-index scans as a technique that allows the DBMS to use multiple indexes for a single query. Instead of selecting only one index, the DBMS retrieves sets of record identifiers from each applicable index and then combines these sets using union or intersection operations, depending on the query predicates. The DBMS then retrieves the corresponding records and applies any remaining predicates. Multi-index scan implementations include bitmap scans in PostgreSQL, index merge in MySQL, and similar techniques in other systems. Set intersection can be efficiently implemented using bitmaps or hash tables.\n\nExpression evaluation is presented as a critical component of query execution. The DBMS represents WHERE clauses as expression trees, where nodes correspond to comparison operators, logical operators, arithmetic operators, constants, attribute references, and function calls. Evaluating predicates by traversing expression trees is inefficient for modern CPUs due to poor instruction locality and branch mispredictions. Instead, DBMSs aim to evaluate expressions directly or in a vectorized manner over batches of tuples.\n\nSeveral expression evaluation optimizations are discussed. Constant folding identifies sub-expressions involving only constants and computes them once rather than repeatedly for each tuple. Common sub-expression elimination detects repeated expressions within an expression tree and computes them once, reusing the result across multiple evaluations. These optimizations reduce redundant computation and improve CPU efficiency.\n\nThe lecture then introduces the query optimization architecture used in DBMSs. Query optimization begins with parsing the SQL query to produce a syntax tree. The binder resolves names and maps them to internal identifiers using catalog information. The optimizer generates a logical query plan and explores alternative physical execution plans using available access paths and operators. A cost model estimates the resource consumption of each plan based on statistics and system parameters. The optimizer selects the physical plan with the lowest estimated cost.\n\nThe distinction between logical and physical plans is emphasized. Logical plans describe what operations should be performed using relational algebra, while physical plans specify how those operations are executed, including access methods, join algorithms, and data layouts. There is not always a one-to-one mapping between logical and physical operators, as physical operators may depend on sorting, compression, or storage format.\n\nQuery optimization is described as an NP-hard problem due to the large search space of possible join orders and execution strategies. In practice, DBMSs restrict the search space and rely on heuristics and cost-based techniques. Heuristic optimization rewrites queries to remove obvious inefficiencies, such as pushing selections and projections down the query tree to reduce intermediate result sizes. These techniques may consult catalog metadata but do not require examining actual data.\n\nLogical plan optimization uses equivalence rules from relational algebra to transform a logical plan into an equivalent but potentially more efficient form. These transformations aim to increase the likelihood that the optimizer will consider efficient plans during physical plan enumeration. A heuristic algebraic optimization algorithm is outlined, including breaking conjunctive selections into cascades, pushing selections down the tree, reordering joins so that the most restrictive operations are executed first, replacing Cartesian products followed by selections with join operations, and pushing projections down the tree to reduce attribute width.\n\nThe lecture concludes with examples of heuristic query optimization, demonstrating how SQL queries are translated into relational algebra expressions, transformed into initial query trees, and then optimized using heuristic rules to produce execution plans that minimize intermediate results and improve performance."
}
