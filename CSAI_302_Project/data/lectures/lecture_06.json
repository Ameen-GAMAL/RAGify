{
  "id": "lecture_06",
  "title": "Query Processing and Optimization (Part 2)",
  "text": "This lecture continues the study of query processing and optimization by focusing on cost-based query optimization, statistics, cardinality estimation, and adaptive query optimization techniques. Cost-based query optimization is introduced as the process by which a DBMS enumerates multiple logically equivalent execution plans for a query and uses a cost model to estimate the cost of executing each plan. The optimizer then selects the plan with the lowest estimated cost, rather than executing every possible plan, which would be prohibitively expensive.\n\nCost-based query optimization typically follows a bottom-up approach. The optimizer enumerates alternative plans for a query and estimates their costs until either all plans are explored or a timeout is reached. This approach applies to queries involving a single relation, multiple relations, and nested subqueries. For single-relation queries, the optimizer focuses on selecting the best access method, such as a sequential scan, binary search, or index scan, and determining the optimal predicate evaluation order. In practice, simple heuristics are often sufficient for optimizing single-table queries.\n\nFor multi-relation queries, the lecture presents two primary planning strategies: generative (bottom-up) and transformation-based (top-down) optimization. In bottom-up optimization, the optimizer starts with individual relations and iteratively assembles them into larger subplans, using static rules followed by dynamic programming to determine the optimal join order. This divide-and-conquer approach is used by systems such as IBM System R, DB2, MySQL, PostgreSQL, and many open-source DBMSs. In contrast, top-down optimization begins with a logical representation of the desired query result and explores alternative plans using a branch-and-bound search, converting logical operators into physical operators while tracking the best plan found so far. This approach treats physical properties of data, such as ordering and partitioning, as first-class entities and is used by systems such as Microsoft SQL Server, Greenplum, and CockroachDB.\n\nThe System R optimizer is discussed in detail as a representative bottom-up optimizer. The query is first decomposed into blocks, and logical operators are generated for each block. For every logical operator, the optimizer generates all possible physical implementations, including combinations of join algorithms and access paths. The optimizer then iteratively constructs left-deep join trees that minimize the estimated execution cost. Bushy join trees are typically avoided due to their higher complexity. A detailed example illustrates how System R chooses access paths for individual tables, enumerates all possible join orderings, and selects the join ordering with the lowest estimated cost.\n\nTop-down optimization is further explained through rule-based plan transformation. Starting from a logical plan, the optimizer applies transformation rules to generate new nodes in the plan tree. These rules include logical-to-logical transformations, such as reordering join operands, and logical-to-physical transformations, such as replacing a logical join with a hash join or merge join. The optimizer may also introduce enforcer operators to ensure that required physical properties, such as sorted input, are satisfied.\n\nThe lecture compares hash joins and merge joins in detail. Hash joins build a hash table on one input relation and probe it with tuples from the other relation, making them well-suited for large, unsorted equi-joins when sufficient memory is available. Merge joins require both inputs to be sorted on the join key and then merge them in a single pass, making them efficient for range joins or when inputs are already ordered. Hash joins can suffer from high memory usage and performance degradation if the hash table spills to disk, while merge joins incur sorting costs if inputs are not already sorted.\n\nNested subqueries are treated by the DBMS as parameterized functions that return either a single value or a set of values. The optimizer may rewrite nested subqueries to decorrelate or flatten them, converting them into equivalent join-based queries. Alternatively, the optimizer may decompose a query into blocks, materializing the results of inner subqueries into temporary tables that are discarded after query execution. Several examples demonstrate how correlated subqueries are rewritten and decomposed into inner and outer blocks.\n\nExpression rewriting is introduced as an important optimization step. The optimizer transforms query predicates into a minimal and more efficient set of expressions using rule-based or pattern-matching techniques. This process includes eliminating impossible or unnecessary predicates, such as conditions that always evaluate to false, and merging overlapping predicates, such as combining multiple range conditions into a single predicate.\n\nThe lecture then focuses on cost estimation, which allows the DBMS to predict the execution behavior of a query plan given the current database state. Cost models estimate both physical costs, such as CPU cycles, disk I/O, cache misses, memory usage, and network communication, and logical costs, such as the estimated output cardinality of each operator. Accurate cost estimation is essential for comparing alternative plans, but it is challenging because it relies on approximations rather than actual execution.\n\nTo support cost estimation, the DBMS maintains internal statistics about tables, attributes, and indexes in its system catalog. These statistics are updated periodically or through manual commands such as ANALYZE or RUNSTATS, depending on the DBMS. Selection cardinality estimation is explained using the concept of selectivity, defined as the fraction of tuples that satisfy a predicate. For equality predicates, selectivity is computed as the number of occurrences of a value divided by the total number of tuples. These estimations often assume uniform data distribution, predicate independence, and the inclusion principle for joins.\n\nThe lecture explains how histograms are used to improve selectivity estimates when data is not uniformly distributed. Equi-width histograms divide the attribute domain into buckets of equal width and store the count of values in each bucket. Equi-depth histograms, also known as quantile histograms, vary bucket widths so that each bucket contains approximately the same number of values. These histograms allow the optimizer to make more accurate estimates for range predicates and skewed data distributions.\n\nIn addition to histograms, the lecture introduces sketches and sampling as alternative statistical techniques. Sketches are probabilistic data structures that provide approximate statistics, such as frequency counts and distinct value estimates, with low memory overhead. Examples include Count-Min Sketch for frequency estimation and HyperLogLog for estimating the number of distinct values. Sampling techniques maintain a representative subset of table data and use it to estimate predicate selectivities, updating samples when underlying data changes significantly.\n\nThe lecture concludes with a discussion of adaptive query optimization. Traditional optimizers generate a single execution plan before query execution, but the optimal plan may change over time due to data modifications, physical design changes, parameter values, or outdated statistics. Bad query plans often result from inaccurate cardinality estimates that propagate errors up the execution plan. Adaptive optimization techniques allow the DBMS to modify execution behavior based on runtime feedback.\n\nThree adaptive strategies are described. Modifying future invocations uses feedback from previous executions to improve subsequent plans. Replanning the current invocation allows the DBMS to halt execution and generate a new plan if observed behavior deviates significantly from estimates, either restarting execution or reusing intermediate results. Plan pivot points embed alternative subplans within a query plan and select between them at runtime based on observed statistics. Techniques such as parametric optimization and proactive reoptimization generate multiple candidate subplans and dynamically choose the best option during execution.\n\nFinally, the lecture discusses plan stability mechanisms, including optimizer hints, fixed optimizer versions, and backward-compatible plans that preserve behavior across DBMS upgrades. The lecture concludes by emphasizing that query optimization is critical to database performance, requiring careful translation from SQL to logical plans and then to physical plans, accurate cost estimation based on summarized statistics, effective expression handling, and systematic plan enumeration using either bottom-up or top-down strategies."
}
