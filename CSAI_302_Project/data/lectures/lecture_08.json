{
  "id": "lecture_08",
  "title": "Concurrency Control",
  "text": "This lecture focuses on concurrency control in database management systems, explaining how a DBMS ensures correct execution of concurrent transactions while maximizing system performance. The primary goal of concurrency control is to guarantee that all possible transaction schedules are either conflict-serializable or view-serializable, and that they are recoverable and preferably cascadeless. Allowing only one transaction to execute at a time would trivially guarantee correctness but would severely limit concurrency and system throughput. Therefore, concurrency control mechanisms must proactively prevent incorrect schedules rather than detecting serializability violations after execution.\n\nThe lecture begins by introducing common concurrency control problems that arise when transactions interleave improperly. A dirty write, also known as a lost update, occurs when two transactions update the same data item concurrently and one update overwrites the other, resulting in an incorrect final value. A dirty read, or temporary update, occurs when a transaction reads a value written by another transaction that later aborts, causing the reading transaction to rely on incorrect data. A non-repeatable read occurs when a transaction reads the same data item multiple times and obtains different values because another transaction modifies the item in between. An incorrect summary, also known as the phantom phenomenon, occurs when a transaction computes an aggregate result while other transactions concurrently modify the underlying data, leading to an inconsistent aggregate.\n\nThe need for recovery is discussed in conjunction with concurrency control. Transactions must either commit, meaning all their effects are permanently recorded in the database, or abort, meaning all their effects are rolled back. To support recovery, the DBMS maintains a system log, which is a sequential, append-only file that records transaction operations and is not affected by system failures. A log buffer temporarily stores log records in main memory and periodically flushes them to disk. Using the log, the DBMS can perform undo and redo operations during recovery.\n\nThe commit point of a transaction is defined as the moment when all database operations of the transaction have completed successfully and a commit record is written to the log. In the event of a system failure, transactions with a recorded start but no commit record are identified and rolled back.\n\nConcurrency control protocols are introduced as mechanisms that allow concurrent schedules while ensuring serializability, recoverability, and cascadeless execution. These protocols do not explicitly test schedules for serializability using precedence graphs. Instead, they impose rules and disciplines that prevent non-serializable schedules from occurring. Serializability tests are primarily used to reason about the correctness of concurrency control protocols rather than as runtime enforcement mechanisms.\n\nThe lecture discusses weak levels of consistency, noting that some applications can tolerate non-serializable schedules in exchange for improved performance. Examples include read-only transactions that compute approximate results and database statistics used for query optimization. Such applications trade accuracy for performance and do not require full serializability.\n\nIsolation levels are introduced as a hierarchy of guarantees that control which concurrency problems are prevented. Lower isolation levels allow more concurrency but tolerate certain anomalies. A simple hierarchy is presented: Level 0 prevents dirty reads, Level 1 prevents dirty writes, Level 2 prevents both dirty reads and dirty writes, and Level 3 provides true isolation by preventing dirty reads, dirty writes, and non-repeatable reads. Protocols that ensure serializable and recoverable schedules operate at Level 3.\n\nThe implementation of isolation levels is discussed through three main approaches: locking, timestamp-based protocols, and multi-versioning. Locking may be applied at different granularities, such as the entire database or individual data items, and locks may be held for varying durations. Locks can be shared for reading or exclusive for writing. Timestamp-based protocols assign timestamps to transactions and data items to detect out-of-order accesses. Multi-version concurrency control maintains multiple versions of data items, allowing transactions to read from consistent snapshots of the database.\n\nLocking is introduced as the most widely used concurrency control mechanism. A lock is a variable associated with a data item that indicates whether the item can be accessed by transactions. In binary locking, each data item has a lock that is either locked or unlocked. When a transaction requests a lock on a locked item, it must wait until the item is unlocked. Binary locking requires that all read and write operations occur between a lock and an unlock operation and that transactions are well-formed. However, binary locking alone does not guarantee serializability and is overly restrictive.\n\nTo improve concurrency, shared and exclusive locking is introduced. Shared locks allow multiple transactions to read the same data item concurrently, while exclusive locks allow a single transaction to write to the item. Each data item maintains a lock state and a count of reading transactions. Rules are defined to ensure that read operations follow a read or write lock and that write operations follow a write lock, with no lock type changes allowed without unlocking first.\n\nTwo-phase locking (2PL) is presented as a protocol that guarantees serializability. A transaction following two-phase locking acquires all its locks during a growing phase and releases them during a shrinking phase, with no lock acquisition allowed after the first unlock. Two-phase locking ensures that every schedule generated is conflict-serializable, allowing serializable schedules to be constructed incrementally without knowing transactions in advance. However, two-phase locking may lead to deadlocks.\n\nDeadlocks are defined as situations where two or more transactions wait indefinitely for each other to release locks, forming a cycle of dependencies. The lecture discusses several strategies for handling deadlocks, including deadlock avoidance, deadlock prevention, deadlock detection, and timeouts. Conservative two-phase locking and predefined lock ordering can prevent deadlocks but significantly reduce concurrency and are impractical in many systems.\n\nDeadlock prevention protocols are introduced, including timestamp-based schemes such as wait-die and wound-wait, as well as waiting-based schemes such as no-waiting and cautious-waiting. In wait-die, older transactions wait while younger transactions abort and restart. In wound-wait, older transactions force younger ones to abort, while younger transactions wait. These protocols guarantee freedom from deadlocks and starvation but may cause unnecessary transaction aborts.\n\nDeadlock detection is described as an alternative approach in which the DBMS maintains a wait-for graph representing dependencies between transactions. A cycle in this graph indicates a deadlock, which is resolved by aborting a selected victim transaction. Timeout-based approaches abort transactions that wait longer than a predefined period, offering simplicity but no guarantees against starvation or unnecessary aborts.\n\nTimestamp-based concurrency control is introduced as a locking-free approach. Transactions are assigned timestamps, and conflicts are resolved by rolling back transactions that violate the timestamp order. Basic timestamp-ordering protocols ensure serializability but may allow cascading rollbacks and non-recoverable schedules. Strict timestamp-ordering protocols delay conflicting operations until earlier transactions commit, ensuring strict and recoverable schedules.\n\nEach data item maintains a read timestamp and a write timestamp, representing the most recent transactions that accessed the item. Rules are defined for handling read-write, write-read, and write-write conflicts. The Thomas write rule is introduced as an optimization that ignores obsolete write operations in certain cases, reducing unnecessary rollbacks while preserving correctness guarantees.\n\nThe lecture then introduces multi-version concurrency control, where the DBMS maintains multiple versions of each data item. Transactions read from appropriate versions based on their timestamps, increasing concurrency and reducing blocking. However, multi-versioning requires additional storage and version management overhead.\n\nFinally, the lecture discusses data item granularity in concurrency control. Granularity refers to the size of data items that can be locked, ranging from individual field values to entire databases. Coarser granularity reduces locking overhead but limits concurrency, while finer granularity increases concurrency at the cost of higher overhead. The optimal granularity depends on transaction access patterns, and systems may support multiple granularities simultaneously to balance performance and concurrency."
}
