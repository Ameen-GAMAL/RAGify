{
  "id": "lecture_09",
  "title": "Database Recovery Techniques",
  "text": "This lecture focuses on database recovery techniques and explains how a database management system restores the database to a correct and consistent state after failures. Recovery mechanisms are a core component of transaction management and are primarily responsible for enforcing the atomicity and durability properties of ACID. Atomicity ensures that a transaction is executed entirely or not at all, while durability guarantees that the effects of committed transactions persist even in the presence of system failures. Recovery works in close coordination with concurrency control and logging mechanisms.\n\nThe lecture begins with a review of recoverable transactions and the hierarchy of recoverability conditions. A schedule is recoverable if a transaction does not commit before all transactions from which it has read data have committed. Cascadeless schedules strengthen this requirement by ensuring that transactions do not read data written by uncommitted transactions, thereby preventing cascading rollbacks. Strict schedules impose an even stronger condition by disallowing both reads and writes on data items modified by uncommitted transactions. Strict schedules simplify recovery and are the preferred execution model in practical DBMS implementations.\n\nRecovery is defined as the process of restoring the database to the most recent consistent state before a failure occurs. The lecture categorizes failures into several types, including system crashes caused by hardware, software, or network failures; transaction failures caused by logical errors or constraint violations; local transaction errors such as missing data or programmed exceptions; concurrency control enforcement actions such as deadlock resolution or serializability violations; disk crashes involving persistent read or write errors; and physical failures such as power outages, fires, or other catastrophic events.\n\nFailures are grouped into two major categories. If the disk is significantly damaged, such as in catastrophic disk crashes, the entire database must be restored from backup storage. If the disk is not damaged, which includes most system crashes and transaction failures, recovery can be performed by undoing and redoing selected operations using the database stored on disk and a log of operations. In this case, the recovery process must take into account the cache in main memory.\n\nThe lecture introduces the role of cache in recovery. DBMSs use a dedicated cache in main memory that consists of buffers corresponding to disk blocks. Data pages, index pages, and log records are temporarily stored in cache for fast access. When a data operation is requested, the cache is checked first. If the required block is present, it is used directly; otherwise, an existing buffer is replaced according to a buffer replacement policy and the required block is loaded from disk. If a write operation modifies a buffer, it becomes dirty.\n\nTwo flushing approaches are discussed. In in-place flushing, which is used in most systems, the modified buffer overwrites the original disk block when flushed. In shadowing, a modified version of the block is written to a new disk location while the original version is preserved, resulting in higher overhead. Each buffer has an associated dirty bit indicating whether it has been modified.\n\nBuffer replacement policies determine which buffers should be flushed when cache space is needed. Least Recently Used (LRU) is introduced as a basic replacement policy that evicts the least recently accessed buffer. However, LRU is not specialized for database workloads because different types of data, such as tables, indexes, and logs, exhibit different access patterns. To address this, domain separation techniques assign dedicated cache regions to different domains, each using its own LRU policy, resulting in better performance. This approach can be further improved using techniques such as hot sets and clock sweep algorithms.\n\nThe system log is introduced as a fundamental component of recovery. The log is a sequential, append-only file that records relevant effects of transaction operations. Logging is performed by stand-alone system operations and is not part of transaction execution itself. Reading and writing the log are unaffected by system crashes except in the case of disk or catastrophic failures, and the log is periodically backed up to guard against such failures.\n\nThe log records only the effects of data writes and possibly reads, not intermediate computations. It also records transaction boundaries, such as begin, commit, and abort events, as well as additional information related to checkpoints and dirty buffer tables. Log entries for write operations may include UNDO (before-image) records, REDO (after-image) records, or both. UNDO records are used to reverse changes made by aborted transactions, while REDO records are used to reapply changes of committed transactions that were not yet flushed to disk.\n\nTo improve performance, DBMSs use a log cache buffer in main memory that temporarily stores recent log records. When the log cache buffer fills up, it is flushed to the log file on disk. The log cache buffer may also be flushed explicitly to enforce write-ahead logging, which requires that log records describing a change be written to disk before the corresponding data page is flushed.\n\nWrite-Ahead Logging (WAL) is introduced as a fundamental recovery principle. A recovery protocol follows WAL if the before-image of a data item is logged and flushed to disk before the data item itself is overwritten on disk, and if a transaction is not allowed to commit until all its UNDO and REDO log records have been flushed to disk. WAL guarantees that the system can undo aborted transactions and redo committed ones after a crash. Most practical recovery protocols are based on WAL.\n\nThe lecture explains WAL operation in detail. First, intended changes are written as log records before any database pages are modified. Second, before a transaction is marked as committed, the log is flushed to disk, guaranteeing durability. Finally, database pages are updated asynchronously, often during checkpointing. If a crash occurs after the log is flushed but before pages are written, the log can be used to redo operations.\n\nCheckpoints are introduced as a mechanism to limit recovery work. A checkpoint is a recovery operation that ensures all committed changes up to a certain point are safely recorded on disk. This is achieved by flushing relevant dirty buffers and logging the set of active transactions. Checkpoints are performed periodically, with more frequent checkpoints reducing recovery time but increasing runtime overhead. The checkpoint operation involves suspending transactions, writing a checkpoint record to the log, flushing the log, flushing dirty buffers, and then resuming transaction execution.\n\nThe lecture then discusses the interaction between recovery and concurrency control. The recovery subsystem assumes that system crashes abort all active transactions and often assumes strict two-phase locking to simplify recovery, ensuring that uncommitted changes are not visible to other transactions.\n\nSeveral recovery protocols are presented. The shadowing (shadow paging) protocol avoids logging by maintaining shadow copies of pages. At the start of a transaction, a shadow directory pointing to original disk blocks is created. Modified pages are written to new disk locations, and the current directory is updated. If the transaction commits, the current directory becomes permanent; if it aborts, the shadow directory is used to restore the old state. Shadowing requires no logging or checkpoints but results in high overhead, random disk layout, and complex garbage collection, making it impractical for most systems.\n\nThe deferred-update recovery protocol postpones all data page flushes until a transaction commits, following a no-steal approach. Only REDO log records are needed because aborted transactions never modify disk pages. This protocol is effective for short transactions with few updates but places pressure on cache space. It assumes strict two-phase locking, system crashes that abort all active transactions, and that all modified pages of a transaction fit in memory. During recovery, committed transactions since the last checkpoint are identified and their operations are redone using the log.\n\nThe immediate-update recovery protocol allows dirty pages to be flushed to disk before transaction commit, following a steal approach. Both UNDO and REDO log records are required to support rollback of aborted transactions and reapplication of committed ones. This protocol generalizes deferred-update and is more flexible but more complex. It relies fully on write-ahead logging and regular checkpoints. During recovery, committed transactions are redone and uncommitted transactions are undone.\n\nThe lecture concludes with the ARIES recovery protocol, which stands for Algorithm for Recovery and Isolation Exploiting Semantics. ARIES is an improved version of the immediate-update protocol and is widely used in practice. It supports steal and no-force policies, uses both UNDO and REDO logging, and follows write-ahead logging. ARIES introduces fuzzy checkpoints that avoid flushing all dirty pages by recording the transaction table and dirty buffer table at checkpoint time.\n\nARIES recovery consists of three main phases. The analysis phase scans the log to identify active transactions and the earliest update of dirty pages. The redo phase replays necessary updates to ensure all committed changes are reflected on disk. The undo phase rolls back uncommitted transactions by traversing the log backward. ARIES logs its own undo and redo actions so that recovery itself is resilient to crashes. This design significantly reduces recovery time while maintaining correctness and durability."
}
