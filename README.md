# Retrieval-Augmented Generation System with Vector Databases (RAGify)

## ğŸ“Œ Project Overview

This repository implements a **complete, modular, and end-to-end Retrieval-Augmented Generation (RAG) system** built around **dense vector databases** and **semantic retrieval**.  
The system is designed to retrieve relevant knowledge from a vector index and generate **context-grounded responses** using a Large Language Model (LLM).

The project follows modern RAG system design principles and emphasizes:
- Vector-based semantic search
- Modular pipeline architecture
- Explainable retrieval
- Clean software engineering practices
- Reproducibility and extensibility

This repository represents the **final implementation** of the project.

---

## ğŸ§  Retrieval-Augmented Generation (RAG)

### Definition

Retrieval-Augmented Generation (RAG) is a hybrid AI architecture that combines:

- **Dense Information Retrieval** using vector similarity search
- **Natural Language Generation** using large language models

Instead of relying solely on a modelâ€™s parametric memory, RAG dynamically retrieves external documents and conditions generation on them.

---

### High-Level Pipeline

```text
User Query
   â†“
Query Embedding
   â†“
Vector Similarity Search (FAISS)
   â†“
Top-k Relevant Documents
   â†“
Context Construction
   â†“
Prompt Injection
   â†“
LLM Generation
   â†“
Final Grounded Answer


## ğŸ—ï¸ System Architecture

### Architecture Diagram (Conceptual)

```text
Lecture Data (JSON Files)
        â”‚
        â–¼
Text Chunking (Overlapping)
        â”‚
        â–¼
Embedding Model (SBERT MPNet)
        â”‚
        â–¼
Vector Database (FAISS)
        â”‚
        â–¼
Semantic Search (Top-k Chunks)
        â”‚
        â–¼
Retrieved Documents
        â”‚
        â–¼
Prompt Assembly + Retrieved Docs
        â”‚
        â–¼
LLM Generation (HF Router API)
        â”‚
        â–¼
Final Answer + Retrieved Docs


## ğŸ“ Repository Structure

RAGify/
â”‚
â”œâ”€â”€ app.py                     # Optional UI entry point
â”‚
â”œâ”€â”€ ragify/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embeddings.py          # Embedding model abstraction
â”‚   â”œâ”€â”€ vector_store.py        # FAISS index management
â”‚   â”œâ”€â”€ retriever.py           # Semantic retrieval logic
â”‚   â”œâ”€â”€ generator.py           # LLM interaction layer
â”‚   â”œâ”€â”€ pipeline.py            # End-to-end RAG pipeline
â”‚   â”œâ”€â”€ chunking.py            # Text chunking utilities
â”‚   â””â”€â”€ loader.py              # Data loading utilities
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Raw input documents
â”‚   â””â”€â”€ processed/             # Chunked / preprocessed data
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


## ğŸ“Š Data Handling
Input Data

Plain text or JSON documents

Each document may contain metadata (IDs, titles, sources)

Chunked Data

Documents are split into overlapping chunks prior to embedding.


| Field       | Description             |
| ----------- | ----------------------- |
| `chunk_id`  | Unique chunk identifier |
| `source_id` | Original document ID    |
| `text`      | Chunk content           |


### âœ‚ï¸ Chunking Strategy
Motivation

Chunking is necessary because:

Embedding models have input length limits

Long documents dilute semantic relevance

Overlapping preserves boundary semantics

Parameters

| Parameter  | Value          |
| ---------- | -------------- |
| Chunk Size | 400 words      |
| Overlap    | 80 words       |
| Strategy   | Sliding window |


### ğŸ§¬ Embedding Layer
Model

Sentence-Transformers (all-mpnet-base-v2)

Properties

| Property         | Value |
| ---------------- | ----- |
| Vector Dimension | 768   |
| Embedding Type   | Dense |
| Normalization    | L2    |



### ğŸ—„ï¸ Vector Database
Engine

FAISS (Facebook AI Similarity Search)

Index Type

IndexFlatIP

Similarity Function

Given normalized embeddings:

cosine similarity
(
ğ‘¥
,
ğ‘¦
)
=
ğ‘¥
â‹…
ğ‘¦
cosine similarity(x,y)=xâ‹…y

Thus inner product search is equivalent to cosine similarity search.






### ğŸ” Retrieval Module
Retrieval Steps

Embed user query

Perform FAISS similarity search

Select top-k chunks

Return texts and similarity scores

scores, indices = index.search(query_embedding, k)

Output

Retrieved documents

Similarity scores

Metadata (IDs, sources)




### ğŸ¤– Generation Module
LLM Interface

API-based Large Language Model

Prompt Construction

Retrieved documents are injected into a structured prompt.

Context:
<retrieved documents>

Instruction:
Answer using ONLY the provided context.
If the answer is not contained in the context, say you do not know.


This enforces grounded generation and minimizes hallucination.





## ğŸ” End-to-End Pipeline
query â†’ embed â†’ retrieve â†’ assemble context â†’ generate answer


The pipeline is orchestrated in a single modular interface for clarity and extensibility.




## ğŸš€ Future Improvements

Persistent FAISS index

Multi-stage retrieval (BM25 + dense)

Re-ranking with cross-encoders

Feedback-driven self-learning

Source citation per answer
