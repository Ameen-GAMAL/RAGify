# ğŸ“š Retrieval-Augmented Generation (RAG) System using Vector Databases

## ğŸ“Œ Project Overview

This project implements a **complete end-to-end Retrieval-Augmented Generation (RAG) system** as part of the **CSAI 302 â€“ Vector Database Assignment**.  
The system is designed to retrieve semantically relevant lecture content using dense vector embeddings and generate grounded, context-aware answers using a Large Language Model (LLM).

The project strictly follows the assignment requirements and includes:
- A **vector database layer**
- A **semantic retrieval mechanism**
- A **generation module grounded in retrieved documents**
- A **user-friendly querying interface (UI)**

---

## ğŸ§  What is Retrieval-Augmented Generation (RAG)?

Retrieval-Augmented Generation (RAG) is a hybrid architecture that combines:
- **Information Retrieval (IR)** using vector similarity search
- **Text Generation** using large language models

### High-level RAG Flow:
```text
User Query
   â†“
Query Embedding
   â†“
Vector Similarity Search (FAISS)
   â†“
Top-k Relevant Chunks
   â†“
Context Injection into Prompt
   â†“
LLM Answer Generation
```


## ğŸ—ï¸ System Architecture
Architecture Diagram (Conceptual)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Lecture Data    â”‚
â”‚  (JSON Files)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Chunking    â”‚
â”‚ (Overlapping)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding Model  â”‚
â”‚ (SBERT MPNet)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector Database   â”‚
â”‚     (FAISS)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Semantic Search  â”‚
â”‚   (Top-k Chunks)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Assembly   â”‚
â”‚ + Retrieved Docs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Generation    â”‚
â”‚ (HF Router API)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Final Answer    â”‚
â”‚ + Retrieved Docs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


## ğŸ“ Project Structure


CSAI_302_Project/
â”‚
â”œâ”€â”€ app.py                      # Streamlit UI (main entry point)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings.py           # Embedding model logic
â”‚   â”œâ”€â”€ vector_store.py         # FAISS vector database
â”‚   â”œâ”€â”€ retrieval.py            # Semantic retrieval logic
â”‚   â”œâ”€â”€ generation.py           # LLM generation module
â”‚   â”œâ”€â”€ chunking.py             # Text chunking with overlap
â”‚   â”œâ”€â”€ load_data.py            # Lecture data loader
â”‚   â””â”€â”€ main.py                 # CLI runner (optional)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ lectures/
â”‚   â”‚   â”œâ”€â”€ lecture_01.json
â”‚   â”‚   â”œâ”€â”€ lecture_02.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ chunks/
â”‚       â””â”€â”€ chunks.json
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## ğŸ“Š Dataset Description
Dataset: Advaned Database Lectures

Lecture Data

Format: JSON

Fields:

id: Lecture identifier

title: Lecture title

text: Full lecture content

Chunked Data

Stored in: data/chunks/chunks.json

Each chunk contains:

chunk_id

lecture_id

text

ğŸ“Œ Total chunks: 41
ğŸ“Œ Total lectures: 9


### âœ‚ï¸ Chunking Strategy
Why Chunking?

Large documents cannot be embedded effectively as a single unit. Chunking:

Preserves semantic coherence

Improves retrieval accuracy

Prevents context truncation

Implementation Details

Chunk size: 400 words

Overlap: 80 words

Type: Word-based sliding window

Overlapping chunks ensure that important information near chunk boundaries is preserved.

### ğŸ§¬ Embedding Model
Model Used

Sentence-Transformers: all-mpnet-base-v2

Reasons for Selection

High performance on semantic similarity tasks

Produces dense vector representations

Well-documented and widely adopted

Embedding Properties

Output dimension: 768

Embeddings are L2-normalized


### ğŸ—„ï¸ Vector Database
Library

FAISS (Facebook AI Similarity Search)

Index Type

IndexFlatIP (Inner Product)

Similarity Metric

Because embeddings are normalized:

cosine similarity
(
ğ‘¥
,
ğ‘¦
)
=
ğ‘¥
â‹…
ğ‘¦
cosine similarity(x,y)=xâ‹…y

Thus, inner product search behaves as cosine similarity search.


### ğŸ” Retrieval Mechanism
Steps

Embed all document chunks

Add embeddings to FAISS index

Embed user query

Retrieve top-k most similar chunks

Output

Retrieved chunk texts

(Optional improvement: similarity scores)

distances, indices = index.search(query_embedding, k)





### ğŸ¤– Generation Module
API Used
Hugging Face OpenAI-Compatible Router

Endpoint:

text
Copy code
https://router.huggingface.co/v1
Authentication
Environment variable:

bash
Copy code
export HF_TOKEN=your_huggingface_token
Prompt Design
Injects retrieved chunks as context

Explicit grounding instruction:

â€œAnswer using ONLY the provided context. If the answer is not present, say you do not know.â€

This minimizes hallucinations and ensures factual grounding.



### ğŸ–¥ï¸ User Interface (Bonus Feature)
Framework

Streamlit

Features

Query input box

Top-k retrieval slider

Displays:

Generated answer

Retrieved document chunks

Run UI:

streamlit run app.py



### ğŸ§ª Demonstration Example
Query
What is backpropagation?

Retrieved Chunks

Chunk from Lecture 03 (Neural Networks)

Chunk from Lecture 04 (Training Algorithms)

Generated Answer

Backpropagation is an algorithm used to train neural networks by computing gradients of the loss function with respect to weights using the chain rule...

Retrieval Explanation

The query embedding was closest to chunks discussing neural network training due to shared semantic concepts such as gradients, loss, and optimization.



### ğŸš€ Future Improvements

Persist FAISS index to disk

Display similarity scores

Add feedback-based re-ranking

Implement self-learning memory

Add multi-document citation tracking
