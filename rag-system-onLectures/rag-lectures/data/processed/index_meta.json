{
  "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
  "normalize_embeddings": true,
  "dim": 384,
  "count": 80,
  "chunks": [
    {
      "chunk_id": "ADB_Lec01_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\nLec 01\nAdvanced DBMS and Database\nSystem Architectures\n\nOutline\n\n-Database system essentials\n-DBMS Architecture Fundamentals\n-Evolution of Database System Architectures\n-Data Models\n-OLTP vs. OLAP Workload Characteristics\n-Course Details\n\nDatabase system\nessentials\n\nDatabase System Architecture\n\n-The overall design and structure of a database system that\noutlines how data is stored, managed, and accessed.\ndefines the components of the system\nhow they interact, and how data flows between them.\n-A well-defined architecture ensures Data:\nIntegrity\nEfficiency\nSecurity\nScalability.\n\nDatabase Architecture Types\n\nTiered\nArchitectures\n\n1-Tier 2-Tier 3-Tier\n\nDatabase Architecture Types\n\nPhysical\nArchitectures\n\nDistributed Centralized Client/Server Parallel Systems Systems\n\nData is stored Multiple clients Uses multiple All data is on a across multiple connect to a processors andsingle computer. physical devices central server. disks. or locations.\n\nSchema\n\n-The blueprint for how data is organized in a\ndatabase\n-defining elements like\ntables, fields, data types, and their relationships\n-but not the actual data itself.\n-This defines the structure of data for a data\nmodel.\n\nThree-Schema Architecture\n\nExternal Schema Conceptual Schema Internal Schema\nView Level Logical Level Physical Level\n\nCustomized data Logical structure of Physical storage\nviews the database structures\n\n- B-tree indexes for\nfrequent lookups\npatient patient - Tables - Hash partitioning for\nmedical demographic - Relationships parallel processing systemHospital records and information - Integrity constraints - Columnar storage for\ntreatment and insurance analytical queries. management\nhistory details.\n\nDBMS\n\n-A software that allows applications to store and\nanalyze information in a database.\n-A general-purpose DBMS is designed to allow the\ndefinition, creation, querying, update, and\nadministration\nof databases\nin accordance with\nsome data model.\n\nDBMS Core\nComponents\n\nDBMS Key Components",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 1,
        "page_end": 11,
        "char_len": 2024
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0002",
      "text": "DBMS\n\n-A software that allows applications to store and\nanalyze information in a database.\n-A general-purpose DBMS is designed to allow the\ndefinition, creation, querying, update, and\nadministration\nof databases\nin accordance with\nsome data model.\n\nDBMS Core\nComponents\n\nDBMS Key Components\n\nDatabase Engine Database Schema Query Processor\n- The blueprint or logical - Interprets and executes- Handles data\nstructure of the database. user queries. processing\n- Translates them into\n- Enables efficient access actions for the database.\nand manipulation.\n\nStorage Manager Transaction Manager\n- Ensures the integrity and - Manages the physical\nconsistency of data by storage of data on\n- Managing transactions various media\n- Enforcing rules like - Handles data access. ACID properties.\n\nDBMS Key Components\n\nDBMS Key Components\n\nDatabase Database Query Storage Transaction\nEngine Schema Processor Manager Manager\n\nThe blueprint or Interprets user ConcurrencyData processing logical structure Buffer Manager queries. Control of the database.\n\nFile OrganizationData access and Builds Execution Recovery and Access manipulation. plans. Management Methods\n\nIndex ACID properties\nManagement Implementation\n\nQuery Processing Engine\n-Transforms high-level SQL statements into\nefficient execution plans through several\ncritical phases.\n\nParser, Abstract SQL Query Query Execution Query Lexical syntax Query Optimizer Plan Engine Execution Analysis tree\n(I/O, Validate (physical CPU, costMinimize planImplement SQL\nquery memory) query syntax operators)\n\nEvolution of\nDatabase System\nArchitectures\n\nHistorical Progression\n\n-The evolution of database architectures reflects\nthe changing demands of data processing, from\nsimple record-keeping to complex analytical\nworkloads handling massive datasets.\n-Architectural evolution Drivers\nScalability Requirements\nData Complexity\nPerformance Expectations\nReliability Demands\n\nFile-Based Systems (1960s-1970s)",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 9,
        "page_end": 16,
        "char_len": 1948
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0003",
      "text": "File-Based Systems (1960s-1970s)\n\n-Early computing relied on flat file systems\n-Each application maintained its own data files.\n-Integrated Data Store\nNetwork data model.\nTuple-at-a-time queries.\nTransaction-oriented\nDedicated working storage area for each record type\n-This approach suffered from data redundancy,\ninconsistency, and lack of standardized access methods.\n\nHierarchical and Network Models (70s-80s)\n\n-IBM's Information Management System (IMS) introduced\nhierarchical structures resembling tree organizations,\n-The Conference on Data Systems Languages (CODASYL)\ndeveloped network models allowing more complex\nrelationships.\n-These systems provided better data organization but\nrequired programmers to navigate complex pointer\nstructures manually.\nHierarchical data model.\nProgrammer-defined physical storage format.\nTuple-at-a-time queries.\n\nRelational Model (80s-90s)\n\n-Relational model introduced mathematical\nfoundations based on set theory and first-order\npredicate logic.\n-Allowed users to focus on what data they wanted rather\nthan how to retrieve it\n-leading to the development of SQL and modern RDBMS\nsystems like Oracle, DB2, and SQL Server.\n-Database abstraction to avoid this maintenance:\nStore database in simple data structures.\nAccess data through high-level language.\nPhysical storage left up to implementation.\n\nObject-Oriented-Systems (2000s)\n\n-As applications became more complex, the need to\nstore complex data types led to object-oriented\ndatabases and object-relational extensions.\n-PostgreSQL exemplifies this evolution, supporting\nboth traditional relational operations and complex\ndata types like arrays, JSON, and user-defined types.\n\n2000s - Internet boom\n\n-All the big players were heavyweight and\nexpensive.\n-Open-source databases were missing\nimportant features.\n-Many companies wrote their own custom\nmiddleware to scale out database across\nsingle-node DBMS instances.\n\n2000s - Data warehouses\n-Rise of the special purpose OLAP\nDBMSs.\nDistributed / Shared-Nothing\nRelational / SQL\nUsually closed-source.\n-Significant performance benefits\nfrom using columnar data\nstorage model.\n\n2000s - NoSQL Systems",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 16,
        "page_end": 22,
        "char_len": 2157
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0004",
      "text": "2000s - Data warehouses\n-Rise of the special purpose OLAP\nDBMSs.\nDistributed / Shared-Nothing\nRelational / SQL\nUsually closed-source.\n-Significant performance benefits\nfrom using columnar data\nstorage model.\n\n2000s - NoSQL Systems\n\n-Focus on high-availability &\nhigh-scalability:\nSchema-less (i.e., “Schema Last”)\nNon-relational data models\n(document, key/value, etc)\nNo ACID transactions\nCustom APIs instead of SQL\nUsually open-source\n\n2010s - NewSQL\n-Provide same performance for OLTP\nworkloads as NoSQL DBMSs without giving\nup ACID:\nRelational / SQL\nDistributed\nUsually closed-source\n\n2010s - Hybrid systems\n-Hybrid Transactional-Analytical Processing.\n-Execute fast OLTP like a NewSQL system while\nalso executing complex OLAP queries like a data\nwarehouse system.\nDistributed / Shared-Nothing\nRelational / SQL\nMixed open/closed-source.\n\n2010s - Cloud systems\n-First database-as-a-service (DBaaS) offerings\nwere \"containerized\" versions of existing\nDBMSs.\n-There are new DBMSs that are designed from\nscratch explicitly for running in a cloud\nenvironment.\n\n2010s - Shared-disk engines\n-Instead of writing a custom storage\nmanager, the DBMS relies on third-party\ndistributed storage (object stores) .\nScale execution layer independently of\nstorage.\nFavors log-structured approaches.\n-This is what most people think of when\nthey talk about a data lake.\n\n2010s - Stream processing\n\n-Execute continuous queries on streams of tuples.\n-Extend processing semantics to include notion of\nwindows.\n-Often used in combination of batch-oriented\nsystems in a lambda architecture deployment.\n\n2010s - Graph systems\n\n-Systems for storing and querying graph data.\n-Their main advantage over other data models\nis to provide a graph-centric query API\nRecent research demonstrated that is unclear\nwhether there is any benefit to using a graphcentric execution engine and storage manager.\n\n2010s - Timeseries systems\n\n-Specialized systems that are designed to\nstore timeseries / event data.\n-The design of these systems make deep\nassumptions about the distribution of data\nand workload query patterns.\n\n2020s-LAKEHOUSE SYSTEMS",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 21,
        "page_end": 30,
        "char_len": 2125
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0005",
      "text": "2010s - Timeseries systems\n\n-Specialized systems that are designed to\nstore timeseries / event data.\n-The design of these systems make deep\nassumptions about the distribution of data\nand workload query patterns.\n\n2020s-LAKEHOUSE SYSTEMS\n\n-Middleware for data lakes that adds support\nfor better schema control / versioning with\ntransactional CRUD operations.\nStore changes in row-oriented log-structured\nfiles with indexes.\nPeriodically compact recently added data into\nread-only columnar files.\n\nData Models\nA collection of concepts for\ndescribing the data in database.\n\nCommon Data Models\n\nRelational Key/Value Graph\n- (Most DBMSs) - (NoSQL) - (NoSQL)\n\nDocument / Wide-Column / Array / Matrix /\nXML / Object Column-family Vector\n- (NoSQL) - (NoSQL) - (Machine Learning)\n\nHierarchical /\nNetwork / MultiValue\n- (Obsolete/ Rare)\n\nRelational\nModel\n\nRelational Model\n\n-The relational model defines a database\nabstraction based on relations to avoid\nmaintenance overhead.\n-It has three key ideas:\nStore database in simple data structures (relations)\nPhysical storage left up to the DBMS implementation\nAccess data through a high-level (declarative)\nlanguage, where the DBMS figures out best execution\nstrategy\n\nRelational Model concepts\n\nStructure Integrity Manipulation\n\nThe definition of\nrelations and their Ensure the database’s\ncontents independent contents satisfy\nof their physical certain constraints.\nAn interface for representation.\naccessing and\nmodifying a\ndatabase’s contents\nAn example of a (e.g. SQL, Dataframes)Each relation has a set constraint would be of attributes. Each that the age of aattribute has a domain person cannot be a of values. negative number.\n\nRelational Model definitions\n\n-Relation\n-Tuple\n-Primary key\n-Foreign key\n-Constraint\n\nDocument\nModel\n\nDocument Data Model\n\n-A collection of record documents containing a hierarchy of\nnamed field/value pairs.\nA field’s value can be either a scalar type, an array of values, or\nanother document.\nModern implementations use JSON.\nOlder systems use XML or custom object representations.\n-Avoid “relational-object impedance mismatch” by tightly\ncoupling objects and database.\n\nRelational vs. Document",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 29,
        "page_end": 39,
        "char_len": 2176
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0006",
      "text": "Relational vs. Document\n\nSUPPLIER SUPPLIER\n(sno, sname, scity) (sno, sname, scity, sstate, parts[])\n\nSUPPLY\n(sno, pno, qty, price)\n\nPART PART\n(pno, pname) (pno, pname, qty, price)\n\nJSON file\n\n{\n{\n\"Suppliers\": [\n\"sno\": 53,\n{\n\"sname\": \"Samy\",\n\"sno\": 17,\n\"scity\": \"6th Oct.\",\n\"sname\": \"Adel\",\n\"parts\": [\n\"scity\": \"Mansoura\",\n{\n\"parts\": [\n\"pno\": 154,\n{\n\"pname\": \"wallet\",\n\"pno\": 124,\n\"qty\": 5,\n\"pname\": \"chair\",\n\"price\":2000\n\"qty\": 45,\n},\n\"price\":1000\n{\n},\n\"pno\": 162,\n{\n\"pname\": \"sunglasses\",\n\"pno\": 135,\n\"qty\": 12,\n\"pname\": \"table\",\n\"price\":1500\n\"qty\": 32,\n}\n\"price\":3000\n]\n}\n}\n]\n]\n},\n}\n\nJSON table\nSuppliers sno 17 53\n\nsname Adel Samy\n\nscity Mansoura 6th Oct.\nparts pno 124 135 pno 154 162\n\npname chair table pname wallet sunglasses\n\nqty 45 32 qty 5 12\n\nprice 1000 3000 price 2000 1500\n\nXML file\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <Suppliers>\n<root> <sno>53</sno>\n<Suppliers> <sname>Samy</sname>\n<sno>17</sno> <scity>6th Oct.</scity>\n<sname>Adel</sname> <parts> <scity>Mansoura</scity>\n<pno>154</pno> <parts>\n<pname>wallet</pname> <pno>124</pno>\n<pname>chair</pname> <qty>5</qty>\n<qty>45</qty> <price>2000</price>\n<price>1000</price> </parts>\n</parts> <parts>\n<parts> <pno>162</pno>\n<pno>135</pno> <pname>sunglasses</pname>\n<pname>table</pname>\n<qty>12</qty> <qty>32</qty>\n<price>1500</price> <price>3000</price>\n</parts> </parts>\n</Suppliers> </Suppliers>\n</root>\n\nVector data\nModel\n\nVector data model\n-One-dimensional arrays used for nearest-neighbor search\n(exact or approximate).\nUsed for semantic search on embeddings generated by MLtrained transformer models (think ChatGPT).\nNative integration with modern ML tools and APIs (e.g.,\nLangChain, OpenAI).\n-At their core, these systems use specialized indexes to\nperform NN searches quickly.\n\nContent-Based Retrieval Workflow Using\nVector Space Embeddings\n\nOLTP vs. OLAP\nWorkload\nCharacteristics\n\nOnline Transaction\nProcessing (OLTP)\n\nOLTP\n\n-OLTP systems handle operational business processes:\n\nrequiring fast, consistent transaction processing\n\nwith high concurrency levels.\n\n-A class of systems designed for managing real-time, day-to-day business transactions.\n\n-Examples\n\nATM transactions\n\nonline banking\n\ncredit card processing",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 39,
        "page_end": 48,
        "char_len": 2195
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0007",
      "text": "-OLTP systems handle operational business processes:\n\nrequiring fast, consistent transaction processing\n\nwith high concurrency levels.\n\n-A class of systems designed for managing real-time, day-to-day business transactions.\n\n-Examples\n\nATM transactions\n\nonline banking\n\ncredit card processing\n\ne-commerce purchases.\n\n-These systems are characterized by\n\nhigh volume\n\nshort transactions\n\nneed for quick response times.\n\nCharacteristics and Requirements\n-OLTP workloads involve\nshort-duration transactions\naccessing small amounts of current data.\n-Response times must be predictable and fast, usually\nunder 100 milliseconds.\n-Consider an e-commerce checkout process\nMultiple users simultaneously place orders, update\ninventory, and process payments.\nEach transaction involves few records but requires\nimmediate consistency to prevent overselling products.\n\nData Access Patterns\n\n-Use indexed access to retrieve specific records or small record sets.\n-Queries involve\nprimary key lookups\nsimple range scans.\n-Update operations modify\nindividual records\nsmall groups of related records.\n-The workload mix typically includes\n70-80% read operations\n20-30% write operations\ndistributed across many different tables and records.\n\nSystem Optimizations\n\n-Row-oriented storage optimizes OLTP performance by\nstoring complete records together, minimizing I/O for\ntransactions accessing multiple attributes of the same\nentity.\n-B+ tree indexes provide efficient access paths for both\nequality and range queries.\n-Connection pooling and prepared statements reduce\noverhead for repetitive operations.\n-Partitioning strategies often use range or hash partitioning\nbased on transaction date or customer ID to distribute load\nevenly.\n\nOnline Analytical\nProcessing (OLAP)\n\nOLAP\n\n-OLAP systems support\ncomplex analytical queries and business\nintelligence applications\nrequiring sophisticated data aggregation and\nanalysis capabilities.\n-OLAP systems enable users to analyze large\ndatasets from multiple perspectives.\n\nOLAP Example",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 48,
        "page_end": 54,
        "char_len": 2030
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0008",
      "text": "Online Analytical\nProcessing (OLAP)\n\nOLAP\n\n-OLAP systems support\ncomplex analytical queries and business\nintelligence applications\nrequiring sophisticated data aggregation and\nanalysis capabilities.\n-OLAP systems enable users to analyze large\ndatasets from multiple perspectives.\n\nOLAP Example\n\n-Analyzing sales data for a retail company using an\nOLAP cube.\nThe cube could organize sales by\nproduct, time, and location,\nallowing users to drill down into specific\nregions, products, or time periods\nto identify\ntrends, such as which products are most popular\nin a particular city during a specific month.\n\nCharacteristics and Requirements\n\n-OLAP workloads involve long-running queries that\nscan large portions of historical data to compute\naggregations, trends, and statistical summaries.\n-Response times range from seconds to minutes, but\nquery complexity and data volume are substantially\nhigher than OLTP systems.\n-Consider a retail analytics query calculating seasonal\nsales trends across multiple product categories,\ngeographical regions, and customer segments over\nseveral years.\n\nData Access Patterns\n\n-OLAP queries typically involve full table scans or\nlarge range scans, often accessing millions or billions\nof records.\n-Aggregation operations like SUM, COUNT, and AVG\nare common, frequently combined with GROUP BY\nclauses creating multi-dimensional summaries.\n-Joins often involve large fact tables with smaller\ndimension tables in star or snowflake schema\nconfigurations.\n\nSystem Optimizations\n\n-Columnar storage dramatically improves OLAP\nperformance by storing each column separately, enabling\nefficient compression and reducing I/O for queries\naccessing few columns.\n-Bitmap indexes support complex Boolean queries\ncommon in analytical workloads.\n-Materialized views pre-compute common aggregations,\ntrading storage space for query performance.\n-Parallel processing distributes query execution across\nmultiple processors or machines.\n\nHybrid Approaches\n\nHybrid Approaches",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 52,
        "page_end": 59,
        "char_len": 1992
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0009",
      "text": "Hybrid Approaches\n\nHybrid Approaches\n\n-HTAP (Hybrid Transactional/Analytical Processing)\nModern systems increasingly support mixed workloads combining\nOLTP and OLAP requirements.\nSAP HANA exemplifies this approach using in-memory storage and\ncolumnar organization to support both transaction processing and realtime analytics on the same data.\n-Data Replication Strategies\nMany organizations maintain separate OLTP and OLAP systems\nconnected through data replication mechanisms.\nChange Data Capture (CDC) identifies and propagates OLTP system\nchanges to analytical systems with minimal impact on operational\nperformance.\n\nSystem\nOptimizations\n\nPhysical Design Optimizations\n\n-Index Selection and Design\n\n-Partitioning Strategies\n\nQuery Optimization Techniques\n\n-Cost-Based Optimization\n\n-Advanced Optimization Techniques\n\nMemory and Caching Strategies\n\n-Buffer Pool Management\n\n-Query Result Caching\n\nCourse\nDetails\n\nCourse plan\n\nW LECTURE LAB Coursework\nAdvanced DBMS and Database\n1 - Environment setup\nSystem Architectures\nAssignment 1\n- Implementing a buffer pool manager.\n- Storage Manager and Buffer\n2 Storage Management - Benchmarking different buffer pool strategies\nPool Implementation with\nunder various workloads\nDifferent Replacement Policies\nStorage Models - Row-Oriented - Implement storage engines.\n3\nand Columnar Systems - Convert row-store to columnar\nAssignment 2\nIndexing Structures and - Build a B+ tree index;\n4 - Indexing structures (b+ trees\nImplementation - Implement extendible hashing.\nand lsm trees)\n- Build inventory-auditing triggers.\nAdvanced Programming and - Atomic SPs with error handling\n5 Quiz 1\nQuery Processing - Implementing join algorithms and vectorized\nexecution operators\nAssignment 3\n- Implementing a cost-based query optimizer.\n6 Query Optimization - Query Processing and\n- Tune query plans.\nOptimization\n- Deadlock simulation; lock contention analysis.\nTransaction Processing and\n7 - Analyzing isolation level bugs in production\nConcurrency Control\nsystems\n8 Mid Term Exam\n\nCourse plan",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 58,
        "page_end": 66,
        "char_len": 2033
      }
    },
    {
      "chunk_id": "ADB_Lec01_c0010",
      "text": "Course plan\n\nW LECTURE LAB Coursework\nAssignment 4\nCrash Recovery - Implementing WAL and - Transaction Processing with 9 recovery procedure Concurrency Control and\nRecovery\n- Implement cross-shard ACID\nParallel and Distributed transactions10 Database Architectures - Distributed transaction\ncoordination\n- Use MongoDB's query language NoSQL Databases and Big Data Assignment 511 to explore document-based Storage Systems - Implement NoSQL queries databases.\n- Implement core components of12 Advanced NoSQL Models. different NoSQL models\nCaching Strategies, ORM - Session data caching Assignment 6\n13 Integration, and Modern - API caching - Build a Simple Web-site\nDatabase Trends - E-commerce platforms Integrating an ORM Framework\n- Implement column-level14 Database Security encryption; audit log triggers\n15 Course Wrap up Assignments Presentation as a Whole Project\n\nAssessment Strategy\nAssessment Method Degrees\nQuizzes (best 2 of 3) (week 5, 10, 14) 10\nMidterm Exam (week 8) 20\nLecture participation 8\nLab participation 7\nTerm Project 15\nFinal Exam 40\nTotal 100",
      "metadata": {
        "lecture_id": "ADB_Lec01",
        "source_file": "ADB_Lec01.pdf",
        "page_start": 66,
        "page_end": 67,
        "char_len": 1067
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\n\nLec 02\nStorage Management\n\nStorage hierarchy\nFaster CPU CPU Registers\nSmaller\nVolatile CPU Caches L1 Expensive\nL2 , L3\nRandom Access\nMemory\nDRAM Byte-Addressable\n\nDisk Non-Volatile SSD\n\nSequential Access\nHDD Slower\nBlock-Addressable\nLarger\nTertiary storage (Network Storage) Cheaper\n\nDisk-based\nArchitecture\n\nDisk-based Architecture\n\n-The DBMS assumes that the primary storage\nlocation of the database is on non-volatile disk.\n-The DBMS's components manage the movement\nof data between non-volatile and volatile storage.\n-Lowest layer of DBMS\nPhysical details hidden from higher levels of system\n-Purpose\nMap pages to locations on disk\nLoad pages from disk to memory\nSave pages back to disk & ensuring write\n\nTerminologies\n\n-Block\nunit of transfer for disk read/write\n-Page\na block-sized chunk of RAM\n\nFiles of Pages of Records\n\n-Each table is stored in one or more OS files\n-Each file contains many pages\n-Each page contains many records\n-Pages are understood by multiple layers\nManaged on disk by the disk space manager:\npages read from/written to physical disk/files\nManaged in memory by the buffer manager:\nhigher levels of DBMS only operate in memory\n\nRepresenting Data Elements\n\nData Record Collection Element\n\nDBMS Attribute Tuple Relation\n\nFile System Field Record File\n\nSystem design goals\n\n-Allow the DBMS to manage databases that\nexceed the amount of memory available.\n-Reading/writing to disk is expensive\nIt must be managed carefully to avoid large\nstalls and performance degradation.\n-Random access on disk is usually much\nslower than sequential access\nthe DBMS needs to maximize sequential access.\n\nDisk-oriented DBMS\n\nGet Page #2 Execution Engine\n\nInterpret Pointer to\nPage #2 Directory Header Pool Page #2\n2 Update Page #2 BufferMemory\n\nFile Directory Header Header Header Header Header Pages 1 2 3 4 5\nDB\nDisk\n\nDatabase Storage Questions\n\n-How the DBMS represents the database in\nfiles on disk?\n\n-How the DBMS manages its memory and\nmoves data back-and-forth from disk?\n\nFile storage",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 1,
        "page_end": 11,
        "char_len": 2051
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0002",
      "text": "File Directory Header Header Header Header Header Pages 1 2 3 4 5\nDB\nDisk\n\nDatabase Storage Questions\n\n-How the DBMS represents the database in\nfiles on disk?\n\n-How the DBMS manages its memory and\nmoves data back-and-forth from disk?\n\nFile storage\n\n-The DBMS stores a database as one or more\nfiles on disk typically in a proprietary format.\nThe OS does not know anything about the\ncontents of these files.\n-Early systems in the 1980s used custom\nfilesystems on raw block storage.\nSome enterprise DBMSs still support this.\nMost newer DBMSs do not do this.\n\nStorage manager\n\n-Responsible for maintaining a database's\nfiles.\n-Organizes the files as a collection of pages.\nTracks data read/written to pages.\nTracks the available space.\n-A DBMS typically does not maintain multiple\ncopies of a page on disk.\n\nDatabase pages\n\n-A page is a fixed-size block of data.\nIt can contain tuples, meta-data, indexes, log\nrecords…\nMost systems do not mix page types.\nSome systems require a page to be self-contained.\n-Each page is given a unique identifier (page ID).\nA page ID could be unique per DBMS instance, per\ndatabase, or per table.\nThe DBMS uses an indirection layer to map page IDs\nto physical locations.\n\nDatabase pages\n\n-Default DB Page Sizes-There are three different notions\nof \"pages\" in a DBMS:\nHardware Page (usually 4KB)\nOS Page (usually 4KB, x64 2MB/1GB)\n4KB Database Page (512B-32KB)\n-A hardware page is the largest\nblock of data that the storage device 8KB\ncan guarantee failsafe writes.\n-DBMSs that specialize in read-only\n16KB workloads have larger page sizes.\n\nPage storage architecture\n\n-Different DBMSs manage pages in files on disk in\ndifferent ways.\nHeap File Organization\nTree File Organization\nSequential / Sorted File Organization (ISAM)\nHashing File Organization\n-At this point in the hierarchy, we do not need to\nknow anything about what is inside of the pages.\n\nDB File Structures",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 9,
        "page_end": 16,
        "char_len": 1918
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0003",
      "text": "DB File Structures\n\n-Unordered heap files\nRecords placed arbitrarily across pages\nAs file shrinks/grows, pages (de)allocated\nSuitable when typical access is a full scan of all records\n-Clustered heap files\nGroup data into blocks to enable fast lookup and efficient modifications\n-Sorted files\nPages and records are in strict sorted order\nBest for retrieval in order, or when a range of records is needed\n-Index files\nB+ trees, linear hashing, extensible hashing, ……\nMay contain records or point to records in other files\n\nHeap File\nOrganization\n\nHeap file\n\n-A heap file is an unordered collection of pages\nwith tuples that are stored in random order.\n-Need additional meta-data to track location\nof files and free space availability.\n-Heap files has one special header page\nTwo pointers for free space and data\n-Location of the heap file and the header page\nare saved in DB catalog\n\nHeap file\n\nOffset = Page# ×PageSize\n\nFile\nPage Page Page Page Page\nGet Page #2 … 00 01 02 03 04 Database\n\nHeap file: Page directory\n\n-The DBMS maintains special pages that tracks the\nlocation of data pages in the database files.\nOne entry per database object.\nMust make sure that the directory pages are in\nsync with the data pages.\n-DBMS also keeps meta-data about pages' contents:\nAmount of free space per page.\nList of free / empty pages.\nPage type (data vs. meta-data).\n\nPage directory\n\nFile Location ⇒Page# ×PageSize\n\nFile Page Page Page Page Page File Page Page Page Page Page\n00 01 02 03 04 … 20 21 22 23 24 …\nDB DB\nGet Page\nPage #23 Directory\nFile Page Page Page Page Page File Page Page Page Page Page\n10 11 12 13 14 … 40 41 42 43 44 …\nDB DB\n\nHeap file: Page directory\n\nFile1 File2\nPage0 Page0\nDirectory\n\nData Table X Data\n\nIndex Y\nPage1 Page1\nTable Z\nData Data ⋮\n\n⋮ ⋮\n\nUnordered Heap Files With Page Directories\n\n-Page directories, with multiple header pages, each\nencoding\nA pointer to page\n# free bytes on the page\n-Header pages are accessed often,\nso likely deployed in cache\n-Finding a page to fit a record can be done efficiently\nOne header page load reveals free space of MANY\n\n23 pages\n\nPage Layout\n\nPage header",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 16,
        "page_end": 25,
        "char_len": 2128
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0004",
      "text": "23 pages\n\nPage Layout\n\nPage header\n\n-Every page contains a header of meta-data about the page's contents.\nPage Size\nPage\nChecksum\nDBMS Version Header\nTransaction Visibility\nCompression / Encoding Meta-data\nData Schema Information\nData Summary / Sketches\n-Some systems require pages to be self-contained (e.g., Oracle).\nData and its associated Metadata should reside together on the same physical\nstorage unit.\n\nPage layout\n\n-For any page storage architecture, we now\nneed to decide how to organize the data inside\nof the page.\nassuming that we are only storing tuples in\na row-oriented storage model.\nTuple- Log- Indexoriented structured organized\nStorage Storage Storage\n\nTuple-oriented storage\n\n-How to store tuples in a page?\nKeep track of the number of tuples in a\npage and then just append a new tuple to\n\n→\n→ the end.\nWhat happens if we delete a tuple?\nWhat happens if we have a variablelength attribute?\n\nTuples in a page\n\nPage Page Page Page\n\nTuples# = 0 Tuples# = 3 Tuples# = 2 Tuples# = 3\n\nTuple #1 Tuple #1 Tuple #1\n\nStore Delete Insert\nTuple #2 Tuple #4 Tuples Tuples Tuples\n\nTuple #3 Tuple #3 Tuple #3\n\nVariable-length attribute\n\n-The most common layout scheme is called\nslotted pages.\n-The slot array maps \"slots“ to the tuples'\nstarting position offsets.\n-The header keeps track of:\nThe # of used slots\nThe offset of the starting location of the last\nslot used.\n\nSlotted pages\nSlot Array\n\n1 2 3 4 5 6 7\nHeader\n\nTuple #4 Tuple #3\n\nTuple #2 Tuple #1\n\nFixed- and Var-length Tuple Data\n\nDelete tuple\n\nSlot Array Slot Array Slot Array\n1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7\nHeader Header Header\n\nTuple #4 Tuple #3 Tuple #4 Tuple #4\n\nTuple #2 Tuple #1 Tuple #2 Tuple #1 Tuple #2 Tuple #1\n\nRecord IDs\n\n-The DBMS assigns each logical tuple a unique\nrecord identifier that represents its physical\nlocation in the database.\nFile Id, Page Id, Slot #\nMost DBMSs do not store ids in tuple.\nSQLite uses ROWID as the true primary key and\nstores them as a hidden attribute.\n-Applications should never rely on these IDs to\nmean anything.\n\nRecord IDs\n\nCTID ROWID %%physloc%% ROWID\n(6-bytes) (8-bytes) (8-bytes) (10-bytes)\n\nTuple Layout\n\nTuple layout\nTuple\nHeader Attribute Data",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 23,
        "page_end": 35,
        "char_len": 2194
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0005",
      "text": "Record IDs\n\nCTID ROWID %%physloc%% ROWID\n(6-bytes) (8-bytes) (8-bytes) (10-bytes)\n\nTuple Layout\n\nTuple layout\nTuple\nHeader Attribute Data\n\n-A tuple is essentially a sequence of bytes.\nThese bytes do not have to be contiguous.\n-It is the job of the DBMS to interpret those bytes into\nattribute types and values.\n-Each tuple is prefixed with a header that contains metadata about it.\nVisibility info (concurrency control)\nBit Map for NULL values.\n\nTuple data\n-Attributes are typically stored Tuple\nin the order that you specify\nHeader a b c d e\nthem when you create the\ntable. CREATE TABLE foo (\nINT PRIMARY KEY,-This is done for software a\nb INT NOT NULL,\nengineering reasons (i.e., c INT,\nd DOUBLE, simplicity).\ne FLOAT\n);-However, it might be more\nefficient to lay them out\ndifferently.\n\nDenormalized tuple data\n\n-DBMS can physically\ndenormalize (e.g., \"pre-join\")\nCREATE TABLE foo ( related tuples and store them\na INT PRIMARY KEY,\ntogether in the same page. b INT NOT NULL,\nPotentially reduces the );\namount of I/O for common\nCREATE TABLE bar (\nworkload patterns. c INT PRIMARY KEY,\na INT REFERENCES foo (a), Can make updates more\n); expensive.\n\nDenormalized tuple data\n\nfoo\nSELECT * FROM foo JOIN bar\nHeader a b\nON foo.a = bar.a;\n\nbar\nHeader c a\nfooHeader c a\nHeader a b c c c . . .Header c a\n\nfoo bar\n\nDenormalized tuple data\n\n-Not a new idea.\nIBM System R did this in\nthe 1970s.\nSeveral NoSQL DBMSs do\nthis without calling it\nphysical denormalization.\n\nOther Memory\nPools\n\nOther memory pools\n\n-The DBMS needs memory for things other than\njust tuples and indexes.\n-These other memory pools may not always\nbacked by disk, depends on implementation.\nSorting + Join Buffers\nQuery Caches\nMaintenance Buffers\nLog Buffers\nDictionary Caches\n\nBuffer pool organization\n\n-Memory region organized as an Buffer\narray of fixed-size pages. Pool\n\nframe1page1-An array entry is called a frame.\nframe2page3\n-When the DBMS requests a frame3\nframe4 page, an exact copy is placed into\none of these frames.\n\npage1 page2 page3 page4-Dirty pages are buffered and not\nwritten to disk immediately On-Disk File\nWrite-Back Cache\n\nBuffer pool meta-data",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 33,
        "page_end": 43,
        "char_len": 2142
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0006",
      "text": "frame1page1-An array entry is called a frame.\nframe2page3\n-When the DBMS requests a frame3\nframe4 page, an exact copy is placed into\none of these frames.\n\npage1 page2 page3 page4-Dirty pages are buffered and not\nwritten to disk immediately On-Disk File\nWrite-Back Cache\n\nBuffer pool meta-data\n\n-The page table keeps track of Page Buffer\npages Table Pool\n\npage1 frame1page1-that are currently in memory. meta-data\npage3\nmeta-data frame2page3 Usually a fixed-size hash table\nframe3page2 protected with latches to ensure page2\nmeta-data frame4 thread-safe access.\n-Additional meta-data per page:\n\npage1 page2 page3 page4 Dirty Flag\nPin/Reference Counter On-DiskFile\nAccess Tracking Information\n\nPage table vs. Page directory\n\nPage directory Page table\n\n-The mapping from -The mapping from\npage ids to page page ids to a copy of\nlocations in the the page in buffer\ndatabase files. pool frames.\nAll changes must be This is an in-memory\nrecorded on disk to data structure that\nallow the DBMS to does not need to be\nfind on restart. stored on disk.\n\nWhen a Page is Requested …\n\n1. If requested page is NOT in the buffer pool:\n1. Choose an un-pinned (pin count = 0) frame for replacement\n2. If frame is dirty, write current page to disk, mark as “clean”\n3. Read requested page into frame\n2. Pin the page and return its address\n-If requests can be predicted (e.g., sequential scans), pages\ncan be prefetched\nSeveral pages at a time\n-When the buffer pool is full …\nNeed to evict an existing page\nNeed a page replacement policy\n\nBuffer\nReplacement\n\nBuffer Replacement Policies\n\n-When the DBMS needs to free up a frame to\nmake room for a new page, it must decide\nwhich page to evict from the buffer pool.\n-Goals:\nCorrectness\nAccuracy\nSpeed\nMeta-data overhead\n\nLEAST-RECENTLY USED\n-Maintain a single timestamp of when\neach page was last accessed.\n-When the DBMS needs to evict a page, page0\nselect the one with the oldest\ntimestamp. Q1 page1\nKeep the pages in sorted order to reduce page2\nthe search time on eviction.\npage3\n\npage4 page1page0 page0page1 page2\n\nNewest ← Oldest page5\nLRU List\n\nLRU Replacement Policy",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 42,
        "page_end": 49,
        "char_len": 2116
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0007",
      "text": "page4 page1page0 page0page1 page2\n\nNewest ← Oldest page5\nLRU List\n\nLRU Replacement Policy\n\n-Least Recently Used (LRU)\nPinned frame: not available to replace\nTrack time for each frame last unpinned (end of use)\nReplace the frame which was least recently used\n\nNeed to “find min” on the last used attributed (priority queue)\nVery common policy: intuitive and simple\n\nGood for repeated access to popular pages (temporal locality)\n-Sequential Scan + LRU\nLead to sequential flooding\n\n0% hit rate in cache\nRepeated sequential scan is very common in DB workload\n\nNested-loop join\n\nBetter Policies: LRU-K\n\n-Track the history of last K references to each page as timestamps\nand compute the interval between subsequent accesses.\nCan distinguish between reference types\n-Use this history to estimate the next time that page is going to be\naccessed.\nReplace the page with the oldest \"K-th\" access.\nBalances recency vs. frequency of access.\nMaintain an ephemeral in-memory cache for recently evicted pages to\nprevent them from always being evicted.\n\nExample: LRU-3\n\n-Page A\nAccess at T=10 , T=40 , T=80\n\n-Page B\nAccess at T=5 , T=30 , T=90\n\nMySQL approximate LRU-K\n\n-Single LRU linked list but with two entry\npoints (\"old\" vs \"young\").\nNew pages are always inserted to the head\nof the old list.\nIf pages in the old list is accessed again,\nthen insert into the head of the young list.\n\nMySQL approximate LRU-K\n\nDisk Pages\n\npage0\n\nQ1 page1\nHEAD YoungList HEAD OldList\npage2\npage4 page5 page9 page3 page6 page2 page8\npage3\n\npage4\nNewest ←Oldest\npage5\n\nMySQL approximate LRU-K\n\nDisk Pages\n\npage0\n\nQ1 page1\nHEAD YoungList HEAD OldList\npage2\npage4 page5 page9 page3 page1 page6 page2\npage3\n\npage4\nNewest ←Oldest\npage5\n\nMySQL approximate LRU-K\n\nDisk Pages\n\npage0\n\nQ2 page1\nHEAD YoungList HEAD OldList\npage2\npage4 page5 page9 page3 page1 page6 page2\npage3\n\npage4\nNewest ←Oldest\npage5\n\nMySQL approximate LRU-K\n\nDisk Pages\n\npage0\n\nQ2 page1\nHEAD YoungList HEAD OldList\npage2\npage1 page4 page5 page9 page3 page6 page2\npage3\n\npage4\nNewest ←Oldest\npage5\n\nBetter policies: Localization",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 48,
        "page_end": 57,
        "char_len": 2076
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0008",
      "text": "page4\nNewest ←Oldest\npage5\n\nMySQL approximate LRU-K\n\nDisk Pages\n\npage0\n\nQ2 page1\nHEAD YoungList HEAD OldList\npage2\npage1 page4 page5 page9 page3 page6 page2\npage3\n\npage4\nNewest ←Oldest\npage5\n\nBetter policies: Localization\n\n-The DBMS chooses which pages to evict on a per\nquery basis.\n-This minimizes the pollution of the buffer pool\nfrom each query.\nKeep track of the pages that a query has accessed.\n-Example\nPostgres assigns a limited number of buffer pool\npages to a query and uses it as a circular ring buffer.\n\nBetter policies: Priority hints\n\n-The DBMS knows about the context of each\npage during query execution.\n\n-It can provide hints to the buffer pool on\nwhether a page is important or not.\n\nDirty Pages\n\n-Fast Path\nIf a page in the buffer pool is not dirty, then the\nDBMS can simply \"drop\" it.\n-Slow Path\nIf a page is dirty, then the DBMS must write back\nto disk to ensure that its changes are persisted.\n-Trade-off between fast evictions versus dirty\nwriting pages that will not be read again in the\nfuture.\n\nBackground writing\n\n-The DBMS can periodically walk through the\npage table and write dirty pages to disk.\n-When a dirty page is safely written, the\nDBMS can either evict the page or just unset\nthe dirty flag.\n-Need to be careful that the system doesn't\nwrite dirty pages before their log records are\nwritten.\n\nBuffer Pool\nOptimizations\nMultiple Buffer Pools\nPre-Fetching\nScan Sharing\nBuffer Pool Bypass\n\nMultiple buffer pools\n-The DBMS does not always have a single\nbuffer pool for the entire system.\nMultiple buffer pool instances\nPer-database buffer pool\nPer-page type buffer pool\n-Partitioning memory across multiple pools\nhelps reduce latch contention and improve\nlocality.\nAvoids contention on LRU tracking\nmeta-data.\n\nMultiple buffer pools\n\n-Object Id GET RECORD #123 Q1\nEmbed an object\nHASH(123) % n <ObjectId, PageId, SlotNum> identifier in record\nids and then\nmaintain a mapping\nfrom objects to\nspecific buffer pools.\n-Hashing\nHash the page id to\nselect which buffer\npool to access. Buffer Pool #1 Buffer Pool #2\n\nPre-fetching",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 55,
        "page_end": 64,
        "char_len": 2066
      }
    },
    {
      "chunk_id": "ADB_Lec02_c0009",
      "text": "-Object Id GET RECORD #123 Q1\nEmbed an object\nHASH(123) % n <ObjectId, PageId, SlotNum> identifier in record\nids and then\nmaintain a mapping\nfrom objects to\nspecific buffer pools.\n-Hashing\nHash the page id to\nselect which buffer\npool to access. Buffer Pool #1 Buffer Pool #2\n\nPre-fetching\n\n-The DBMS can also prefetch pages based on a\nquery plan.\nExamples: Sequential vs. Index Scans\n-Some DBMS prefetch to fill in empty frames\nupon start-up.",
      "metadata": {
        "lecture_id": "ADB_Lec02",
        "source_file": "ADB_Lec02.pdf",
        "page_start": 63,
        "page_end": 64,
        "char_len": 445
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\n\nLec 03\nStorage Management [2]\n\nTuple-oriented storage\n\n-Insert a new tuple:\nCheck page directory to find a page with a free slot.\nRetrieve the page from disk (if not in memory).\nCheck slot array to find empty space in page that will fit.\n-Update an existing tuple using its record id:\nCheck page directory to find location of page.\nRetrieve the page from disk (if not in memory).\nFind offset in page using slot array.\nIf new data fits, overwrite existing data.\nOtherwise, mark existing tuple as deleted and insert new version\nin a different page.\n\nProblems\n\n-Fragmentation\nPages are not fully utilized (unusable space, empty slots).\n-Useless Disk I/O\nDBMS must fetch entire page to update one tuple.\n-Random Disk I/O\nWorse case scenario when updating multiple tuples is that\neach tuple is on a separate page.\n-What if the DBMS cannot overwrite data in pages and could\nonly create new pages?\nExamples: Some object stores, HDFS, Google Colossus\n\nLog-structured\nStorage\n\nLog-structured Storage\n\n-Instead of storing tuples in pages and updating them inplace, the DBMS maintains a log that records changes to\ntuples.\nEach log entry represents a tuple PUT/ DELETE\noperation.\nOriginally proposed as log-structure merge trees\n(LSM Trees) in 1996.\n-The DBMS applies changes to an in-memory data\nstructure (MemTable) and then writes out the changes\nsequentially to disk (SSTable).\n\nLog-structured storage\n\nPUTPUTPUTPUT (key101,a1)(key103,c1)(key101,a2)(key102,b1)\n\nMemTable\nMemory\n\nKey\nPUT (key101,a2)\n(Low SSTable\nPUT (key102,b1)\n→\nPUT (key103,c1) Disk High)\n\nLog-structured storage\n\n-Key-value storage that appends log records\non disk to represent changes to tuples (PUT,\nDELETE). SSTable\nEach log record must contain the tuple’s Key DEL (key100)\nunique identifier.\nPUT (key101,a2) (Low\nPut records contain the tuple contents. → PUT (key102,b1)\nDeletes marks the tuple as deleted.\nPUT (key103,c1) High)\n-As the application makes changes to the\ndatabase, the DBMS appends log records to\nthe end of the file without checking previous\nlog records.",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 1,
        "page_end": 7,
        "char_len": 2091
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0002",
      "text": "Log-structured compaction\n-Periodically compact SSTAbles to reduce wasted space and speed\nup reads.\nOnly keep the \"latest\" values for each key using a sort- merge\nalgorithm.\nSSTable SSTable SSTable\n\nDEL (key100) DEL (key100) PUT (key101,a2)\nPUT (key101,a3) PUT (key101,a3) PUT (key102,b1)\nPUT (key102,b2) PUT (key102,b2) DEL (key103)\nPUT (key103,c1) PUT (key103,c1) PUT (key104,d2)\nPUT (key104,d2)\nNewest → Oldest\n\nDiscussion\n\n-Log-structured storage managers are more common\ntoday than in previous decades.\nThis is partly due to the proliferation of RocksDB.\n\n-What are some downsides of this approach?\nWrite-Amplification.\nCompaction is expensive.\n\nIndex-organized\nStorage\n\nObservation\n\n-The two-table storage approach relies on\nindexes to find individual tuples.\n\nSuch indexes are necessary because the tables\nare inherently unsorted.\n\n-But what if the DBMS could keep tuples\nsorted automatically using an index?\n\nIndex-organized Storage\n\n-DBMS stores a table's tuples as the\nvalue of an index data structure.\nStill use a page layout that looks like a\nslotted page.\nTuples are typically sorted in page\nbased on key.\n-B+Tree pays maintenance costs\nupfront, whereas LSMs pay for it later.\n\nIndex-organized Storage\n\nInner\nNodes\n\nkey→ key→ key→\nHeader offset offset offsetLeaf\nNodes\n\nTuple #3 Tuple #2 Tuple #6\n\nTuple storage\n-A tuple is essentially a sequence of bytes prefixed\nwith a header that contains meta-data about it.\n-It is the job of the DBMS to interpret those bytes\ninto attribute types and values.\n-The DBMS's catalogs contain the schema\ninformation about tables that the system uses to\nfigure out the tuple's layout.\nCREATE TABLE foo (\nid INT PRIMARY KEY, unsigned char[]\nvalue BIGINT\nheader id value );\n\nWord-aligned tuples\n\n-All attributes in a tuple must be word aligned\nto enable the CPU to access it without any\nunexpected behavior or additional work.\n\nCREATE TABLE foo (\n32-bits id INT PRIMARY KEY, unsigned char[]\ncdate TIMESTAMP,64-bits\nid cdate c zipc\ncolor CHAR(2),16-bits\nzipcode INT32-bits 64-bit Word 64-bit Word 64-bit Word 64-bit Word\n);",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 8,
        "page_end": 15,
        "char_len": 2073
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0003",
      "text": "CREATE TABLE foo (\n32-bits id INT PRIMARY KEY, unsigned char[]\ncdate TIMESTAMP,64-bits\nid cdate c zipc\ncolor CHAR(2),16-bits\nzipcode INT32-bits 64-bit Word 64-bit Word 64-bit Word 64-bit Word\n);\n\nWord-alignment: Padding\n-Add empty bits after attributes to ensure that\ntuple is word aligned.\n-Essentially round up the storage size of types\nto the next largest word size.\n\nCREATE TABLE foo (\n32-bits id INT PRIMARY KEY,\n00000000 0000\n00000000 000064-bits cdate TIMESTAMP,\n00000000 0000 id Cdate c zipc\n16-bits color CHAR(2), 00000000 0000\n32-bits zipcode INT 64-bitWord 64-bitWord 64-bitWord 64-bitWord\n);\n\nWord-alignment: Reordering\n\n-Switch the order of attributes in the tuples'\nphysical layout to make sure they are aligned.\nMay still have to use padding to fill remaining space.\n\nCREATE TABLE foo (\n32-bits id INT PRIMARY KEY, id cdate c zipc\n64-bits cdate TIMESTAMP,\n16-bits color CHAR(2),\n32-bits zipcode INT 000000000000000000000000\nid zipc cdate c 000000000000\n); 000000000000\n\n64-bitWord 64-bitWord 64-bitWord 64-bitWord\n\nStorage models\n\nStorage models\n-A DBMS's storage model specifies how it physically\norganizes tuples on disk and in memory.\nN-ary Storage Model (NSM)\nDecomposition Storage Model (DSM)\nHybrid Storage Model (PAX)\n-Can have different performance characteristics\nbased on the target workload (OLTP vs. OLAP).\n-Influences the design choices of the rest of the\nDBMS.\n\nN-ary Storage\nModel (NSM)\n\nN-ary Storage Model (NSM)\n\n-The DBMS stores (almost) all attributes for a\nsingle tuple contiguously in a single page.\nAlso commonly known as a row store\n-Ideal for OLTP workloads where queries are\nmore likely to access individual entities and\nexecute write-heavy workloads.\n-NSM database page sizes are typically some\nconstant multiple of 4 KB hardware pages.\n\nNSM: Physical Organization\n\n-A disk-oriented NSM system stores a tuple's\nfixed-length and variable-length attributes\ncontiguously in a single slotted page.\n\n-The tuple's record id (page#, slot#) is how\nthe DBMS uniquely identifies a physical tuple.\n\nNSM: Physical Organization\n\nCol A Col B Col C\nRow#0 a0 b0 c0\nheader\nRow#1 a1 b1 c1",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 15,
        "page_end": 23,
        "char_len": 2116
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0004",
      "text": "-The tuple's record id (page#, slot#) is how\nthe DBMS uniquely identifies a physical tuple.\n\nNSM: Physical Organization\n\nCol A Col B Col C\nRow#0 a0 b0 c0\nheader\nRow#1 a1 b1 c1\n\nRow#2 a2 b2 c2 Page\nRow#3 a3 b3 c3 header a5 b5 c5 header a4\nRow#4 a4 b4 c4 b4 c4 header a3 b3 c3\nRow#5 a5 b5 c5 header a2 b2 c2 header a1 Database\nb1 c1 header a0 b0 c0\n\nNSM: OLTP example\n\nSELECT * FROM useracct\nWHERE userName = ?\nIndex AND userPass = ?\n\nINSERT INTO useracct\nVALUES (?,?,…?)\nNSM Disk Page\n\nheader userID userName userPass hostname lastLogin\n\nheader userID userName userPass hostname lastLogin\n\nheader userID userName userPass hostname lastLogin File\nheaderheader userID- userName- userPass- hostname- lastLogin-Disk Database\n\nNSM: OLAP example\n\nSELECT COUNT(U.lastLogin),\nEXTRACT(month FROM U.lastLogin) AS month\nFROM useracct AS U\nWHERE U.hostname LIKE '%.gov'\nGROUP BY EXTRACT(month FROM U.lastLogin)\n\nNSM Disk Page\n\nheader userID userName userPass hostname lastLogin\n\nheader userID userName userPass hostname lastLogin\n\nheader userID userName userPass hostname lastLogin File\nheader userID userName userPass hostname lastLoginDisk Database\n\nUseless Data!\n\nNSM: summary\n-Advantages\nFast inserts, updates, and deletes.\nGood for queries that need the entire tuple (OLTP).\nCan use index-oriented physical storage for\nclustering.\n-Disadvantages\nNot good for scanning large portions of the table\nand/or a subset of the attributes.\nTerrible memory locality in access patterns.\nNot ideal for compression because of multiple value\ndomains within a single page.\n\nDecomposition\nStorage Model\n(DSM)\n\nDecomposition Storage Model (DSM)\n\n-Store a single attribute for all tuples\ncontiguously in a block of data.\nAlso known as a \"column store\"\n-Ideal for OLAP workloads where read-only\nqueries perform large scans over a subset of\nthe table’s attributes.\n-DBMS is responsible for combining/splitting\na tuple's attributes when reading/writing.\n\nDSM: Physical Organization",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 22,
        "page_end": 29,
        "char_len": 1959
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0005",
      "text": "DSM: Physical Organization\n\n-Store attributes and meta-data (e.g., nulls) in\nseparate arrays of fixed- length values.\nMost systems identify unique physical\ntuples using offsets into these arrays.\nNeed to handle variable-length values.\n-Maintain separate pages per attribute with a\ndedicated header area for meta-data about\nentire column.\n\nDSM: Physical Organization\n\n#1 header Null bitmap\nPage a0 a1 a2 a3 a4 a5\nCol A Col B Col C\nRow#0 a0 b0 c0\n#2 header Null bitmap\nRow#1 a1 b1 c1\nb0 b1 b2 b3 b4 b5\nRow#2 a2 b2 c2 Page\nRow#3 a3 b3 c3\n\nRow#4 a4 b4 c4\nheader Null bitmap\nRow#5 a5 b5 c5 #3 c0 c1 c2 c3 c4\nc5 Page\n\nDSM: OLAP example\n\nSELECT COUNT(U.lastLogin),\nEXTRACT(month FROM U.lastLogin) AS month\nFROM useracct AS U\nWHERE U.hostname LIKE '%.gov'\nGROUP BY EXTRACT(month FROM U.lastLogin)\n\nDSM Disk Page\n\nhostname hostname hostname header\nuserID lastLogin hostname hostname hostname hostname hostname\n\nhostname hostname hostname hostname hostname\n\nhostname hostname hostname hostname hostnameDisk File userName Database userPass\n\nDSM: OLAP example\n\nSELECT COUNT(U.lastLogin),\nEXTRACT(month FROM U.lastLogin) AS month\nFROM useracct AS U\nWHERE U.hostname LIKE '%.gov'\nGROUP BY EXTRACT(month FROM U.lastLogin)\n\nDSM Disk Page\n\nlastlogin lastlogin lastlogin header\nuserID lastLogin lastlogin lastlogin lastlogin lastlogin lastlogin\n\nlastlogin lastlogin lastlogin lastlogin lastlogin\n\nlastlogin lastlogin lastlogin lastlogin lastloginDisk File userName Database userPass\n\nDSM: Variable-length Data\n\n-Padding variable-length fields to ensure they\nare fixed-length is wasteful, especially for\nlarge attributes.\n-A better approach is to use dictionary\ncompression to convert repetitive variablelength data into fixed- length values (typically\n32-bit integers).\n\nDSM: Summary\n\n-Advantages\nReduces the amount wasted I/O per query because\nthe DBMS only reads the data that it needs.\nFaster query processing because of increased locality\nand cached data reuse.\nBetter data compression.\n-Disadvantages\nSlow for point queries, inserts, updates, and deletes\nbecause of tuple splitting/stitching/reorganization.\n\nOBSERVATION",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 29,
        "page_end": 35,
        "char_len": 2113
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0006",
      "text": "OBSERVATION\n\n-OLAP queries almost never access a single column in\na table by itself.\nAt some point during query execution, the DBMS\nmust get other columns and stitch the original tuple\nback together.\n-But we still need to store data in a columnar format to\nget the storage + execution benefits.\n-We need columnar scheme that still stores attributes\nseparately but keeps the data for each tuple physically\nclose to each other.\n\nPartition\nAttributes Across\n(PAX)\n\nPAX Storage Model\n\n-Partition Attributes Across (PAX) is a hybrid\nstorage model that vertically partitions\nattributes within a database page.\nExamples: Parquet, ORC, and Arrow.\n-The goal is to get the benefit of faster\nprocessing on columnar storage while\nretaining the spatial locality benefits of row\nstorage.\n\nPAX: Physical Organization\n\n-Horizontally partition data into row groups.\n-Then vertically partition their attributes into\ncolumn chunks.\n-Global meta-data directory contains offsets\nto the file's row groups.\nThis is stored in the footer if the file is\nimmutable (Parquet, Orc).\n-Each row group contains its own meta-data\nheader about its contents.\n\nPAX: Physical Organization\n\nCol A Col B Col C\nRow#0 a0 b0 c0\n\nRow#1 a1 b1 c1 Row Group Meta-data ColumnRow#2 a2 b2 c2 Chunk Group a0 a1 a2 b0 b1 b2\n\nRow#3 a3 b3 c3 Row c0 c1 c2\nRow#4 a4 b4 c4\nRow Group Meta-dataRow#5 a5 b5 c5 Group a3 a4 a5 b3 b4 b5 File\nRow c3 c4 c5\nPAX\nFile Meta-data\n\nObservation\n\n-I/O is the main bottleneck if the DBMS fetches\ndata from disk during query execution.\n-The DBMS can compress pages to increase the\nutility of the data moved per I/O operation.\n-Key trade-off is speed vs. compression ratio\nCompressing the database reduces DRAM\nrequirements.\nIt may decrease CPU costs during query execution.\n\nDatabase\nCompression\n\nDatabase Compression\n-Goal #1: Must produce fixed-length values.\nOnly exception is var-length data stored in separate\npool.\n-Goal #2: Postpone decompression for as long as\npossible during query execution.\nAlso known as late materialization.\n-Goal #3: Must be a lossless scheme.\nPeople (typically) don't like losing data.\nAny lossy compression must be performed by\napplication.",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 35,
        "page_end": 42,
        "char_len": 2159
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0007",
      "text": "Compression Granularity\n-Block-level\nCompress a block of tuples for the same table.\n-Tuple-level\nCompress the contents of the entire tuple (NSM-only).\n-Attribute-level\nCompress a single attribute within one tuple (overflow).\nCan target multiple attributes for the same tuple.\n-Column-level\nCompress multiple values for one or more attributes\nstored for multiple tuples (DSM-only).\n\nNAÏVE Compression\n-Compress data using a general-purpose\nalgorithm.\n-Scope of compression is only based on the\ndata provided as input.\nLZO (1996), LZ4 (2011), Snappy (2011), Oracle OZIP\n(2014), Zstd (2015)\n-Considerations\nComputational overhead\nCompress vs. decompress speed.\n\nMYSQL Innodb Compression\n\nBuffer Pool Database File\n\nmod log mod logWriteRead [1,2,4,8] KB Compressed Page0 Compressed Page0\n\nmod log\nRead Compressed Page1 Uncompressed\n16 KB\nPage0 mod log\nCompressed Page2\n\nNAÏVE Compression\n\n-The DBMS must decompress data first before\nit can be read and (potentially) modified.\nThis limits the \"scope\" of the compression\nscheme.\n-These schemes also do not consider the highlevel meaning or semantics of the data.\n\nObservation\n\n-Ideally, we want the DBMS to operate on\ncompressed data without decompressing it\nfirst.\nDatabase\nMagic!\nSELECT * FROM users SELECT * FROM users\nWHERE name = 'Andy' WHERE name = XX\n\nNAME SALARY NAME SALARY\nAndy 99999 XX AA\nJignesh 88888 YY BB\n\nCompression Granularity\n-Block-level\nCompress a block of tuples for the same table.\n-Tuple-level\nCompress the contents of the entire tuple (NSM-only).\n-Attribute-level\nCompress a single attribute within one tuple (overflow).\nCan target multiple attributes for the same tuple.\n-Column-level\nCompress multiple values for one or more attributes\nstored for multiple tuples (DSM-only).\n\nColumnar\nCompression\n\nColumnar Compression\n\n-Run-length Encoding\n-Bit-Packing Encoding\n-Bitmap Encoding\n-Delta / Frame-of-Reference Encoding\n-Incremental Encoding\n-Dictionary Encoding\n\nRun-length encoding",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 43,
        "page_end": 51,
        "char_len": 1965
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0008",
      "text": "Columnar\nCompression\n\nColumnar Compression\n\n-Run-length Encoding\n-Bit-Packing Encoding\n-Bitmap Encoding\n-Delta / Frame-of-Reference Encoding\n-Incremental Encoding\n-Dictionary Encoding\n\nRun-length encoding\n\n-Compress runs of the same value in a single\ncolumn into triplets:\nThe value of the attribute.\nThe start position in the column segment.\nThe # of elements in the run.\n-Requires the columns to be sorted\nintelligently to maximize compression\nopportunities.\n\nRun-length Encoding\n\nCompressed Original\nData Data\n\nid isDead id isDead\n1 (Y,0,3) 1 Y\n2 (N,3,1)\nSELECT isDead, COUNT(*) 2 Y\n3 (Y,4,1) 3 Y\nFROM users 4 (N,5,1) 4 N\nGROUP BY isDead 6 (Y,6,2) 6 Y\n\n7 N 7 RLETriplet\n8 - Value 8 Y\n- Offset 9 9 Y - Length\n\nRun-length Encoding\n\nSorted Data Compressed Data\n\nid isDead id isDead\n1 Y 1 (Y,0,6)\n2 Y 2 (N,7,2)\n3 Y 3\n6 Y 6\n8 Y 8\n9 Y 9\n4 N 4\n7 N 7\n\nBit Packing\n\nOriginal\nData-If the values for an integer attribute is\nsmaller than the range of its given int32 13\ndata type size, then reduce the number 191\n56 of bits to represent each value.\n92\n81\n120\n-Use bit-shifting tricks to operate on 231\n172 multiple values in a single word.\n\nBit Packing\n\nOriginal Original\nData Data\nint32 int32\n13 00000000 00000000 00000000 00001101 13 00001101\n191 00000000 00000000 00000000 10111111 191 10111111\n56 00000000 00000000 00000000 00111000 56 00111000\n92 00000000 00000000 00000000 01011100 92 01011100\n81 00000000 00000000 00000000 01010001 81 01010001\n120 00000000 00000000 00000000 01111000 120 01111000\n231 00000000 00000000 00000000 11100111 231 11100111\n172 00000000 00000000 00000000 10101100 172 10101100\n\nCompressed: Original:\n8 × 32-bits = 256 bits 8 × 8-bits = 64 bits\n\nPatching / mostly Encoding\n\n-A variation of bit packing for when an\nattribute's values are \"mostly\" less than the\nlargest size, store them with smaller data type.\nThe remaining values that cannot be\ncompressed are stored in their raw form.\n\nPatching / mostly Encoding\n\nOriginal Compressed Data\nData\nint32 mostly8\noffset value\n13 13\n3 99999999\n191 191\n999999999 XXX\n92 92\n81 81\nCompressed: 120 120\n231 231 (8 × 8-bits) +16-bits + 32-bits\n172 172 = 112 bits\nOriginal:\n8 × 32-bits = 256 bits\n\nBitmap Encoding",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 49,
        "page_end": 58,
        "char_len": 2178
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0009",
      "text": "Patching / mostly Encoding\n\nOriginal Compressed Data\nData\nint32 mostly8\noffset value\n13 13\n3 99999999\n191 191\n999999999 XXX\n92 92\n81 81\nCompressed: 120 120\n231 231 (8 × 8-bits) +16-bits + 32-bits\n172 172 = 112 bits\nOriginal:\n8 × 32-bits = 256 bits\n\nBitmap Encoding\n\n-Store a separate bitmap for each unique\nvalue for an attribute where an offset in the\nvector corresponds to a tuple.\nThe ith position in the Bitmap corresponds to the\nith tuple in the table.\nTypically segmented into chunks to avoid\nallocating large blocks of contiguous memory.\n-Only practical if the value cardinality is low.\n-Some DBMSs provide bitmap indexes.\n\nBitmap encoding\n\nCompressedOriginal Data\nisDead Data 2 × 8-bits id isDead\nid Y N = 16-bits 1 Y\n1 1 0\n2 Y\n2 1 0\n3 Y\n3 1 0\n4 N 8 × 2-bits 4 0 1\n6 Y = 16-bits 6 1 0\n7 N\n7 0 1\n8 Y\n8 1 0\n9 Y\n9 1 0\nOriginal: Compressed:\n8 × 8-bits = 64 bits 16-bits +16-bits = 32-bits\n\nBitmap encoding: Example\n\n-Assume we have 10 million tuples. CREATE TABLE customer (\n43,000 zip codes in the US. id INT PRIMARY KEY,\n10000000 32-bits = 40 MB name VARCHAR(32), ×\nemail VARCHAR(64), 10000000 43000 = 53.75 GB × address VARCHAR(64),\n-Every time the application inserts a zip_code INT\nnew tuple, the DBMS must extend );\n43,000 different bitmaps.\n-There are compressed data\nstructures for sparse data sets:\nRoaring Bitmaps\n\nDelta Encoding\n\n-Recording the difference between values that\nfollow each other in the same column.\n\n→\nStore base value in-line or in a separate look-up\ntable.\nCombine with RLE to get even better\ncompression ratios.\n-Frame-of-Reference Variant: Use global min\nvalue.\n\nDelta Encoding\n\nOriginal Compressed Compressed\nData Data Data\ntime64 temp time64 temp temp\n12:00 99.5 12:00 99.5 99.5 time64\n12:01 99.4 +1 -0.1 -0.1 12:00\n12:02 99.5 +1 +0.1 +0.1 (+1,4)\n12:03 99.6 +1 +0.1 +0.1\n12:04 99.4 +1 -0.2 -0.2\n\n5 × 64bits 64bits + (4 × 16bits) 64bits + (2 × 16bits)\n= 320 bits = 128 bits = 96 bits",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 57,
        "page_end": 62,
        "char_len": 1925
      }
    },
    {
      "chunk_id": "ADB_Lec03_c0010",
      "text": "5 × 64bits 64bits + (4 × 16bits) 64bits + (2 × 16bits)\n= 320 bits = 128 bits = 96 bits\n\nDictionary Compression\n-Replace frequent values with smaller fixed-length\ncodes and then maintain a mapping (dictionary) from\nthe codes to the original values\nTypically, one code per attribute value.\nMost widely used native compression scheme in DBMSs.\n-The ideal dictionary scheme supports fast encoding\nand decoding for both point and range queries.\nEncode/Locate: For a given uncompressed value,\nconvert it into its compressed form.\nDecode/Extract: For a given compressed value, convert\nit back into its original form.\n\nDictionary: Order-preserving\n\n-The encoded values need to support the\nsame collation as the original values.\n\nSELECT * FROM users SELECT * FROM users\nWHERE name LIKE 'And%' WHERE name BETWEEN 10 AND 20\n\nOriginal Data Compressed Data\n\nname name value code\nAndrea 10 Andrea 10\nMr.Pickles 40 Andy 20\nAndy 20 Jignesh 30 Sorted\n30 Jignesh Mr.Pickles 40 Dictionary\nMr.Pickles 40\n\nOrder-preserving encoding\n\nSELECT name FROM users Still must perform\nscan on column WHERE name LIKE 'And%'\n\nSELECT DISTINCT name FROM users Only need to\naccess dictionary WHERE name LIKE 'And%'\n\nOriginal Data Compressed Data\n\nname name value code\nAndrea 10 Andrea 10\nMr.Pickles 40 Andy 20\nAndy 20 Jignesh 30 Sorted\n30 Jignesh Mr.Pickles 40 Dictionary\nMr.Pickles 40\n\nConclusion\n\n-It is important to choose the right storage model\nfor the target workload:\nOLTP → Row Store\nOLAP → Column Store\n-DBMSs can combine different approaches for\neven better compression.\n-Dictionary encoding is probably the most useful\nscheme because it does not require pre-sorting.",
      "metadata": {
        "lecture_id": "ADB_Lec03",
        "source_file": "ADB_Lec03.pdf",
        "page_start": 62,
        "page_end": 66,
        "char_len": 1647
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\nLec 04\nIndexing Structures and\nImplementation\n\nIndex\n\n-A data structure that improves the speed of\ndata retrieval operations on a database table.\n-Built on one or more columns of a table and\nstore a sorted copy of the indexed data along\nwith pointers to the corresponding rows in the\nmain table.\nExample: B+Tree\n\nTypes of indexes\n\nSparse Dense\n\n-One entry per record-One entry per data block\n-Requires data to be -Data do not have to be\nsorted sorted\n-Identifies the first -Can tell if a given\nrecord of the block record exists without\nFaster access accessing the file\n\nIndexes based on primary keys\n\n-Each key value corresponds to a specific\nrecord\n-Two cases to consider:\nTable is sorted on its primary key\nCan use a sparse index\nTable is either non-sorted or sorted on another\nfield\nMust use a dense index\n\nSparse Index\n\n.Alan Ahmed … …\n.Dana Amita … …\n.Gina Brenda … …\nCarlos … …\n\nDana … …\nDino … …\nEmily … …\nFrank … …\n\nDense Index\n\nAhmed Ahmed … …\nAmita Frank … …\nBrenda Brenda … …\nCarlos Dana … …\nDana\nDino Emily … …\nEmily Dino … …\nFrank Carlos … …\nAmita … …\n\nIndexes based on other fields\n\n-Each key value may correspond to more than\none record\nclustering index\n-Two cases to consider:\nTable is sorted on the field\nCan use a sparse index\nTable is either non-sorted or sorted on another\nfield\nMust use a dense index\n\nSparse clustering index\n\n.Austin Ahmed Austin…\n.Dallas Frank Austin…\n.Laredo Brenda Austin…\nDana Dallas …\nEmily Dallas …\nDino Dallas …\nCarlos Laredo…\nAmita Laredo…\n\nDense clustering index\n\nAustin Ahmed Austin…Austin Amita Laredo…Austin Brenda Austin…\nDallas Carlos Laredo…Dallas\nDana DallasDallas …\nDino DallasLaredo …\nEmily DallasLaredo …\nFrank Austin…\n\nAnother realization\n\nAustin Ahmed Austin…\n.Dallas Amita Laredo…\n.Laredo Brenda Austin…\nCarlos Laredo…\nDana Dallas … We save space Dino Dallas … and add one extra\nEmily Dallas … level of indirection\nFrank Austin…\n\nB-trees and B+\ntrees\n\nMotivation",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 1,
        "page_end": 12,
        "char_len": 1971
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0002",
      "text": "Another realization\n\nAustin Ahmed Austin…\n.Dallas Amita Laredo…\n.Laredo Brenda Austin…\nCarlos Laredo…\nDana Dallas … We save space Dino Dallas … and add one extra\nEmily Dallas … level of indirection\nFrank Austin…\n\nB-trees and B+\ntrees\n\nMotivation\n\n-To have dynamic indexing structures that can\nevolve when records are added and deleted\nStatic indexes are completely rebuilt\n-Optimized for searches on block devices\n-Both B trees and B+ trees are not binary\nObjective is to increase branching factor to\nreduce the number of device accesses\n\nB-Tree family\n\n-B-Tree (1970)\n-B+Tree (1973)\n-B* Tree (1977)\n-BlinkTree (1981)\n-B𝛆-Tree (2003)\n-Bw-Tree (2013)\n\nBinary vs. higher-order tree\n\n-Higher-order trees:\n-Binary trees: Designed for searching\nDesigned for in- data on block devices\nmemory searches Try to minimize the\nTry to minimize the number of device\naccesses number of memory\naccesses Searching within a\nblock is cheap!\n\nB trees\n\n-Generalization of binary search trees\n Not binary trees\nThe B stands for Bayer (or Boeing)\n-Designed for searching data stored on blockoriented devices\n\nA very small B tree\n\n-Bottom nodes are leaf nodes: all their\npointers are NULL\n\nSearching the tree\n\nkeys > 16keys < 7\n\n7 < keys < 16\n\nBalancing B trees\n\n-Objective is to ensure that all terminals\nnodes be at the same depth\n\nInsertions\n\n1\n\n1 2\n\n1 2 3 2\n\n1 3\n\nInsertions\n\n2\n1 3 4\n\n2\n1 3 4 5\n\n2 4\n1 3 5\n\nInsertions\n\n2 4\n1 3 5 6\n\n2 4\n1 3 5 6 7\n\nInsertions\n\n2 4\n1 3 6\n5 7\n\n2 4 6\n1 3\n5 7\n\nInsertions\n\n2 4 6\n1 3\n5 7\n\n4\n\n2 6\n\n1 3 5 7\n\nTwo basic operations\n\n-Split:\nWhen trying to add to a full node\n5 6 7\nSplit node at central value\n6\n\n5 7\n-Promote:\nMust insert root of split node higher up\nMay require a new split\n\nInsertion in B-Trees\n\nInsertion in B-Trees\n\nB+ trees\n\n-Variant of B trees\n-Two types of nodes\nInternal nodes have no data pointers\nLeaf nodes have no in-tree pointers\n\nB+Tree",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 10,
        "page_end": 28,
        "char_len": 1883
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0003",
      "text": "5 7\n-Promote:\nMust insert root of split node higher up\nMay require a new split\n\nInsertion in B-Trees\n\nInsertion in B-Trees\n\nB+ trees\n\n-Variant of B trees\n-Two types of nodes\nInternal nodes have no data pointers\nLeaf nodes have no in-tree pointers\n\nB+Tree\n\n-A self-balancing, ordered m-way tree\n-for searches, sequential access, insertions, and\ndeletions in O(logmn) where m is the tree fanout.\nIt is perfectly balanced (i.e., every leaf node is at the same\ndepth in the tree)\nEvery node other than the root is at least half-full\nm/2-1 ≤ #keys ≤ m-1\nEvery inner node with k keys has k+1 non-null children.\nOptimized for reading/writing large data blocks.\n\nB+Tree Example\n\n<node*>|<key>|<node*>|…|<key>|<node*>\n\n20 Root Node\n<20 ≥20\n\n10 Sibling Pointers 35 Inner / Non-Leaf Nodes\n\n<10 ≥10 <35 ≥35\n\n6 10 20 31 38 44 Leaf Nodes\n\nIndex Key(s) Low → High\n\n<node*>|<key>|<value>|…|<key>|<value>|<node*><key>|<value>|<key>|<value>\n\nNodes\n\n-Every B+Tree node is comprised of an array of\nkey/value pairs.\nThe keys are derived from the index's target\nattribute(s).\nThe values will differ based on whether the node is\nclassified as an inner node or a leaf node.\n-The arrays are (usually) kept in sorted key order.\n-Store all NULL keys at either first or last leaf\nnodes.\n\nSearches\ndef tree_search (k, node) :\nif node is a leaf :\nreturn node\nelif k < k_0 :\nreturn tree_search(k, p_0)\n…\nelif k_i ≤ k < k_{i+1}\nreturn tree_search(k, p_{i+1})\n…\nelif k_d ≤ k\nreturn tree_search(k, p_{d+1});\n\nInsertion in B+ Trees\n\n-1. Navigate to the correct leaf node\n-2. Insert the new key in sorted order\n-3. If overflow occurs:\nSplit the leaf node\nPush the middle key to the parent\nThis process may propagate up to the root\n\nInsertions\n\n-def insert (entry) :\nFind target leaf L\nif L has less than m - 2 entries :\nadd the entry\nelse :\nAllocate new leaf L'\nPick the m/2 highest keys of L and move them to L'\nInsert highest key of L and corresponding address leaf into the parent\nnode\nIf the parent is full :\n- Split it and add the middle key to its parent node\nRepeat until a parent is found that is not full\n\nB+TREE - INSERT",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 24,
        "page_end": 35,
        "char_len": 2116
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0004",
      "text": "B+TREE - INSERT\n\n-Find correct leaf node L.\n-Insert data entry into L in sorted order.\n-If L has enough space, done!\n-Otherwise, split L keys into L and a new node L2\nRedistribute entries evenly, copy up middle key.\nInsert index entry pointing to L2 into parent of L.\n-To split inner node, redistribute entries evenly, but\npush up middle key.\n\nInsertions\n\n1\n\n1 2\n\n1 2 3 2\n\n1 2 3\n\nInsertions\n\n2\n1 2 3 4\n\n2\n1 2 3 4 5\n\n2 4\n1 2 3 4 5\n\nInsertions\n\n2 4\n1 2 3 4 5 6\n\n2 4\n1 2 3 4 5 6 7\n\nInsertions\n\n2 4\n1 2 3 4 6\n5 6 7\n\n2 4 6\n1 2 3 4\n5 6 7\n\nInsertions\n\n2 4 6\n1 3\n5 7\n\n4\n\n2 6\n\n1 3 5 7\n\nInsert\n\n9\n\n5 16 30\n\n1 3 5 6 9 30 40 16 17\n\n- Insert a pair with key = 2.\n\n- New pair goes into a full node.\n\nInsert Into A Full Node\n-Insert new pair so that the keys\nare in ascending order.\n1 2 3\n\n- Split into two nodes.\n\n1 2 3\n\n- Insert smallest key in new node and pointer\nto this new node into parent.\n\n2\n\n1 2 3\n\nInsert\n\n9\n\n5 2 16 30\n\n2 3\n1 5 6 9 30 40 16 17\n\n- Insert an index entry 2 plus a pointer into parent.\n\nInsert\n\n9\n\n2 5 16 30\n\n1 2 3 5 6 9 30 40 16 17\n\n- Now, insert a pair with key = 18.\n\nInsert\n\n9\n17\n\n2 5 16 30 17 18\n\n1 2 3 5 6 9 16 30 40\n\n- Now, insert a pair with key = 18.\n- Insert an index entry17 plus a pointer into parent.\n\nInsert\n\n17 9\n\n2 5 16 30\n\n1 2 3 5 6 9 16 17 18 30 40\n\n- Now, insert a pair with key = 18.\n- Insert an index entry17 plus a pointer into parent.\n\nInsert\n\n9 17\n\n2 5 16 30\n\n1 2 3 5 6 9 16 17 18 30 40\n\n- Now, insert a pair with key = 7.\n\nInsertion in B+Trees\n\nDeletion in B+Trees\n\n-Deletion involves:\n1. Finding and removing the key from the\nappropriate leaf node\n2. Rebalancing the tree to maintain B+ Tree\nproperties:\nBorrowing keys from siblings or\nMerging nodes if necessary\n\nB+TREE - DELETE\n-Start at root, find leaf L where entry belongs.\nRemove the entry.\n-If L is at least half-full, done! If L has\nonly m/2-1 entries,\n→ Try to re-distribute, borrowing from sibling (adjacent node\nwith same parent as L).\n→ If re-distribution fails, merge L and sibling.\n-If merge occurred, must delete entry (pointing to L or\nsibling) from parent of L.\n\nDeletions",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 35,
        "page_end": 51,
        "char_len": 2082
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0005",
      "text": "Deletions\n\n-def delete (record) :\nLocate target leaf and remove the entry\nIf leaf is less than half full:\nTry to re-distribute, taking from sibling (adjacent node with same\nparent)\nIf re-distribution fails:\n- Merge leaf and sibling\n- Delete entry to one of the two merged leaves\n- Merge could propagate to root\n\nDelete\n\n9\n\n2 5 16 30\n\n1 2 3 5 6 9 17 30 40\n\n- Delete pair with key = 16.\n- Note: delete pair is always in a leaf.\n\nDelete\n\n9\n\n2 5 16 30\n\n1 2 3 5 6 9 17 30 40\n\n- Delete pair with key = 1.\n\n- Get >= 1 from adjacent sibling and update parent key.\n\nDelete\n\n9\n\n3 5 16 30\n\n2 3 5 6 9 17 30 40\n\n- Delete pair with key = 1.\n\n- Get >= 1 from sibling and update parent key.\n\nDelete\n\n9\n\n3 5 16 30\n\n2 3 5 6 9 17 30 40\n\n- Delete pair with key = 2.\n\n- Merge with sibling, delete in-between key in parent.\n\nDelete\n\n9\n\n5 16 30\n\n3 9 17 30 40 5 6\n\n- Delete pair with key = 3.\n\n-Get >= 1 from sibling and update parent key.\n\nDelete\n\n9\n\n6 16 30\n\n5 6 9 17 30 40\n\n- Delete pair with key = 9.\n\n- Merge with sibling, delete in-between key in parent.\n\nDelete\n\n9\n\n6 30\n\n30 40 175 6\n\nDelete\n\n9\n\n6 16 30\n\n5 6 9 17 30 40\n\n- Delete pair with key = 6.\n\n- Merge with sibling, delete in-between key in parent.\n\nDelete\n\n9\n\n16 30\n\n5 9 17 30 40\n\n- Index node becomes deficient.\n\n-Get >= 1 from sibling, move last one to parent, get\nparent key.\n\nDelete\n\n16\n\n9 30\n\n5 17 30 40 9\n\n- Delete 9.\n\n- Merge with sibling, delete in-between key in parent.\n\nDelete\n\n16\n\n30\n\n5 17 30 40\n\n-Index node becomes deficient.\n\n- Merge with sibling and in-between key in parent.\n\nDelete\n\n16 30\n\n17 30 40 5\n\n-Index node becomes deficient.\n\n- It’s the root; discard.\n\nLeaf node values\n\n-1: Record IDs\nA pointer to the location of the tuple to\nwhich the index entry corresponds.\nMost common implementation.\n-2: Tuple Data\nIndex-Organized Storage\nPrimary Key Index\nLeaf nodes store the contents of the tuple.\nSecondary Indexes\nLeaf nodes store tuples' primary key as\ntheir values.\n\nB-TREE VS. B+TREE\n\n-The original B-Tree from 1971 stored keys\nand values in all nodes in the tree.\nMore space-efficient, since each key only\nappears once in the tree.\n-A B+Tree only stores values in leaf nodes.\nInner nodes only guide the search process.",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 51,
        "page_end": 65,
        "char_len": 2197
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0006",
      "text": "B-TREE VS. B+TREE\n\n-The original B-Tree from 1971 stored keys\nand values in all nodes in the tree.\nMore space-efficient, since each key only\nappears once in the tree.\n-A B+Tree only stores values in leaf nodes.\nInner nodes only guide the search process.\n\nB+Tree design\nchoices\nNode Size\nMerge Threshold\nVariable-Length Keys\nIntra-Node Search\n\nNode Size\n\n-The slower the storage device, the larger the\noptimal node size for a B+Tree.\nHDD: ~1MB\nSSD: ~10KB\nIn-Memory: ~512B\n-Optimal sizes can vary depending on the\nworkload\nLeaf Node Scans vs. Root-to-Leaf Traversals\n\nMerge Threshold\n-Some DBMSs do not always merge nodes when\nthey are half full.\nAverage occupancy rate for B+Tree nodes is 69%.\n-Delaying a merge operation may reduce the\namount of reorganization.\n-It may also be better to let underfilled nodes exist\nand then periodically rebuild entire tree.\n-This is why PostgreSQL calls their B+Tree a \"nonbalanced\" B+Tree (nbtree).\n\nVariable-length Keys\n-Pointers\nStore the keys as pointers to the tuple’s attribute.\n-Variable-Length Nodes\nThe size of each node in the index can vary.\nRequires careful memory management.\n-Padding\nAlways pad the key to be max length of the key type.\n-Key Map / Indirection\nEmbed an array of pointers that map to the key +\nvalue list within the node.\n\nIntra-node Search\n-Linear\nScan node keys from beginning to end.\nUse SIMD to vectorize comparisons.\nFind Key=8\n\n0 0 0 0 1 0 0 0\n\n4 5 6 7 8 9 10\n\n8 8 8 8 8 8 8 8\n\nIntra-node Search\n-Binary\nJump to middle key\nPivot left/right depending on comparison.\nFind Key=8\n\n→\n\n→ 4 5 6 7 8 9 10\n\nIntra-node Search\n-Interpolation\nApproximate location of desired key based on\nknown distribution of keys.\n\n→\nFind Key=8\n\n→ 4 5 6 7 8 9 10\n\n𝟖−𝟒\nOffset: 𝟏𝟎−𝟒× 𝟕= 𝟒\n\nOptimizations\n\nPrefix Compression Deduplication\n\nSuffix Truncation Pointer Swizzling\n\nBulk Insert Buffered Updates\n\nPrefix compression\n\n-Sorted keys in the same leaf\nnode are likely to have the robbed robbing robot\nsame prefix.\n-Instead of storing the entire Prefix: rob\nbed bing ot key each time, extract\ncommon prefix and store only\nunique suffix for each key.\n\nDeduplication",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 65,
        "page_end": 75,
        "char_len": 2125
      }
    },
    {
      "chunk_id": "ADB_Lec04_c0007",
      "text": "Bulk Insert Buffered Updates\n\nPrefix compression\n\n-Sorted keys in the same leaf\nnode are likely to have the robbed robbing robot\nsame prefix.\n-Instead of storing the entire Prefix: rob\nbed bing ot key each time, extract\ncommon prefix and store only\nunique suffix for each key.\n\nDeduplication\n\n-Non-unique indexes can\nend up storing multiple K1 V1 K1 V2 K1 V3 K2 V4\ncopies of the same key in\nleaf nodes.\n-The leaf node can store the K1 V1 V2 V3 K2 V4\nkey once and then maintain\na \"posting list\" of tuples with\nthat key\n\nSuffix truncation\n\nabcdefghijk lmnopqrstuv-The keys in the inner\nnodes are only used to\n\"direct traffic\". … … … …\nWe don't need the entire\nkey.\nabc lmn-Store a minimum prefix\nthat is needed to correctly\nroute probes into the … … … …\nindex.\n\nPointer swizzling\n-Nodes use page ids to FindKey>3 reference other nodes in the\n6 9 index. Page #2\n-The DBMS must get the Page #3\nmemory location from the page 1 3 6 7\ntable during traversal.\nPage #2 →<Page*>-If a page is pinned in the buffer Page #3 →<Page*>\npool, then we can store raw\n\nHeader Header Header pointers instead of page ids. Pool\n1 2 3-This avoids address lookups\nfrom the page table. Buffer\n\nBulk insert\n\n-The fastest way to build a new B+Tree for an\nexisting table is to first sort the keys and then\nbuild the index from the bottom up.\nKeys: 3, 7, 9, 13, 6, 1\nSorted Keys: 1, 3, 6, 7, 9, 13\n\n6 9\n\n1 3 6 7 9 13",
      "metadata": {
        "lecture_id": "ADB_Lec04",
        "source_file": "ADB_Lec04.pdf",
        "page_start": 73,
        "page_end": 78,
        "char_len": 1387
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\nLec 05\nQuery Processing\n& Optimization [1]\n\nQUERY EXECUTION\n-A query plan is a DAG of SELECT R.id, S.cdate\nFROM R JOIN S operators.\nON R.id = S.id\n-A pipeline is a sequence of WHERE S.value > 100\noperators where tuples\nPipeline #2\ncontinuously flow between them  without intermediate storage. R.id, S.cdate\n-A pipeline breaker is an operator ⨝R.id=S.id\n-that cannot finish until all its value>100\nchildren emit all their tuples.\nR S Joins (Build Side), Subqueries, Order\nPipeline #1 By\n\nQuery Execution\n\nProcessing Model\n\n-A DBMS's processing model defines how the system executes a\nquery plan and moves data from one operator to the next.\nDifferent trade-offs for workloads (OLTP vs. OLAP).\n-Each processing model has two types of execution paths:\nControl Flow\nHow the DBMS invokes an operator.\nData Flow\nHow an operator sends its results.\n-The output of an operator can be either whole tuples (NSM) or\nsubsets of columns (DSM).\n\nProcessing Model\n\nIterator Model\n- Most Common\n\nVectorized / Batch Model\n- Common\n\nMaterialization Model\n- Rare\n\nIterator Model\n\n-Each query plan operator implements a Next()\nfunction.\nOn each invocation, the operator returns either a single\ntuple or a EOF marker if there are no more tuples.\nThe operator implements a loop that calls Next() on its\nchildren to retrieve their tuples and then process them.\n-Each operator implementation also has Open() and\nClose() functions.\n-Also called Volcano or Pipeline Model.\n\nIterator model\n\nSELECT R.id, S.cdate\nfor t in\nNext() FROM R JOIN S child.Next():\nemit(projection(t)) ON R.id = S.id\nWHERE S.value > 100 for t1 in left.Next(): Next()\nbuildHashTable(t1)\nfor t2 in right.Next():\nif probe(t2): emit(t1⨝t2)  R.id, S.cdate\n\nfor t in child.Next():\nNext() if evalPred(t): emit(t) ⨝R.id=S.id\n\nfor t in R: for t in S: value>100Next() Next() emit(t) emit(t)\nR S\n\nIterator model",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 1,
        "page_end": 8,
        "char_len": 1895
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0002",
      "text": "for t in child.Next():\nNext() if evalPred(t): emit(t) ⨝R.id=S.id\n\nfor t in R: for t in S: value>100Next() Next() emit(t) emit(t)\nR S\n\nIterator model\n\nSELECT R.id, S.cdate\nfor t in child.Next(): FROM R JOIN S\nemit(projection(t)) 1 ON R.id = S.id\nWHERE S.value > 100\n22 forbuildHashTable(t1)t1 in left.Next():\nfor t2 in right.Next():  R.id, S.cdate if probe(t2): emit(t1⨝t2)\nfor t in child.Next(): ⨝R.id=S.id Single Tuple if evalPred(t): emit(t) 4\nvalue>100\n3 foremit(t)t in R: foremit(t)t in S: 5 R S\n\nControl Flow Data Flow\n\nIterator model\n-The Iterator model is used in almost every\nDBMS.\nEasy to implement / debug.\nOutput control works easily with this approach.\n-Allows for pipelining where the DBMS tries to\nprocess each tuple through as many operators\nas possible before retrieving the next tuple.\n\nVectorization model\n\n-Like the Iterator Model where each operator\nimplements a Next() function, but…\n-Each operator emits a batch of tuples instead of a\nsingle tuple.\nThe operator's internal loop processes multiple\ntuples at a time.\nThe size of the batch can vary based on hardware or\nquery properties.\nEach batch will contain one or more columns each\ntheir own null bitmaps.\n\nVectorization model\n\nout = [ ]\nfor t in child.Next(): 1 SELECT R.id, S.cdate\nFROM R JOIN S out.add(projection(t))\nif |out|>n: emit(out) ON R.id = S.id\nWHERE S.value > 100 out = [ ] 2\nfor t1 in left.Next():\nbuildHashTable(t1)\nfor t2 in right.Next(): if probe(t2): out.add(t1⨝t2)  R.id, S.cdate\nif |out|>n: emit(out)\n\nout = [ ] Tuple Batch 4 ⨝R.id=S.id for t in child.Next():\nif evalPred(t): out.add(t)\nif |out|>n: emit(out) value>100\nout = [ ] out = [ ] 3\nfor t in R: for t in S: 5 R S out.add(t) out.add(t)\nif |out|>n: emit(out) if |out|>n: emit(out)\n\nVectorization model\n\nout = [ ]\nfor t in child.Next(): 1 SELECT R.id, S.cdate\nFROM R JOIN S out.add(projection(t))\nif |out|>n: emit(out) ON R.id = S.id\nWHERE S.value > 100 out = [ ] 2\nfor t1 in left.Next():\nbuildHashTable(t1)\nfor t2 in right.Next(): if probe(t2): out.add(t1⨝t2)  R.id, S.cdate\nif |out|>n: emit(out)",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 7,
        "page_end": 12,
        "char_len": 2058
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0003",
      "text": "out = [ ]\nfor t in child.Next(): 1 SELECT R.id, S.cdate\nFROM R JOIN S out.add(projection(t))\nif |out|>n: emit(out) ON R.id = S.id\nWHERE S.value > 100 out = [ ] 2\nfor t1 in left.Next():\nbuildHashTable(t1)\nfor t2 in right.Next(): if probe(t2): out.add(t1⨝t2)  R.id, S.cdate\nif |out|>n: emit(out)\n\nout = [ ] Tuple Batch ⨝R.id=S.id for t in S:\nif evalPred(t): out.add(t)\nif |out|>n: emit(out) value>100\nout = [ ] 3 Operator Fusion\nfor t in R: R S out.add(t)\nif |out|>n: emit(out)\n\nVectorization model\n-Ideal for OLAP queries because it greatly reduces the\nnumber of invocations per operator.\n-Allows an out-of-order CPU to efficiently execute\noperators over batches of tuples.\nOperators perform work in tight for-loops over arrays,\nwhich compilers know how to optimize / vectorize.\nNo data or control dependencies.\nHot instruction cache.\n\nMaterialization model\n\n-Each operator processes its input all at once and\nthen emits its output all at once.\nThe operator \"materializes\" its output as a single\nresult.\nThe DBMS can push down hints (e.g., LIMIT) to avoid\nscanning too many tuples.\nCan send either a materialized row or a single\ncolumn.\n-The output can be either whole tuples (NSM) or\nsubsets of columns (DSM).\n\nMaterialization model\n\nout = [ ] 1 SELECT R.id, S.cdate\nfor t in child.Output(): FROM R JOIN S\nout.add(projection(t))\nON R.id = S.id return out\nWHERE S.value > 100\nout = [ ] 2\nfor t1 in left.Output():\nbuildHashTable(t1)\nfor t2 in right.Output():  R.id, S.cdate if probe(t2): out.add(t1⨝t2)\nreturn out\n4 ⨝R.id=S.id out = [ ]\nALL Tuples for t in child.Output():\nif evalPred(t): out.add(t) return out value>100\n\nout = [ ] out = [ ]3 for t in R: for t in S: 5 R S\nout.add(t) out.add(t)\nreturn out return out\n\nMaterialization model\n-Better for OLTP workloads because queries only\naccess a small number of tuples at a time.\n→ Lower execution / coordination overhead.\n→ Fewer function calls.\n-Not ideal for OLAP queries with large intermediate\nresults because DBMS must allocate buffers.\n\nPlan Processing\n\nPlan Processing Direction",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 12,
        "page_end": 18,
        "char_len": 2047
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0004",
      "text": "Plan Processing\n\nPlan Processing Direction\n\n-Top-to-Bottom (Pull)\nStart with the root and \"pull\" data up from its\nchildren.\nTuples are always passed between operators using\nfunction calls (unless it's a pipeline breaker).\n-Bottom-to-Top (Push)\nStart with leaf nodes and \"push\" data to their parents.\nCan \"fuse\" operators together within a for-loop to\nminimize intermediate result staging.\n\nAccess methods\n\n-An access method is the way that the DBMS\naccesses the data stored in a table.\nNot defined in relational algebra.\n-Three basic approaches:\nSequential Scan.\nIndex Scan (many variants).\nMulti-Index Scan.\n\nSEQUENTIAL SCAN\n-For each page in the table:\nRetrieve it from the buffer pool.\nIterate over each tuple and check whether to\ninclude it.\nfor page in table.pages:\nfor t in page.tuples:\nif evalPred(t):\n// Do Something!\n-The DBMS maintains an internal cursor that\ntracks the last page examined.\n\nSequential scan: Optimizations\n\nPrefetching / TaskData Encoding / Scan Sharing / Parallelization / Compression Buffer Bypass Multi-threading\n\nMaterialized Clustering / Late Views / Result Sorting Materialization Caching\n\nData Code\nData Skipping Parallelization / Specialization /\nVectorization Compilation\n\nData skipping\n\n-Approximate Queries (Lossy)\nExecute queries on a sampled subset of the\nentire table to produce approximate results.\n-Zone Maps (Lossless)\nPre-compute columnar aggregations per\npage that allow the DBMS to check whether\nqueries need to access it.\nTrade-off between page size vs. filter efficacy.\n\nZone maps\n-Pre-computed aggregates for the attribute\nvalues in a page.\n-DBMS checks the zone map first to decide\nwhether it wants to access the page.\n\nOriginal Data Zone Map\n\nval type val\nSELECT * FROM table 100 MIN 100\nWHERE val > 600 200 MAX 400\n300 AVG 280\n400 SUM 1400\n400 COUNT 5\n\nIndex scan\n\n-The DBMS picks an index to find the tuples that\nthe query needs.\n-Which index to use depends on:\nWhat attributes the index contains\nWhat attributes the query references\nThe attribute's value domains\nPredicate composition\nWhether the index has unique or non-unique keys",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 17,
        "page_end": 24,
        "char_len": 2105
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0005",
      "text": "Index scan\n\n-The DBMS picks an index to find the tuples that\nthe query needs.\n-Which index to use depends on:\nWhat attributes the index contains\nWhat attributes the query references\nThe attribute's value domains\nPredicate composition\nWhether the index has unique or non-unique keys\n\nIndex scan\n-Suppose that we have a single table with 100\ntuples and two indexes: SELECT * FROM students\nWHERE age < 30 Index #1: age\nAND dept = 'CS'\nIndex #2: dept\nAND country = 'US'\n\nScenario #1 Scenario #2\nThere are 99 people There are 99 people in\nunder the age of 30 but the CS department but\nonly 2 people in the CS only 2 people under the\ndepartment. age of 30.\n\nMulti-index scan\n\n-If there are multiple indexes available for a query, the\nDBMS does not have to pick only one:\nCompute sets of Record IDs using each matching index.\nCombine these sets based on the query’s predicates (union vs.\nintersect).\nRetrieve the records and apply any remaining predicates.\n-Examples:\nDB2 Multi-Index Scan\nPostgreSQL Bitmap Scan\nMySQL Index Merge\n\nMulti-index scan\n\n-Given the following query on a\ndatabase with an index #1 on age\nand an index #2 on dept: SELECT * FROM students\nRetrieve the Record IDs WHERE age < 30\nsatisfying age<30 using index #1. AND dept = 'CS'\nAND country = 'US' Retrieve the Record IDs\nsatisfying dept='CS' using index #2.\nTake their intersection.\nRetrieve records and check\ncountry='US'.\n\nMulti-index scan\n\nSELECT * FROM studentsSet intersection can be done\nefficiently with bitmaps or WHERE age < 30\nAND dept = 'CS'hash tables.\nAND country = 'US'\n\ndept='CS' age<30\n\nrecord ids record ids\n\nfetch records country='US'\n\nExpression\nevaluation\n\nExpression evaluation\n-The DBMS represents a WHERE clause as an\nexpression tree.\n-The nodes in the tree represent different\nexpression types:\nComparisons (=, <, >, !=)\nConjunction (AND), Disjunction (OR)\nArithmetic Operators (+, -, *, /, %)\nConstant Values\nTuple Attribute References\nFunctions\n\nExpression evaluation\n\nSELECT R.id, S.cdate\nFROM R JOIN S\nON R.id = S.id\nWHERE S.value > 100;\n\nAND\n\n= >\n\nAttribute(R.id) Attribute(S.id) Attribute(value) Constant(100)\n\nExpression evaluation",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 24,
        "page_end": 32,
        "char_len": 2151
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0006",
      "text": "Expression evaluation\n\nSELECT R.id, S.cdate\nFROM R JOIN S\nON R.id = S.id\nWHERE S.value > 100;\n\nAND\n\n= >\n\nAttribute(R.id) Attribute(S.id) Attribute(value) Constant(100)\n\nExpression evaluation\n\nPREPARE xxx AS\nSELECT * FROM S ExecutionContext\nWHERE S.val = $1 + 9 Current Tuple Query Parameters Table Schema\n(123, 1000) (int:991) S→(int:id, int:val)\nEXECUTE xxx(991)\n\n=\n\nAttribute(S.val) +\n\nParameter($1) Constant(9)\n\nExpression evaluation\n\nSELECT * WHERE s.val = 1;\n-Evaluating predicates by traversing a\n= tree is terrible for the CPU.\nThe DBMS traverses the tree and for Attribute(s.val) Constant(1)\neach node that it visits, it must figure\nout what the operator needs to do.\n\nbool check(val) {-A better approach is to evaluate the\nreturn (val == 1);\nexpression directly. }\n-An even better approach is to gcc, Clang, LLVM,\nvectorize it evaluate a batch of tuples\nMachine Code\nat the same time…\n\nEXPRESSION EVALUATION:\nOPTIMIZATIONS\n-Constant Folding:\n-→ Identify redundant / unnecessary operations\nthat are wasteful.\n-→ Compute a sub-expression on a constant\n-value once and reuse result per tuple.\n\n→\n-Common Sub-Expr. Elimination: Identify\n\n→ repeated sub-expressions that can be shared\nacross expression tree.\n-Compute once and then reuse result.\n\nConstant Folding\n\nWHERE UPPER(col1) = UPPER('wutang');-Identify redundant /\n\n= unnecessary\n\nUPPER() UPPER() operations that are\n\nAttribute(col1) Constant('wutang') wasteful.\n-Compute a subWHERE UPPER(col1) = UPPER('wutang');\n\n→ expression on a\n=\n\n→ constant\nUPPER() Constant('WUTANG')\n-value once and reuse Attribute(col1)\nresult per tuple.\n\nCommon Sub-Expr. Elimination\n\nWHERE STRPOS('x', col1) < 2\nOR STRPOS('x', col1) > 8\n\nOR-Identify repeated\n\nOp(<) Op(>) sub-expressions\n\nSTRPOS() Constant(2) STRPOS() Constant(8) that can be shared\n\nConstant('x') Attribute(col1) Constant('x') Attribute(col1) across expression\n\n→ tree.\nOR\n\n→ Op(<) Op(>)-Compute once and\n\nSTRPOS() Constant(2) Constant(8) then reuse result.\n\nConstant('x') Attribute(col1)\n\nQuery Optimization\n\nOptimization architecture",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 31,
        "page_end": 38,
        "char_len": 2043
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0007",
      "text": "Constant('x') Attribute(col1) Constant('x') Attribute(col1) across expression\n\n→ tree.\nOR\n\n→ Op(<) Op(>)-Compute once and\n\nSTRPOS() Constant(2) Constant(8) then reuse result.\n\nConstant('x') Attribute(col1)\n\nQuery Optimization\n\nOptimization architecture\n\nApplication\nCost\nModel\nSystem SchemaInfo\nCatalog\n1 SQL Query Estimates\nOptimizer\nParser\nName→Internal ID 4 Physical\nPlan\nBinder\n2 Abstract 3 Logical\nSyntax Tree Plan\n\nLogical vs. Physical plans\n\n-The optimizer generates a mapping of a\nlogical algebra expression to the optimal\nequivalent physical algebra expression.\n\n-Physical operators define a specific\nexecution strategy using an access path.\nThey can depend on the physical format of the\ndata that they process (i.e., sorting, compression).\nNot always a 1:1 mapping from logical to physical.\n\nAnnotated RA Tree = The Physical Plan\n\nSimple projection\nEstimates: output cardinality = 20, … πename\n\nNL-IDX using unclusteredindex on EMP.id To thescheduler\nEstimates: output cardinality = 20, …\nto run the query ⋈EMP.did = DEPT.did\n\nEMP AccessPath: FileScan\nAccessPath: UnclusteredB-tree Estimates: output cardinality = 10K\nEstimates: output cardinality = 1, … σdname= ‘Toy’\n\nDEPT\n\nQuery optimization (QO)\n-Identify candidate equivalent trees\n(logical).\nIt is an NP-hard problem, so the space\np1 is large. pn\n\npi p2-For each candidate, find the\np3 execution plan (physical).\nEstimate the cost of each plan.\nEntire search space very\nlarge, as QO is NP-hard-Choose the best (physical) plan. (w.r.t. # joins)\n-Practically: Choose from a subset\nof all possible plans.\n\nQUERY OPTIMIZATION\n\nHeuristics / Cost-based\nRules Search\n\nHeuristics / Rules\n\n-Rewrite the query to remove (guessed)\ninefficiencies.\n-Examples:\nalways do selections first or push down\nprojections as early as possible.\n-These techniques may need to examine\ncatalog, but they do not need to examine\ndata.\n\nLogical plan optimization",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 36,
        "page_end": 44,
        "char_len": 1903
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0008",
      "text": "Heuristics / Rules\n\n-Rewrite the query to remove (guessed)\ninefficiencies.\n-Examples:\nalways do selections first or push down\nprojections as early as possible.\n-These techniques may need to examine\ncatalog, but they do not need to examine\ndata.\n\nLogical plan optimization\n\n-Transform a logical plan into an equivalent\nlogical plan using pattern matching rules.\n-The goal is to increase the likelihood of\nenumerating the optimal plan in the search.\nMany equivalence rules for relational algebra!\n-Cannot compare plans because there is no\ncost model but can \"direct\" a transformation to\na preferred side.\n\nHeuristic Algebraic\nOptimization\nAlgorithm\n\nAlgorithm Outline\n\n-Using rule 1, break up any select operations with\nconjunctive conditions into a cascade of select operations.\n-Using rules 2, 4, 6, and 10 concerning the commutatively\nof select with other operations, move each select\noperation as far down the query tree as is permitted by\nthe attributes involved in the select condition.\n-Using rule 9 concerning associatively of binary operations,\nrearrange the leaf nodes of the tree so that the leaf node\nrelations with the most restrictive select operations are\nexecuted first in the query tree representation.\n\nAlgorithm Outline\n\n-Using Rule 12, combine a Cartesian product\noperation with a subsequent select operation in the\ntree into a join operation.\n-Using rules 3, 4, 7, and 11 concerning the cascading of\nproject and the commuting of project with other\noperations, break down and move lists of projection\nattributes down the tree as far as possible by\ncreating new project operations as needed.\n-Identify subtrees that represent groups of operations\nthat can be executed by a single algorithm.\n\nEXAMPLE\n\n-Heuristic Optimization of Query Trees:\nThe same query could correspond to many different\nrelational algebra expressions - and hence many different\nquery trees.\nThe task of heuristic optimization of query trees is to find a\nfinal query tree that is Efficient to Execute.\n-Example:\nSELECT LNAME\nFROM EMPLOYEE, WORKS_ON, PROJECT\nWHERE PNAME = ‘AQUARIUS’ AND PNMUBER=PNO\nAND ESSN=SSN AND BDATE > ‘1957-12-31’;\n\nUsing Heuristics in Query Optimization",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 43,
        "page_end": 49,
        "char_len": 2171
      }
    },
    {
      "chunk_id": "ADB_Lec05_c0009",
      "text": "Using Heuristics in Query Optimization\n\nCartesian Product\n→Avoid it!!!\n\nApply Select to reduce\nnumber of tuples\n\nMove Project down since\nit is more selective\n\nSlide 15- 50\n\nReplace Cart. Product\nfollowed by Join Condition\n→ JOIN\n\nMove Project\noperations down the\nquery tree\n\nSlide 15- 51\n\nSummary of Heuristics\n-The main heuristic is to apply first the operations that\nreduce the size of intermediate results.\nPerform select operations as early as possible to reduce\nthe number of tuples\nPerform project operations as early as possible to reduce\nthe number of attributes.\nMove select & project operations down the tree\n-The select and join operations that are most\nrestrictive should be executed before other similar\noperations.\nReorder the leaf nodes of the tree to have most\nrestrictive operations far down\n\nExample\n\n10/28/2025 Introduction to Database Systems 53\n\nQuery optimization example\n\n10/28/2025 Introduction to Database Systems 54\n\n-SELECT S.sid, S.name, S.age\n-FROM Sailors S, Boats B, Reservers R\n-WHERE S.sid=R.sid AND B.bid=R.bid AND\nB.color=“Red” AND S.age>30\n\nQuery optimization example\n\n⚫Step 1: translate SQL query to algebra query\n\n⚫Step 2: generate initial query tree\n\n⚫Step 3: apply heuristic rules and\nGenerate optimized tree\n\n10/28/2025 Introduction to Database Systems 56\n\nQuery optimization example\n\n10/28/2025 Introduction to Database Systems 57",
      "metadata": {
        "lecture_id": "ADB_Lec05",
        "source_file": "ADB_Lec05.pdf",
        "page_start": 49,
        "page_end": 56,
        "char_len": 1376
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\nLec 06\nQuery Processing\n& Optimization [2]\n\nCost-based Search\n\n-Use a model to estimate the cost of executing\na plan.\n-Enumerate multiple equivalent plans for a\nquery and pick the one with the lowest cost.\n\nCost-based query optimization\n-Start with cost-based, bottom-up QO\n-Enumerate different plans for the query and\nestimate their costs.\n-It chooses the best plan it has seen for the\nquery after exhausting all plans or some\ntimeout.\nSingle relation.\nMultiple relations.\nNested sub-queries.\n\nSingle-relation query planning\n\n-Pick the best access method.\nSequential Scan\nBinary Search\nIndex Scan\n-Predicate evaluation ordering.\n-Simple heuristics are often good enough for\nthis.\n\nMulti-relation query planning\n-Generative / Bottom-Up\nStart with nothing and then iteratively assemble and\nadd building blocks to generate a query plan.\nExamples: System R, Starburst\n-Transformation / Top-Down\nStart with the outcome that the query wants, and then\ntransform it to equivalent alternative sub-plans to find\nthe optimal plan that gets to that goal.\nExamples: Volcano, Cascades\n\nBottom-up optimization\n\n-Use static rules to perform initial\noptimization.\n-Then use dynamic programming to\ndetermine the best join order for tables using\na divide-and-conquer search method\n-Examples\nIBM System R, DB2, MySQL, Postgres, most\nopen-source DBMSs.\n\nSystem R optimizer\nLeft-DeepTree\n-Break query into blocks and generate\nlogical operators for each block. D\n\nC-For each logical operator, generate a set\nB A of physical operators that implement it. outer inner\nAll combinations of join algorithms and\nBushyTree\naccess paths\n-Then, iteratively construct a “left-deep”\njoin tree that minimizes the estimated\nA B C D\namount of work to execute the plan.\n\nSystem R optimizer",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 1,
        "page_end": 8,
        "char_len": 1800
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0002",
      "text": "System R optimizer\n\nSELECT ARTIST.NAME\nFROM ARTIST, APPEARS, ALBUM ARTIST: Sequential Scan\nWHERE ARTIST.ID=APPEARS.ARTIST_ID APPEARS: Sequential Scan\nAND APPEARS.ALBUM_ID=ALBUM.ID ALBUM: Index Look-up on NAME\nAND ALBUM.NAME=“Andy's OG Remix”\nORDER BY ARTIST.ID\nARTIST ⨝ APPEARS ⨝ ALBUM\nStep #1: Choose the best access APPEARS ⨝ALBUM ⨝ARTIST\npaths to each table ALBUM ⨝APPEARS ⨝ARTIST\nAPPEARS ⨝ ARTIST ⨝ ALBUM\nStep #2: Enumerate all possible ARTIST × ALBUM ⨝ APPEARS\njoin orderings for tables ALBUM × ARTIST ⨝ APPEARS\n⋮ ⋮ ⋮ Step #3: Determine the join\nordering with the lowest cost\n\nTop-down optimization\n\n-Start with a logical plan of what we want the\nquery to be.\n-Perform a branch-and-bound search to\ntraverse the plan tree by converting logical\noperators into physical operators.\nKeep track of global best plan during search.\nTreat physical properties of data as first-class\nentities during planning.\n-Examples: MSSQL, Greenplum, CockroachDB\n\nTop-down optimization\n\nARTIST ⨝APPEARS ⨝ALBUM\n-Start with a logical plan of what ORDER-BY(ARTIST.ID)\nwe want the query to be.\n-Invoke rules to create new SORT(A1.ID) HASH_JOIN(A1⨝A2,A3)\nnodes and traverse tree.\nMERGE_JOIN(A1⨝A2,A3)\nLogical→Logical: HASH_JOIN(A1⨝A2,A3)\nJOIN(A,B) to JOIN(B,A)\nLogical→Physical: ARTIST⨝APPEARS ALBUM⨝APPEARS ARTIST⨝ALBUM\nJOIN(A,B) to\nHASH_JOIN(A1,A2) MERGE_JOIN(A1,A2)\nHASH_JOIN(A,B)\n-Can create \"enforcer\" rules that\nARTIST ALBUM APPEARS\nrequire input to have certain\nproperties.\nLogical Op Physical Op\n\nHash join vs. Merge join\nFeature Hash Join Merge Join\n\nBuilds a hash table from one table's join Sorts both tables on the join key and then\nHow it column and then probes the hash table merges them together by comparing rows in a works\nwith rows from the other table. single pass.\n\nWhen Large, unsorted tables, especially when Tables that are already sorted on the join key,\nit's best one fits into memory. or when the join is a range join.\n\nLower, as it primarily requires memory forMemory Can be high, especially if the hash table sorting (if needed) and the merge process usage doesn't fit in memory and spills to disk. itself.",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 8,
        "page_end": 11,
        "char_len": 2117
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0003",
      "text": "Lower, as it primarily requires memory forMemory Can be high, especially if the hash table sorting (if needed) and the merge process usage doesn't fit in memory and spills to disk. itself.\n\nFast for large datasets when the hashPerform Slower if tables need to be sorted first; very fast table fits in memory; performance ance and efficient if data is already sorted. degrades if it has to use disk.\n\nJoin Primarily used for equi-joins (=). Can handle both equi-joins and non-equi-joins. types\n\nMerge Join\n\nNested sub-queries\n\n-The DBMS treats nested sub-queries in the\nwhere clause as functions that take\nparameters and return a single value or set of\nvalues.\n1. Rewrite to de-correlate and/or flatten\nthem.\n2. Decompose nested query and store results\nin a temporary table.\n\nNested sub-queries: Rewrite\n\nSELECT name FROM sailors AS S\nWHERE EXISTS (\nSELECT * FROM reserves AS R\nWHERE S.sid = R.sid\nAND R.day = '2024-10-25'\n)\n\nSELECT name\nFROM sailors AS S, reserves AS R\nWHERE S.sid = R.sid\nAND R.day = '2024-10-25'\n\nDecomposing queries\n\n-For harder queries, the optimizer breaks up\nqueries into blocks and then concentrates on\none block at a time.\n-Sub-queries are written to temporary tables\nthat are discarded after the query finishes.\n\nDecomposing queries\n\nSELECT S.sid, MIN(R.day)\nFROM sailors S, reserves R, boats B\nWHERE S.sid = R.sid AND R.bid\n= B.bid AND B.color = 'red'\nAND S.rating = (SELECT MAX(S2.rating)\nFROM sailors S2)\nGROUP BY S.sid\nHAVING COUNT(*) > 1\nNested Block\n\nDecomposing queries\n\nInner Block SELECT MAX(rating) FROM sailors\n\nSELECT S.sid, MIN(R.day)\nFROM sailors S, reserves R, boats B\nWHERE S.sid = R.sid AND R.bid\n= B.bid AND B.color = 'red'\nAND S.rating =\n\nGROUP BY S.sid\nHAVING COUNT(*) > 1\nOuter Block\n\nExpression rewriting\n\n-An optimizer transforms a query’s expressions\n(e.g., WHERE/ON clause predicates) into the\nminimal set of expressions.\n-Implemented using if/then/else clauses or a\npattern-matching rule engine.\nSearch for expressions that match a pattern.\nWhen a match is found, rewrite the expression.\nHalt if there are no more rules that match.\n\nExpression rewriting\n\nImpossible / Unnecessary Predicates",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 11,
        "page_end": 19,
        "char_len": 2145
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0004",
      "text": "Expression rewriting\n\nImpossible / Unnecessary Predicates\n\nSELECT * FROM A WHERE 1 = 0; SELECT * FROM A WHERE false;\n\nSELECT * FROM A WHERE NOW() IS NULL;\n\nSELECT * FROM A WHERE false;\n\nMerging Predicates\n\nSELECT * FROM A\nSELECT * FROM A\nWHERE val BETWEEN 1 AND 100\nWHERE val BETWEEN 1 AND 150;\nOR val BETWEEN 50 AND 150;\n\nCost estimation\n\nCost estimation\n\n-The DBMS uses a cost model to predict the\nbehavior of a query plan given a database\nstate.\nThis is an internal cost that allows the DBMS to\ncompare one plan with another.\n-It is too expensive to run every possible plan\nto determine this information, so the DBMS\nneed a way to derive this information.\n\nCost model components\n\n-Physical Costs\nPredict CPU cycles, I/O, cache misses, RAM\nconsumption, network messages…\nDepends heavily on hardware.\n-Logical Costs\nEstimate output size per operator.\nIndependent of the operator algorithm.\nNeed estimations for operator result sizes.\n\nStatistics\n\n-The DBMS stores internal statistics about tables,\nattributes, and indexes in its internal catalog.\n-Different systems update them at different times.\n-Manual invocations:\nPostgres/SQLite: ANALYZE\nOracle/MySQL: ANALYZE TABLE\nSQL Server: UPDATE STATISTICS\nDB2: RUNSTATS\n\nSelection cardinality\n\n-The selectivity (sel) of a predicate P is the fraction\nof tuples that qualify: SELECT * FROM people\nEquality Predicate: A=constant WHERE age = 9\nsel(A=constant) = #occurences / |R|\nExample: sel(age = 9) = 4/45\n\nSC(age=9)=4\n# of occurrences\n10\n\nDistinct values\n0 of attribute\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nage\n\nSelection cardinality\n-Uniform Data\nThe distribution of values is the same.\n-Independent Predicates\nThe predicates on attributes are independent\n-Inclusion Principle\nThe domain of join keys overlap such that each\nkey in the inner relation will also exist in the\nouter table.\n\nStatistics\n\n- Maintain an occurrence count perHistograms value (or range of values) in a column.\n\n- Probabilistic data structure that gives\nan approximate count for a given Sketches\nvalue.\n\n- DBMS maintains a small subset of each\ntable that it then uses to evaluate Sampling\nexpressions to compute selectivity.\n\nHistograms",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 19,
        "page_end": 27,
        "char_len": 2171
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0005",
      "text": "- Probabilistic data structure that gives\nan approximate count for a given Sketches\nvalue.\n\n- DBMS maintains a small subset of each\ntable that it then uses to evaluate Sampling\nexpressions to compute selectivity.\n\nHistograms\n\nOur formulas are nice, but we assume that data\nvalues are uniformly distributed.\n\nHistogram\n10 #of occurrences\n\n5\n\n0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n\nDistinctvaluesof attribute 15Keys× 32-bits=60bytes\n\nEqui-width histogram\nMaintain counts for a group of values instead of\neach unique key. All buckets have the same width\n(i.e., same # of value).\n\nNon-Uniform Approximation\n10\n\n5\n\n0\nBucket Ranges 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nBucket #1 Bucket #2 Bucket #3 Bucket #4 Bucket #5\nCount=8 Count=4 Count=15 Count=3 Count=14\n\nEqui-width histogram\nMaintain counts for a group of values instead of\neach unique key. All buckets have the same width\n(i.e., same # of value).\n\nEqui-Width Histogram\n15\n10\n5\n0\n1-3 4-6 7-9 10-12 13-15 Bucket Ranges\nBucket #1 Bucket #2 Bucket #3 Bucket #4 Bucket #5\nCount=8 Count=4 Count=15 Count=3 Count=14\n\n145\nEQUI-DEPTH HISTOGRAMS\n\nVarythe width of bucketsso that the total number\nof occurrencesfor each bucket isroughlythe same.\n\nHistogram(Quantiles)\n10\n\n5\n\n15-445/645 (Spring 2025) 0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n\nEqui-depth histograms\n\nVarythe width of bucketsso that the total number\nof occurrencesfor each bucket isroughlythe same.\n\nHistogram(Quantiles)\n10\n\n5\n\n0\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n\nBucket #1 Bucket #2 Bucket #3 Bucket #4\nCount=12 Count=12 Count=9 Count=12\n\nEqui-depth histograms\n\nVarythe width of bucketsso that the total number\nof occurrencesfor each bucket isroughlythe same.\n\nHistogram(Quantiles)\n15\n10\n5\n0\n1-5 6-8 9-13 14-15\n\nSketches\n\n-Probabilistic data structures that generate\napproximate statistics about a data set.\n-Cost-model can replace histograms with sketches to\nimprove its selectivity estimate accuracy.\n-Most common examples:\nCount-Min Sketch\nApproximate frequency count of elements in a set.\nHyperLogLog\nApproximate the number of distinct elements in a set.\n\nSampling",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 26,
        "page_end": 34,
        "char_len": 2078
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0006",
      "text": "Sampling\n\nSELECT AVG(age)\n-Modern DBMSs also collect samples\nFROM people\nfrom tables to estimate selectivities.\nWHERE age > 50\n-Update samples when the underlying\nid name age status\ntables changes significantly. 1001 Obama 63 Rested\n1002 Swift 34 Paid\n1003 Tupac 25 Dead\nTable Sample 1004 Bieber 30 Crunk\n1005 Andy 43 Illin\n1001 Obama 63 Rested\n1006 TigerKing 61 Jailedsel(age>50) = 1/3 1003 Tupac 25 Dead\n⋮ 1005 Andy 43 Illin\n1 billion tuples\n\nAdaptive Query\nOptimization\n\nAdaptive Query Optimization\n\n-The best plan for a query can change as the\ndatabase evolves over time.\nPhysical design changes.\nData modifications.\nPrepared statement parameters.\nStatistics updates.\n-The query optimizers that we have talked about\nso far all generate a plan for a query before the\nDBMS executes a query.\n\nBad query plans\n\n-The most common problem in a query plan is\nincorrect join orderings.\nThis occurs because of inaccurate cardinality\nestimations that propagate up the plan.\n-If the DBMS can detect how bad a query plan\nis, then it can decide to adapt the plan rather\nthan continuing with the current sub-optimal\nplan.\n\nWhy good plans go bad?\n\n-Estimating the execution behavior of a plan\nto determine its quality relative to other plans.\n-These estimations are based on a static\nsummarizations of the contents of the\ndatabase and its operating environment:\nStatistical Models / Histograms / Sampling\nHardware Performance\nConcurrent Operations\n\nOptimization timing\n\n-Static Optimization\nSelect the best plan prior to execution.\nPlan quality is dependent on cost model accuracy.\nCan amortize over executions with prepared statements.\n-Dynamic Optimization\nSelect operator plans on-the-fly as queries execute.\nWill have re-optimize for multiple executions.\nDifficult to implement/debug (non-deterministic)\n-Adaptive Optimization\nCompile using a static algorithm.\nIf the estimate errors > threshold, change or re-optimize.\n\nAdaptive query optimization",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 34,
        "page_end": 40,
        "char_len": 1956
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0007",
      "text": "Adaptive query optimization\n\n-Modify the execution behavior of a query by\ngenerating multiple plans for it:\nIndividual complete plans.\nEmbed multiple sub-plans at materialization points.\n-Use information collected during query\nexecution to improve the quality of these plans.\nCan use this data for planning one query or merge\nthe it back into the DBMS's statistics catalog.\n\nAdaptive query optimization\n\n-Modify Future Invocations\n-Replan Current Invocation\n-Plan Pivot Points\n\nModify future invocations\n\n-The DBMS monitors the behavior of a query\nduring execution and uses this information to\nimprove subsequent invocations.\nPlan Correction\nFeedback Loop\n\nReversion-based plan correction\n\n-The DBMS tracks the history of query\ninvocations:\nCost Estimations\nQuery Plan\nRuntime Metrics\n-If the DBMS generates a new plan for a query,\nthen check whether that plan performs worse\nthan the previous plan.\nIf it regresses, then switch back to the cheaper plans.\n\nReplan current invocation\n\n-If the DBMS determines that the observed\nexecution behavior of a plan is far from its\nestimated behavior, them it can halt execution\nand generate a new plan for the query.\nStart-Over from Scratch\nKeep Intermediate Results\n\nPlan pivot points\n\n-The optimizer embeds alternative sub-plans\nat materialization points in the query plan.\n-The plan includes \"pivot\" points that guides\nthe DBMS towards a path in the plan based on\nthe observed statistics.\nParametric Optimization\nProactive Reoptimization\n\nParametric optimization\n\n-Generate multiple sub- SELECTJOIN *B FROMON A.idA = B.id\nplans per pipeline in the JOIN C ON A.id = C.id;\nCandidate Pipeline #1\nIf |input|>X, choose#1 HASH_JOIN(A⨝B,C) query.\nElse, choose #2\n\nSEQ_SCAN(C) CHOOSE-PLAN-Add a choose-plan\nCandidate Pipeline #2\n\nHASH_JOIN(A,B) operator that allows the NL_JOIN(A⨝B,C)\n\nSEQ_SCAN(A) SEQ_SCAN(B) DBMS to select which plan IDX_SCAN(C)\nto execute at runtime.\n\nProactive reoptimization\n\n-Generate multiple subplans within a single SELECT * FROM A JOIN B ON A.id = B.id\nJOIN C ON A.id = C.id; pipeline.\n-Use a switch operator to ComputeBounding Boxes Optimizer GenerateSwitchable Plans choose between different",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 40,
        "page_end": 47,
        "char_len": 2168
      }
    },
    {
      "chunk_id": "ADB_Lec06_c0008",
      "text": "Proactive reoptimization\n\n-Generate multiple subplans within a single SELECT * FROM A JOIN B ON A.id = B.id\nJOIN C ON A.id = C.id; pipeline.\n-Use a switch operator to ComputeBounding Boxes Optimizer GenerateSwitchable Plans choose between different\n\nReoptimize sub-plans during execution\nExecution ExecuteQuery in the pipeline. CollectStatistics Engine\n-Computes bounding boxes SwitchPlans\nto indicate the uncertainty of\nestimates used in plan.\n\nPlan stability\n\n-Hints\nAllow the DBA to provide hints to the optimizer.\n-Fixed Optimizer Versions\nSet the optimizer version number and migrate\nqueries one-by-one to the new optimizer.\n-Backwards-Compatible Plans\nSave query plan from old version and provide it\nto the new DBMS.\n\nConclusion\n\n-Query optimization is critical for a database\nsystem.\nSQL → Logical Plan → Physical Plan\nFlatten queries before going to the optimization part.\n-Expression handling is also important.\nEstimate costs using models based on\nsummarizations.\n-QO enumeration can be bottom-up or top-down.",
      "metadata": {
        "lecture_id": "ADB_Lec06",
        "source_file": "ADB_Lec06.pdf",
        "page_start": 47,
        "page_end": 49,
        "char_len": 1025
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\nLec 07\nTransaction Processing and\nSchedules\n\nIntroduction\n\n-Transactions (informally):\nmechanism for managing logical units of data processing:\nindependent on others, all or nothing\nExamples:\na single data retrieval query\na sequence of data manipulation queries that should be executed together\n-Transaction management (or processing) systems:\nsystems with large databases and many concurrent users require\nhigh availability and fast response time\nExamples:\nbanking, airline booking, online retail, stocks\n\nFund Transfer Example\n\n-Transfer $50 from account A to account B:\n1. read(A) Failures, such as:\n2. A := A - 50 - hardware failures\n- system crashes 3. write(A)\n4. read(B)\nConcurrent execution of\n5. B := B + 50 multiple transactions\n6. write(B)\n\nFund Transfer Example\n\n-If the transaction fails after-Transfer $50 from\nstep 3 and before step 6, account A to account B: money will be “lost” leading\n1. read(A) to an inconsistent database\nstate 2. A := A - 50\n-The system should ensure 3. write(A)\nthat updates of a partially\n4. read(B) executed transaction are\nnot reflected in the 5. B := B + 50\ndatabase\n6. write(B)\nAtomicity requirement\n\nFund Transfer Example\n\n-Once the user has been-Transfer $50 from\nnotified that the account A to account B: transaction has completed\n1. read(A) The transfer of the $50 has\ntaken place 2. A := A - 50\n-the updates to the database 3. write(A)\nby the transaction must\n4. read(B) persist even if there are\n5. B := B + 50 software or hardware\nfailures. 6. write(B)\nDurability requirement\n\nFund Transfer Example\n\n-Transfer $50 from -Sum of A and B is unchanged by\naccount A to account B: the execution of the transaction\nA transaction must see a\n1. read(A) consistent database.\n2. A := A - 50 During transaction execution\nthe database may be\n3. write(A) temporarily inconsistent.\n4. read(B) When the transaction\ncompletes successfully the 5. B := B + 50 database must be consistent\n6. write(B)\nConsistency requirement\n\nFund Transfer Example",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 1,
        "page_end": 7,
        "char_len": 2051
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0002",
      "text": "Fund Transfer Example\n\n-if between steps 3 and 6,-Transfer $50 from\nanother transaction T2 is\naccount A to account B: allowed to access the partially\nupdated database 1. read(A)\n-T2 2. A := A - 50\nread(A), read(B), print(A+B)\n3. write(A)\n-it will see an inconsistent\n4. read(B) database\n5. B := B + 50 -(the sum A + B will be less\nthan it should be).\n6. write(B)\nIsolation requirement\n\nSingle-user vs multi-user\n\n-Single-user DBMS:\nat most one user at a time (usually, personal computer)\n-Multi-user DBMS:\nmany users (processes with own computation) with concurrent access to the\nsame data\n(usually, servers with many CPUs, but may be handled by one CPU with interleaving\nconcurrency)\n-Both need transaction management, with:\nconcurrency control\n(transactions independent of each other)\nfail recovery\n(each transaction executed all or nothing)\n\nThe place of transaction management\n\nThe place of transaction management\n\nTransaction idea\n-Transaction is a program that forms a logical unit of\ndatabase processing\nhas its own memory and computation ability\nincludes one or more access operations to (shared)\ndatabase (e.g., retrieval, insertion, deletion)\n-boundaries can be specified by begin and end\nstatements\nif something goes wrong on the way, effects roll back\n-an application program may have several transactions\nmay be read-only or read-write\n\nMain transaction operations\n\n-read(X)\nreads an item named X from the global database into a local program\nvariable named X\nincludes finding the address of the block on the disk (or in cache) with X , and\ncopying to a main memory buffer\n-write(X)\nwrites the value of local program variable named X into the global\ndatabase item named X\nincludes finding the address of the block on the disk (or in cache) with X , read\nit to the local memory buffer, modify it, and write it back (to the disk or cache)\n-Program local operations\n(for example, update X := X + 50)\n\nExample transactions\n\n-Let X and Y be the numbers of reserved\nseats in two flights (stored in a database)\n-Transaction T1 transfers N reservations\nfrom X to Y\n-Transaction T2 reserves M seats in X",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 7,
        "page_end": 13,
        "char_len": 2130
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0003",
      "text": "Example transactions\n\n-Let X and Y be the numbers of reserved\nseats in two flights (stored in a database)\n-Transaction T1 transfers N reservations\nfrom X to Y\n-Transaction T2 reserves M seats in X\n\n-Important:\nupdates are local, the database is not\nupdated until the new value is written\nsome commands are often omitted if\nthey are not relevant (both local and\ntransaction management)\n\nTypes of failures\nComputer failure - hardware, software, network errors\n(system crash)\n\n- division by zero, integrity constraint violation, user Transaction failure interrupt, etc.\n\nLocal transaction errors - no data found, programmed exception, etc.\n\nConcurrency control - serializability violation, deadlock resolving, etc.\nenforcement\n\nDisk failure - errors with disk reads or writes\n\nPhysical problems - power cut, fire, catastrophe, etc.\n\nTransaction State\n-Active\nthe initial state; the transaction stays in this state while it is executing\n-Partially committed\nafter the final statement has been executed.\n-Failed\n after the discovery that normal execution can no longer proceed.\n-Aborted\nafter the transaction has been rolled back and the database restored to its\nstate prior to the start of the transaction.\nTwo options after it has been aborted:\nRestart the transaction (only if no internal logical error)\nKill the transaction\n-Committed\nafter successful completion.\n\nTransaction State (Cont.)\n\nDesirable\nproperties\nACID principles\n\nACID properties\n\nAtomicity Consistency Isolation Durability\n\nTransaction Transaction The database Changes of should not performed in should always committed interfere with its entirety or remain transactions other not at all consistent must persist transactions\n\nensured by ensured by ensured by the ensured by\ntransaction transaction concurrency transaction\nrecovery recovery control recovery\nsubsystem subsystem subsystem subsystem\n\nExample",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 13,
        "page_end": 19,
        "char_len": 1881
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0004",
      "text": "ensured by ensured by ensured by the ensured by\ntransaction transaction concurrency transaction\nrecovery recovery control recovery\nsubsystem subsystem subsystem subsystem\n\nExample\n\n-Consider two transactions (Xacts):\n-T1: BEGIN A=A+100, B=B-100 END\n-T2: BEGIN A=1.06*A, B=1.06*B END\n1st xact transfers $100 from B’s account to A’s\n2nd credits both accounts with 6% interest.\n-Assume at first A and B each have $1000. What are the legal outcomes of\nrunning T1 and T2?\nT1 ; T2 (A=1166,B=954)\nT2 ; T1 (A=1160,B=960)\nIn either case, A+B = $2000 *1.06 = $2120\nThere is no guarantee that T1 will execute before T2 or vice-versa, if both are\nsubmitted together.\n\nExample (Contd.)\n-Consider a possible interleaved schedule:\n-T1: A=A+100, B=B-100\n-T2: A=1.06*A, B=1.06*B\nThis is OK (same as T1;T2). But what about:\n-T1: A=A+100, B=B-100\n-T2: A=1.06*A, B=1.06*B\nResult: A=1166, B=960; A+B = 2126, bank loses $6 !\n-The DBMS’s view of the second schedule:\n-T1: R(A), W(A), R(B), W(B)\n-T2: R(A), W(A), R(B), W(B)\n\nInterleaved vs. Parallel processing\n\nTransactions and\nschedules\n\nTransactions\n\n-Transaction is a sequence of operations, an atomic unit of work\n-Structure of a committed (successful) transaction:\n1. begin ← marks the beginning of transaction execution\n2. one or several read(X), write(X), local operations (e.g., var updates),\ntransaction control operations (e.g., locks), etc.\n3. End ← mark the end of transaction execution\n4. one or several operations checking consistency, serializability, etc.\n5. commit ← successful completion, changes cannot be undone\n-Structure of an aborted (unsuccessful) transaction: same beginning, but\nends at any point with\nN. abort ← unsuccessful completion, all changes must be undone\n-A partial transaction:\na beginning of one above (‘waiting’ for next operation)\n\nSchedules\n\n-Schedule (or history, execution plan) S of\ntransactions T1, . . . , Tn: a (total) ordering of the\noperations of T1, . . . , Tn\n\noperations of different transactions can interleave\n\noperations of each Ti are in the same order as in Ti\n\nSchedules\n-Example:\nSa: r1(X ), r2(X ), w1(X ), r1(Y ), w2(X ), w1(Y )",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 18,
        "page_end": 25,
        "char_len": 2129
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0005",
      "text": "operations of different transactions can interleave\n\noperations of each Ti are in the same order as in Ti\n\nSchedules\n-Example:\nSa: r1(X ), r2(X ), w1(X ), r1(Y ), w2(X ), w1(Y )\n\n-Example (complete):\nSb : r1(X ), w1(X ), r2(X ), w2(X ), r1(Y ), a1, c2\n\nComplete Schedule\n-all T1, . . . , Tn are committed or aborted\nA transaction that successfully completes its\nexecution will have a commit instructions as the\nlast statement\nBy default, transaction assumed to execute commit\ninstruction as its last step\nA transaction that fails to successfully complete\nits execution will have an abort instruction as the\nlast statement\n\nExample\n\n-Let\nT1 transfer $50 from A to B,\n-and\nT2 transfer 10% of the balance from A\nto B.\n\nSchedule 1\n\n-A serial schedule in which\nT1 is followed by T2\n\nSchedule 2\n\n-A serial schedule\nwhere T2 is followed\nby T1\n\nSchedule 3\n\n-This schedule is not a\nserial schedule, but it is\nequivalent to Schedule 1\nIn Schedules 1, 2 and 3, the\nsum A + B is preserved.\n\nSchedule 4\n\n-The following concurrent\nschedule does not preserve\nthe value of (A + B ).\n\nSerializability\n\nSerializability\n\n-Each transaction should preserve database\nconsistency.\nSerial execution of a set of transactions\npreserves database consistency.\n-A (possibly concurrent) schedule is\nserializable if it is equivalent to a serial\nschedule.\n\nSerializable schedules\n\n-C not serializable D serializable\n\nConflicting Instructions\n\n-Instructions li and lj of transactions Ti and Tj\nrespectively,\n-conflict\nif and only if there exists some item Q accessed\nby both li and lj,\nand at least one of these instructions wrote Q.\n\nConflicting Instructions\n-li = read(Q), lj = read(Q). don’t conflict.\n-li = read(Q), lj = write(Q). They conflict\n\nr1(X), r2(X), w1(X), r1(Y), w2(X), w1(Y)\n\n-li = write(Q), lj = read(Q). They conflict\n\nr1(X), w1(X), r2(X), w2(X), r1(Y), a1\n\n-li = write(Q), lj = write(Q). They conflict\nr1(X), w1(X), r2(X), w2(X), r1(Y), a1\n\nConflicting Instructions",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 24,
        "page_end": 37,
        "char_len": 1967
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0006",
      "text": "r1(X), r2(X), w1(X), r1(Y), w2(X), w1(Y)\n\n-li = write(Q), lj = read(Q). They conflict\n\nr1(X), w1(X), r2(X), w2(X), r1(Y), a1\n\n-li = write(Q), lj = write(Q). They conflict\nr1(X), w1(X), r2(X), w2(X), r1(Y), a1\n\nConflicting Instructions\n\n-A conflict between li and lj forces a (logical)\ntemporal order between them.\n-If li and lj are consecutive in a schedule and\nthey do not conflict,\ntheir results would remain the same even if they\nhad been interchanged in the schedule.\n\nSerializability Types\n\nDifferent forms of\nschedule equivalence\ngive the notions of:\n\nConflict View\nserializability serializability\n\nConflict Serializability\n\n-If a schedule S can be transformed into a\nschedule S’ by a series of swaps of nonconflicting instructions, we say that S and S’\nare conflict equivalent.\n-We say that a schedule S is conflict\nserializable if it is conflict equivalent to a\nserial schedule\n\nConflict equivalent\n\n-Two schedules are conflict equivalent if\nthey are schedules of the same transactions\nif the relative order of every conflict (readwrite, write-write) is the same in both\nschedules\n\nConflict Serializability (Cont.)\n-Schedule 3 can be transformed into Schedule 6, a serial schedule\nwhere T2 follows T1, by series of swaps of non-conflicting\ninstructions.\nTherefore Schedule 3 is conflict serializable.\n\nSchedule 3 Schedule 6\n\nConflict Serializability (Cont.)\n-Example of a schedule that is not conflict\nserializable:\n\n-We are unable to swap instructions in the above\nschedule to obtain either the serial schedule < T3,\nT4 >, or the serial schedule < T4, T3 >.\n\nConflict Serializability (Cont.)\n\nSchedule C is not (conflict-)serializable\nSchedule D is serializable (equivalent to T1;T2)\n\nTesting for Serializability\n-Consider some schedule of a set of transactions\nT1, T2, ..., Tn\n-Precedence graph\na direct graph where the vertices are the\ntransactions.\n-We draw an arc from Ti to Tj if the two\ntransactions conflict, and Ti accessed the data\nitem on which the conflict arose earlier.\n-We may label the arc by the item that was\naccessed.\n\nTest for Conflict Serializability",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 36,
        "page_end": 45,
        "char_len": 2087
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0007",
      "text": "Test for Conflict Serializability\n\n-A schedule is conflict serializable if and\nonly if its precedence graph is acyclic.\n-If precedence graph is acyclic, the\nserializability order can be obtained by a\ntopological sorting of the graph.\n This is a linear order consistent with the\npartial order of the graph.\nFor example, a serializability order for\nSchedule A would be\nT5 → T1 → T3 → T2 → T4\nAre there others?\n\nconflict serializable\n\nExample 1\n\nExample 2\n\nExample 3\n\nView Serializability\n\n-Let S and S’ be two schedules with the same set of transactions.\n-S and S’ are view equivalent if the following three conditions are\nmet, for each data item Q.\n1. If in S, transaction Ti reads the initial value of Q,\nthen in S’ transaction Ti must read the initial value of Q.\n2. If in S transaction Ti executes read(Q), and that value was\nproduced by transaction Tj ,\nthen in S’ transaction Ti must read the value of Q that was\nproduced by the same write(Q) of Tj .\n3. The transaction that performs the final write(Q) operation in S\nmust also perform the final write(Q) operation in S’.\n\nView-serializability\n\n-serializability based on view equivalence\n-Each conflict-serializable is view-serializable\n-Each view-serializable is semanticserializable (so result-serializable)\n-Difficult to check view-equivalence (NPcomplete)\n\nView Serializability (Cont.)\n\n-A schedule S is view serializable if it is viewequivalent to a serial schedule.\n-Below is a schedule which is view-serializable but not\nconflict serializable.\n\nTest for View Serializability\n\n-The precedence graph test for conflict serializability\ncannot be used directly to test for view serializability.\nExtension to test for view serializability has cost exponential in\nthe size of the precedence graph.\n-The problem of checking if a schedule is view-serializable\nfalls in the class of NP-complete problems.\nThus, existence of an efficient algorithm is extremely unlikely.\n-However practical algorithms that just check some\nsufficient conditions for view serializability can still be\nused.\n\nSerializability in concurrency control",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 45,
        "page_end": 54,
        "char_len": 2082
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0008",
      "text": "Serializability in concurrency control\n\n-Every serial schedule is serializable, but not other way round\n-Serializable schedules give benefit of concurrent execution\nwithout giving up any correctness\n-Difficult to test for serializability in practice (even\nconflict-serializability)\nsystem load, time of transaction submission, and process priority\naffect ordering of operations\noften, we need to ensure serializability before the transactions complete\n-DBMSs enforce concurrency control protocols that ensure\nserializability\n\nRecoverability\n-For durability, we need to be able to recover\nschedules from transaction and system failures\n-Schedules with respect to recoverability, are\nclassified into:\nimpossible to recover\npossible to recover\nrecoverable\neasy to recover\ncascadeless, strict\n\nRecoverable Schedules\n\n-Need to address the effect of transaction\nfailures on concurrently running\ntransactions.\n-Recoverable schedule\nif a transaction Tj reads a data item previously\nwritten by a transaction Ti ,\nthen the commit operation of Ti appears before\nthe commit operation of Tj.\n\nRecoverable Schedules\n\n-The following schedule is not recoverable\n-If T8 should abort, T9 would have read (and possibly shown to\nthe user) an inconsistent database state.\n\nExamples\n-r1(X ), r2(X ), w1(X ), r1(Y ), w2(X ), c2, w1(Y ), c1\nrecoverable\n-r1(X ), w1(X ), r2(X ), r1(Y ), w2(X ), w1(Y ), c2, a1\nnon-recoverable\n-r1(X ), w1(X ), r2(X ), r1(Y ), w2(X ), w1(Y ), a1, c2\nstill non-recoverable\n-r1(X ), w1(X ), r2(X ), r1(Y ), w2(X ), w1(Y ), a1, a2\nrecoverable\n\nCascading Rollbacks\n-A single transaction failure leads to a series of transaction\nrollbacks.\nCan lead to the undoing of a significant amount of work\n-Consider the following schedule where none of the transactions\nhas yet committed (so the schedule is recoverable)\nIf T10 fails, T11 and T12 must also be rolled back.\n\nCascadeless schedules",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 54,
        "page_end": 60,
        "char_len": 1902
      }
    },
    {
      "chunk_id": "ADB_Lec07_c0009",
      "text": "Cascadeless schedules\n\n-Recoverable schedules may need cascading aborts\n(cascading rollbacks) to recover\nan (non-committed) transaction has to abort since it has\nread from another failed transaction\nExample\nr1(X), w1(X), r2(X), r1(Y), w2(X), w1(Y), a1, a2\n-This may affect many transactions, and rollbacks may\nbe expensive\n-Cascadeless schedule idea: no cascade rollbacks\n\nCascadeless schedules\n-Every transaction in the schedule reads only\nfrom committed transactions\n(i.e., for each r(X), X can be previously written by\nthe same transaction, written by another\ncommitted transaction or have not been written\nbefore)\n-Example:\nr1(X), w1(X), r1(Y), r2(Z), w1(Y), c1, w2(Z), r2(X),\nw2(X), c2\n\nStrict schedules\n-Cascadeless schedules may still have fairy complicated\nrecovery protocols, because we cannot just restore the values\nof all writes of an aborted transaction\nExample: w1(X ), w2(X ), a1\nThis may all affect many transactions (no cascades though)\n-Strict schedule: no transaction can read or write an item X\nuntil the previous write of X is committed or aborted\n-Examples:\nw1(X ), c1, w2(X )\nw1(X ), a1, w2(X )\n\nRecovery hierarchy\n\n-Every strict schedule is cascadeless\n-Every cascadeless is recoverable",
      "metadata": {
        "lecture_id": "ADB_Lec07",
        "source_file": "ADB_Lec07.pdf",
        "page_start": 60,
        "page_end": 63,
        "char_len": 1219
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\n\nLec 08\nConcurrency Control\n\nIntroduction\n\n-A database must provide a mechanism that will\nensure that all possible schedules are:\neither conflict or view serializable, and\nare recoverable and preferably cascadeless\n-A policy in which only one transaction can\nexecute at a time generates serial schedules, but\nprovides a poor degree of concurrency\n-Testing a schedule for serializability after it has\nexecuted is a little too late!\n\nConcurrency control problems\n\n-Dirty write (or lost update)\n\n-Dirty read (or temporary update)\n\n-Non-repeatable read\n\n-Incorrect summary (or phantom\nphenomena)\n\nDirty write (lost update)\n\n-Two transactions update (read and write) the same\nitem, second update starts before the first is complete\n(updates are interleaved)\n-Result: incorrect value\n\nDirty read (temporary update)\n\n-A transaction updates an item,\n-new value is used by another transaction,\n-the first transaction fails and its update is rolled back\n-Result: the second transaction relies on an incorrect value\n\nNon-repeatable read\n\n-One transaction reads the same item twice\n-another transaction changes its value in between\n-Result: the first transaction gets different values of\nthe same item\n\nIncorrect summary (Phantom phenomena)\n\n-One transaction calculates an aggregate summary\n-while other transactions update some involved items\n-Result: the aggregate is inconsistent\n\nWhy recovery is needed?\n\n-(if something goes wrong, effects roll back)\n-Should be either\nCommitted:\nevaluated in full and all effects permanently\nrecorded\nAborted:\nno effect on the database (everything done\nrolled back)\n\nThe System Log\n\n-System log keeps track of transaction operations\nSequential, append-only file\nNot affected by failure\n-Log buffer\nMain memory buffer\nWhen full, appended to end of log file on disk\n-Log file is backed up periodically\n-Undo and redo operations based on log possible\n\nCommit Point of a Transaction\n\n-Occurs when all operations that access the\ndatabase have completed successfully\n-Transaction writes a commit record into the\nlog\nIf system failure occurs, can search for\ntransactions with recorded start_transaction\nbut no commit record",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 1,
        "page_end": 10,
        "char_len": 2191
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0002",
      "text": "Commit Point of a Transaction\n\n-Occurs when all operations that access the\ndatabase have completed successfully\n-Transaction writes a commit record into the\nlog\nIf system failure occurs, can search for\ntransactions with recorded start_transaction\nbut no commit record\n\nConcurrency\nControl\n\nConcurrency Control (Cont.)\n-Goal\nDevelop concurrency control protocols that\nwill assure serializability.\n-Concurrency-control schemes tradeoff between\nthe amount of concurrency they allow and the\namount of overhead that they incur.\nSome schemes allow only conflict-serializable\nschedules to be generated, while others allow viewserializable.\n\nConcurrency Control vs. Serializability Tests\n\n-Concurrency-control protocols\nallow concurrent schedules,\nensure that the schedules are conflict/view serializable,\nand are recoverable and cascadeless.\n-Concurrency control protocols do not examine the\nprecedence graph as it is being created\nInstead, a protocol imposes a discipline that avoids nonserializable schedules.\n-Tests for serializability help us understand why a\nconcurrency control protocol is correct.\n\nWeak Levels of Consistency\n\n-Some applications are willing to live with weak\nlevels of consistency, allowing schedules that are\nnot serializable\nE.g., a read-only transaction that wants to get an\napproximate total balance of all accounts\nE.g., database statistics computed for query\noptimization can be approximate.\nSuch transactions need not be serializable with\nrespect to other transactions\n-Tradeoff accuracy for performance\n\nIsolation levels\n\n-It is not always necessary to ensure no problems of all types\nin some applications we can tolerate some problems\nfor the sake of more concurrency (i.e., efficiency)\n-Simple isolation level hierarchy of schedules (and transaction\nmanagement protocols):\nLevel 0: no dirty reads\nLevel 1: no dirty writes\nLevel 2: no dirty reads, no dirty writes\nLevel 3: (true isolation): no dirty reads, no dirty writes, no non-repeatable\nreads\n-A protocol that ensure recoverable and serializable schedules is Level 3\n\nImplementation of Isolation Levels",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 10,
        "page_end": 16,
        "char_len": 2099
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0003",
      "text": "Implementation of Isolation Levels\n\n-Locking\nLock on whole database vs lock on items\nHow long to hold lock?\nShared vs exclusive locks\n-Timestamps\nTransaction timestamp assigned e.g. when a transaction begins\nData items store two timestamps\n\nRead timestamp\n\nWrite timestamp\nTimestamps are used to detect out of order accesses\n-Multiple versions of each data item\nAllow transactions to read from a “snapshot” of the database\n\nLocking\n\nLocks\n\n-Lock: a variable associated with a data item\ndescribes status for operations that can be applied\nunique Lock(X) for each data item X\n-binary locks:\nLock(X) takes values locked or unlocked (or 1 and 0)\n-shared/exclusive locks:\nLock(X) takes values read-locked, write-locked, or unlocked\n-These variables are managed by concurrency control\nsubsystem\n\nBinary locks\n\n-for each item X, variable Lock(X) can have on\nof the two values:\n\nLock(X) is locked (or 1) means that X is locked\nby a transaction and cannot be accessed by\nany other transaction\nLock(X) is unlocked (or 0) means that X is\navailable and can be accessed when requested\n\n-schedules start with all unlocked\n\nBinary locks in transactions\n-Use two operations lock(X) and unlock(X) applying\nthese rules:\nall read(X) and write(X) must be between the two\nwell-formed: no unlocking without locking, etc.\n-These operations are implemented such that\nwhen lock(X) is issued\nif Lock(X) = 1, the transaction is forced to wait for Lock(X) = 0\n(in the schedule)\nif Lock(X) = 0, it is set to 1 and the transaction can proceed\nwhen unlock(X) is issued, Lock(X) is set to 0\n\nEx1: Transactions with binary locks\n\nT1 T2\nlock(X ); lock(X );\nread(X ); lock(Y );\nX := X − 1; read(X );\nwrite(X ); unlock(X );\nunlock(X ); X := X + 1;\nlock(Y ); lock(X );\nread(Y ); write(X );\nY := Y + 1; unlock(Y );\nwrite(Y ); unlock(X );\nunlock(Y );\n\nT1 : l1(X ), r1(X ), w1(X ), u1(X ), l1(Y ), r1(Y ), w1(Y ), u1(Y ): Ok!\nT2 : l2(X ), l2(Y ), r2(X ), u2(X ), l2(X ), w2(X ), u2(Y ), u2(Y ): Ok!\nNote: T2 locks and unlocks X twice and locks Y without a need\n\nEx2: Transactions with binary locks",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 16,
        "page_end": 22,
        "char_len": 2078
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0004",
      "text": "T1 : l1(X ), r1(X ), w1(X ), u1(X ), l1(Y ), r1(Y ), w1(Y ), u1(Y ): Ok!\nT2 : l2(X ), l2(Y ), r2(X ), u2(X ), l2(X ), w2(X ), u2(Y ), u2(Y ): Ok!\nNote: T2 locks and unlocks X twice and locks Y without a need\n\nEx2: Transactions with binary locks\n\nT1 T2\nlock(X );\nlock(X );\nread(X\nlock(Y ); );X := X − 1;\nread(X );\nwrite(X );\nunlock(X );\nunlock(X );\nX := X + 1;\nread(Y );\nlock(X );\nlock(Y );\nwrite(X );\nY := Y + 1;\nwrite(Y );\nunlock(X );\nunlock(Y );\n\nT1 : l1(X ), r1(X ), w1(X ), u1(X ), r1(Y ), l1(Y ), w1(Y ), u1(Y ): Not ok\nT2 : l2(X ), l2(Y ), r2(X ), u2(X ), l2(X ), w2(X ), u2(Y ): Not ok\nNote: T1 reads Y without locking, and T2 does not unlock Y\n\nEx: Schedules with binary locks\n\n-S1 : l1(X ), r1(X ), u1(X ), l2(X ), r2(X ), u2(X ), l1(X ), w1(X ), u1(X )\nOk!\n-S2 : l1(X ), r1(X ), l2(X ), u1(X ), r2(X ), u2(X ), l1(X ), w1(X ), u1(X )\nOk!\n\n-S3 : l1(X ), r1(X ), l2(X ), r2(X ), u1(X ), u2(X ), l1(X ), w1(X ), u1(X )\nNot ok\n\n(implementation of l2(X ) is ‘broken’: should not allow to continue T2\nwith r2(X ) before u1(X ))\n-Note: locks (i.e., the rules and implementations as above)\ndo not guarantee serializability by themselves;\n\nLock table\n\n-Concurrency control subsystem has a lock manager\nmodule that relies on a lock table:\nhas schema [Data_item_name, Locking_transaction]\nkeeps only locked items (others are unlocked)\n-Lock manager keeps track of and controls access\nto locks by checking the rules for transactions\nand enforcing allowed schedules (waiting, etc.)\n-Binary locking is too restrictive anyway\n\nShared/exclusive locks\n\n-Allow sharing for several reading transactions (but exclusive writes)\n-for each item X, Lock(X) is\nread-locked (or share-locked) means that X is read by a\ntransaction\nvariable #READS(X) keeps the number of reading transactions\nwrite-locked (or exclusive-locked) means that X is exclusively\nlocked for writing by some transaction and cannot be accessed by\nothers\nunlocked means that X is available for access (with locking)\n-schedules start with all Lock(X) = unlocked and #READS(X) = 0\n\nTransaction with shared/exclusive locks",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 21,
        "page_end": 26,
        "char_len": 2083
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0005",
      "text": "Transaction with shared/exclusive locks\n\n-use three operations, read_lock(X), write_lock(X) and unlock(X) with\nrules:\n each read(X) is after read_lock(X) or write_lock(X) (with no\nunlock(X) in between)\n each write(X) is after write_lock(X) (with no unlock(X) in\nbetween)\n each read_lock(X) and write_lock(X) has unlock(X) at some\npoint after\n no changing of lock type before unlocking;\n that is, no read_lock(X) after write_lock(X) without unlock(X)\nbetween\n\nImplementation of read_lock\n\nImplementation of write_lock\n\nImplementation of unlock\n\nExamples\n\n-S1 : rl1(X ), r1(X ), u1(X ), rl2(X ), r2(X ), u2(X ), wl1(X ), w1(X ), u1(X )\nOk!\n-S1′: rl1(X ), r1(X ), u1(X ), rl2(X ), r2(X ), u2(X ), rl1(X ), w1(X ), u1(X )\nNot ok (write with read lock)\n-S1′′: wl1(X ), r1(X ), u1(X ), rl2(X ), r2(X ), u2(X ), wl1(X ), w1(X ), u1(X )\nOk!\n-S3 : rl1(X ), r1(X ), rl2(X ), r2(X ), u1(X ), u2(X ), wl1(X ), w1(X ), u1(X )\nOk!\n-S3′: rl1(X ), r1(X ), wl2(X ), w2(X ), u1(X ), u2(X ), wl1(X ), w1(X ), u1(X )\nNot ok (implementation of wl2(X ) is ‘broken’: should be exclusive)\n-S3′′: rl1(X ), r1(X ), rl2(X ), r2(X ), u2(X ), wl1(X ), w1(X ), u1(X )\nNot ok (T1 is not following the rules: a second lock without unlocking)\n\nTwo-phase locking\n\nTwo-phase locking\n\n-A transaction (binary or shared/exclusive)\nfollows two-phase locking protocol if:\nall locking operations are before all unlocking\noperations\n-Two phases:\nlocking (first, expanding, or growing) phase\nunlocking (second or shrinking) phase\n\nTwo-phase locking: negative example\n\nTransactions T1 and T2 (left) do not follow the two-phase locking\nprotocol, so they allow for a non-serializable schedule (right)\n\nTwo-phase locking: positive example\n\nVersions T1′and T2′(left) follow the two-phase locking protocol, so no\nnon-serializable schedule complies the rules\n\n35\n\nserializability by two-phase locking\n\n-Two-phase locking guarantees serializability\n(i.e., every allowed schedule of transactions\nfollowing the rules of the two-phase locking\nprotocol is (conflict-serializable)\n-As the result, we can create a serializable\nschedule operation by operation, without\nknowing the transactions in full\n\nDrawback\n\n-we may end up in a deadlock",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 26,
        "page_end": 36,
        "char_len": 2198
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0006",
      "text": "Drawback\n\n-we may end up in a deadlock\n\nDeadlocks\n\nDeadlock problem\n\n-Two (or more) transactions may be stuck in a\ndeadlock, where each transaction is waiting for\nanother transaction, making a ‘waiting cycle’\n\n-This may happen for all types of locks,\n\nDeadlock avoidance: Overview\n\n-protocols that deal with deadlocks:\n(naive)\ndeadlock prevention\ndeadlock detection\ntimeouts\n-Most of protocols may roll back (i.e., abort and restart) some transactions\ncascadeless (i.e., recoverable) due to two-phase locking (all deadlocked\ntransactions in the growing phase, so no one have read what they have\nwritten)\nmay face and have to deal with starvation:\na transaction rolls back over and over again indefinitely\n\nnaive approaches\n\n-conservative two-phase locking (not practical)\n-OR\n-locking according to a predefined order of all items\nall items are ordered in advance (e.g., by address)\nall locking of a transaction is in the increasing order\nno deadlocks by construction\nnot practical due to\nlimited concurrency\nunreasonable restrictions on transactions\n\nDeadlock prevention protocols\n\n-Timestamp-based (wait-die and wound-wait)\n-Waiting-based (no-waiting and cautious-waiting)\nAll four avoid deadlocks, but often force transaction\naborts without a real deadlock\n-Deadlock prevention\nis not a popular approach in practice due to high\noverhead,\nbut may be useful when transactions are long and\nuse many items\n\nTimestamp-based deadlock prevention\n\n-Transaction timestamp:\na unique number TS (T) assigned to each\ntransaction T so that\nTS (T′) < TS (T) for each T′ started before (for the\nfirst time)\n\nWait-die rule\n-let transaction T try to lock an item that is\nalready locked by T′\nif TS (T) < TS (T′) (i.e., T is older than T′), then T\nwaits for T′ to release\notherwise (i.e., T is younger) T aborts (i.e., ‘dies’)\nand restarts with the same timestamp\n-The older proceeds (maybe, after waiting)\nand the younger restarts, so the wait-die rule\nguarantees no deadlocks, no starvation",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 36,
        "page_end": 43,
        "char_len": 1995
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0007",
      "text": "Wound-wait rule\n-let transaction T try to lock an item that is\nalready locked by T′\nif TS (T) < TS (T′) (i.e., T is older than T′), then T′\naborts (i.e., is ‘wounded’) and restarts with the\nsame timestamp\notherwise (i.e., T is younger) T waits for T′ to\nrelease\n-The older proceeds and the younger restarts\n(maybe, after waiting), so the wound-wait rule\nguarantees no deadlocks, no starvation\n\nExamples\n-When T1 tries to lock X that is already locked by T2\n\n-if T1 is older\nwait-die: T1 waits\n-b1, . . . , b2, . . . , l2(X ), . . . , l1(X ), [T1 waits] . . . , u2(X ), . . .\nwound-wait: T2 dies\n-b1, . . . , b2, . . . , l2(X ), . . . , l1(X ), a2, . . .\n\n-if T1 is younger\nwait-die: T1 dies\n-b2, . . . , b1, . . . , l2(X ), . . . , l1(X ), a1, . . .\nwound-wait: T1 waits\n-b2, . . . , b1, . . . , l2(X ), . . . , l1(X ), [T1 waits] . . . , u2(X ), . . .\n\nWaiting-based deadlock prevention\n-(No timestamps in these protocols)\n-Let transaction T try to lock an item that is already locked\nby T′\nno-waiting rule:\nT aborts and restarts\ncautious-waiting rule:\nif T′ is not waiting for anyone, then T waits (regularly checked)\notherwise, T aborts and restarts\n-Any of these rules guarantees no deadlocks (no cyclic\nwaiting is possible)\n-Special care should be taken to avoid starvation.\n-Easier, but even more unnecessary aborts\n\nDeadlock detection\n\n-Deadlock happens when there is a cycle of transactions waiting\nfor each other\n-We can detect a cycle and break the cycle by aborting one\ntransaction (the victim)\n-Can be done by maintaining the wait-for graph:\n nodes: active transactions\n (directed) edges: transactions waiting for transactions\n-A cycle in the wait-for graph means a deadlock\n-May be resource-consuming to maintain\n-Victim selection should be done cautiously to avoid starvation\n\nTimeouts\n\n-Timeout-based deadlock avoidance:\nfix a waiting timeout period in advance\na transaction waits for the timeout period\n(at most) after this abort and restart\n-Simple and easy to maintain\n-No guarantees\n(unknown overhead, possible starvation)\n\nTimestamps",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 44,
        "page_end": 49,
        "char_len": 2069
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0008",
      "text": "Timeouts\n\n-Timeout-based deadlock avoidance:\nfix a waiting timeout period in advance\na transaction waits for the timeout period\n(at most) after this abort and restart\n-Simple and easy to maintain\n-No guarantees\n(unknown overhead, possible starvation)\n\nTimestamps\n\nTimestamp-based concurrency control\n-We can use timestamps for concurrency\ncontrol without locking:\nRun transactions until something goes\nwrong (i.e., a conflict is detected)\nRoll back one of the conflict transactions\nbased on their timestamps\nroll back is abort and restart with a new\ntimestamp\n\nTimestamp-based concurrency control\n\n-Timestamp-based concurrency control protocols:\nall ensure serializability\ntransactions arranged by their timestamps\nmay not guarantee recoverability\nmay not guarantee no starvation\n-Several timestamp-based protocols\nbasic: ensures serializability, but not recoverability\nstrict: ensures serializability and strictness (so recoverability)\nbasic/strict with Thomas write rule: less roll-backs\n\nItem timestamp variables\n\n-Besides the timestamp of a transaction , the concurrency\ncontrol subsystem maintains two variables for each item X :\nread timestamp ReadTS(X):\nthe timestamp of the youngest transaction that has read X\nwrite timestamp WriteTS(X):\nthe timestamp of the youngest transaction that has written X\n-Maintained in appropriate timestamp tables\n(similar to the lock table in lock-based protocols)\n-(All initialised by 0)\n\nBasic timestamp-ordering protocol\n\n-If there is a conflict that violates the\norder (of the serial schedule imposed by\nthe timestamps),\n-then roll back the transaction of the\nsecond operation of the conflict\n\nRead-write conflict 1\n\n-a transaction may try to read data item X too\nlate\n\n-Should abort and restart T1\n\nRead-write conflict 2\n\n-a transaction may try to write data item X too\nlate\n\n-Should abort and restart T1\n\nWrite-write conflict\n\n-a transaction may try to write data item X too\nlate\n\n-Should abort and restart T1\n\nBasic timestamp-ordering Rules",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 48,
        "page_end": 57,
        "char_len": 2004
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0009",
      "text": "-Should abort and restart T1\n\nRead-write conflict 2\n\n-a transaction may try to write data item X too\nlate\n\n-Should abort and restart T1\n\nWrite-write conflict\n\n-a transaction may try to write data item X too\nlate\n\n-Should abort and restart T1\n\nBasic timestamp-ordering Rules\n\n-Rule 1. When transaction T issues read(X ):\nif WriteTS (X) > TS (T) (i.e., someone younger written X ),\nthen roll-back T (i.e., abort and restart T with a new timestamp)\notherwise, execute the rest of read(X ) and set ReadTS (X) to\nmax(ReadTS (X), TS (T))\n-Rule 2. When transaction T issues write(X ):\nif ReadTS (X) > TS (T) or WriteTS (X) > TS (T) (i.e., someone younger\naccessed X ),\nthen roll-back T (i.e., abort and restart T with a new timestamp)\notherwise, execute the rest of write(X ) and set WriteTS (X ) to TS (T )\n\nBasic timestamp-ordering properties\n\n-Ensures serializable schedules, but they\nmay be not recoverable,\n(ensured by strict version)\nmay have cascading roll-backs,\n(avoided by strict version)\nmay have starving transactions\n\nStrict timestamp-ordering protocol\n\n-Idea\nIn the rules the execution is postponed\n(using e.g., locking) until the pervious write\nis committed (so ensuring strict schedules)\n\nRule 1\n\n-When transaction T issues read(X ):\n-if WriteTS (X ) > TS (T ) then roll-back T\n-otherwise\nwait until the transaction T′such that\n-WriteTS (X ) = TS (T′) is committed (if there is\nsuch)\nexecute the rest of read(X ) and\n-set ReadTS (X ) to max(ReadTS (X ), TS (T ))\n\nRule 2\n\n-When transaction T issues write(X ):\n-if ReadTS (X ) > TS (T ) or WriteTS (X ) > TS (T ) then\nroll-back T\n-otherwise\nwait until the transaction T′such that\n-WriteTS (X ) = TS (T′) is committed (if there is\nsuch)\nexecute the rest of write(X ) and set WriteTS (X ) to\nTS (T )\n\nThomas writing rule\n\n-Improvement (less roll-backs) for both basic and strict protocols in case of writewrite conflict\n\n-Rule 1. When transaction T issues read(X ): . . . (same)\n\n-Rule 2. When transaction T issues write(X ):\n\nif ReadTS (X ) > TS (T ) then roll-back T\n\nif WriteTS (X ) > TS (T ) ignore the operation and proceed\n\notherwise . . . (same, i.e., possibly wait, execute, update)",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 54,
        "page_end": 62,
        "char_len": 2165
      }
    },
    {
      "chunk_id": "ADB_Lec08_c0010",
      "text": "-Rule 1. When transaction T issues read(X ): . . . (same)\n\n-Rule 2. When transaction T issues write(X ):\n\nif ReadTS (X ) > TS (T ) then roll-back T\n\nif WriteTS (X ) > TS (T ) ignore the operation and proceed\n\notherwise . . . (same, i.e., possibly wait, execute, update)\n\n-Same guarantees as the versions without the improvement\n\nMulti-versioning\n\nMultiversioning concurrency control\n-Multi-versioning idea:\nseveral versions of the (value of the) same item are\nkept by the system\ntransactions are the same as usual (in particular, they\nrequest reads/writes for items, not versions)\nthe versions of items are managed by the\nconcurrency control subsystem\n-Pros: increase concurrency\n-Cons: more storage is needed to maintain\nmultiple versions\n\nConcurrency\nwith\nMultiple granularity\n\nData item granularity\n\n-Granularity: the size of data items\nfield value of a record (fine)\nrecord\ndisk block\ntable file\nwhole database (coarse)\n-The tradeoff:\nmore coarse allows for lower degree of concurrency\nmore fine leads to more overhead (lock management, etc.)\n-Best item size depends on transaction type:\nif a transaction accesses few records, a record as item is good\nif a transaction accesses many records, a block as item is good\n-It may be nice to have different granularity for different transactions",
      "metadata": {
        "lecture_id": "ADB_Lec08",
        "source_file": "ADB_Lec08.pdf",
        "page_start": 62,
        "page_end": 66,
        "char_len": 1307
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0001",
      "text": "CSAI 302\n\nAdvanced Database\n\nSystems\n\nLec 09\nDatabase recovery techniques\n\nACID principles: Reminder\n\nAtomicity -- TransactionEnsured by theperformedrecoveryinsubsystemits entirety or not at all\n\nConsistency - The database should always remain consistent - Ensured by the transaction program\n\nIsolation -- TransactionEnsured by theshouldconcurrencynot interferecontrolwith subsystemothers\n\nDurability -- ChangesEnsured byof committedthe recoverytransactionssubsystemmust persist\n\nRecoverable Transactions\n\n-To ensure durability in ACID, a schedule may be\n\nRecoverable Cascadeless Strict\n\n- T does not - T does not read - T is cascadeless\ncommit before from and does not\neach T’ from uncommitted overwrite any\nwhich T read transactions values written by\nhave committed uncommitted\ntransactions\n\nRecovery\n\nLecture content\n\nRecovery concepts Recovery protocols\n\n-Intro -Shadowing\n-Cache -Deferred-update\n-Log -Immediate-update\n-Checkpoint -ARIES\n\nRecovery intro\n\nComputer failure Transaction\n(system crash) failure\n- hardware, software, - division by zero,-Recovery: network errors constraint violation,\netc.\nrestores database to\nLocal transaction Concurrency most recent errors control enforced\n- no data found, - serializability consistent state programmed violation, deadlock\nbefore failure exception, etc. break, etc.\n-Types of failures ➔ Disk crash Physical problems - persistent errors with - power cut, fire,\ndisk reads or writes catastrophe, etc.\n\nGroups of failures\nDisk is (significantly) Disk is not damageddamaged (catastrophe, (everything else) disk crash, etc.)\n\nrecover consistency by undoing\nand redoing some operations\nfollowing a recovery protocol\n\nrestore the whole database from use the database on the disk and\nback-up storage. a log of operations\n\nneed to take into account the\ncache in the main memory\n\nCache\n\nCache\n\n-DBMSs use (dedicated) cache:\n\nCollection of buffers (pages in main memory) blocks on disk\n\n-Some information (data itself, indexes, logs, etc.)\n\nfrom the disk are kept in main-memory cache for quick access\n\n-to perform an operation with data, the cache is first checked:\n\nif the needed block is in cache, it is used",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 1,
        "page_end": 9,
        "char_len": 2159
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0002",
      "text": "Collection of buffers (pages in main memory) blocks on disk\n\n-Some information (data itself, indexes, logs, etc.)\n\nfrom the disk are kept in main-memory cache for quick access\n\n-to perform an operation with data, the cache is first checked:\n\nif the needed block is in cache, it is used\n\notherwise, an existing cache buffer is replaced by the needed block\nfrom disk using buffer replacement policies\n\nif the operation is write, then the buffer becomes dirty\n\nFlushing approaches\n\n-Each buffer in the cache has a dirty bit (flag) attached:\nwas the buffer modified or not\n-Flushing approaches:\nin-place flushing:\nnew (cache) version replaces the old (disk) one if dirty bit is 1\n- used in most cases\nshadowing:\nnew version is written in another place and the old version\nis kept\n- overhead\n\nBuffer\nreplacement\npolicies\n\nmethods to decide which cache\nbuffers should be flushed\n\nLRU\n\n-Basic standard replacement policy is LRU:\n\nthe least recently used buffer is flushed\n(replaced)\n\nnot specialized for DBs and not effective, because\ndifferent domains (types of information) may\nbenefit from different treatment\n\nDomain separation technique\n\n-Each domain (data, indexes, logs, or relations) has\nits own dedicated cache\n-LRU (least recently used) policy is used in each\nindividually\nbetter results than common LRU\n-Can be improved in several ways:\nHot set, clock sweep\n\nClock sweep with hot set\n\n-Hot set:\nblocks that should always be in cache until a certain point\ndecided externally (e.g., smaller relation in nested-loop join)\n-Clock sweep over other buffers (for each domain separately):\neach (non-hot) cache buffer has an integer count value\ncache buffers are checked in a round-robin ‘cycle’ with some\nregularity, maintaining individual counts\nif a buffer has started to be used since the last time visited, increment its\ncount\notherwise, decrement its count\nif we need to replace a buffer with a new block from the disk, the buffer with\nthe least count is replaced\n\nSystem Log\n\nSystem log",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 9,
        "page_end": 16,
        "char_len": 2011
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0003",
      "text": "System Log\n\nSystem log\n\n-System log keeps track of executed operations of\ntransactions:\nsequential, append-only file (i.e., a table of entries)\nreading and writing the log are not affected by failure\nexcept disk or catastrophic failures\n(i.e., logging is done by stand-alone system operations\nwhich is not a part of any transaction operations)\nbacked up periodically to guard against these failures\n\nSystem log: Properties\n\n-System log keeps track of relevant executed operations of\ntransactions:\nin particular, we do not need to log computations\nlog keeps the effects of data writes and maybe reads (not write\nand read operations)\n-log also keeps the beginnings and the ends (commit or\nabort) of transactions\n(e.g., entries of the form [commit, T])\n-log keeps other information\ne.g., about checkpoints and dirty buffer table\n\nSystem log: Read and Write entries\n\n-System log table can have read and write entries of several\ntypes:\nUNDO- (OLD-) WRITE entry\nkeeps the before-image (i.e., old value) of a written item used to\nundo the write if the transaction is aborted\nREDO- (NEW-) WRITE entry\nkeeps the after-image (i.e., new value) of a written item used for\nredo the write if the transaction is committed,\n- but the effect is not on the disk (only in a dirty cache buffer)\nUNDO&REDO-WRITE entry\nkeeps both above\n\nLog cache buffer\n-System log cache buffer:\nmain memory buffer (cache)\nkeeps the last part of the log\nwhen full, appended (i.e., flushed) to the log file on\nthe disk\n-Log cache buffer may also be flushed to ensure\nwrite-ahead logging:\n\nlog information about a transaction operation is\nflushed before the effect of the operation is\nflushed itself\n\nWrite-Ahead Logging (WAL)\n\nWrite-ahead logging: More detail\n-A recovery protocol follows write-ahead logging if\n\nbefore-image (the old version) of each item on the\ndisk cannot be overwritten (by the flushing buffer)\nby its after-image (the new version) until all UNDO\n(OLD) log entries have been flushed to disk\na transaction cannot commit\n-until all UNDO (OLD) and REDO (NEW) log\nentries for that transaction have been flushed\nto disk\n-(Most of) recovery protocols are write-ahead logging\n\nWAL steps",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 15,
        "page_end": 22,
        "char_len": 2181
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0004",
      "text": "WAL steps\n\n-Log First\nDBMS writes the intended changes as WAL records to an append-only log file before\nmodifying any database files.\n-Flush to Disk\nBefore DBMS marks the transaction as committed, it flushes the WAL to disk.\n\nThis guarantees durability: even if the system crashes before applying the changes to the\nmain database files, the WAL ensures the transaction can be recovered.\n-Apply Changes Later\nDBMS applies the changes to the data files asynchronously during a process\ncalled checkpointing.\nAt each checkpoint, the dirty pages in memory (modified data) are written back to the\nstorage to reduce reliance on WAL during recovery.\n-If a crash happens between the WAL flush and the data file update, DBMS can use\nthe WAL to \"redo\" the operations that occurred after the last checkpoint.\n\nCheckpoints\n\nCheckpoint idea\n\n-a recovery operation that makes it possible to safely return to a\nconsistent state, in case of failure\n(i.e., all committed transactions at a point of the failure are logically\nredone, all aborted undone) by\nflushing all relevant buffers and\nlogging currently active transactions\n-Ensures that everything committed so far is on the disk and does\nnot need any action in case of a failure\n-Repeated regularly (e.g., every 5 minutes) by recovery subsystem\nmore frequent-less recovery work\nless frequent-more overhead\n\ncheckpoint operation\n\nSuspend execution of all transactions\n\nFlush all main non-pinned dirty memory buffers\n\nWrite checkpoint entry [checkpoint, list of active transactions] to log and\nflush the log to the disk\n\nResume executing transactions\n\nRecovery protocols\n\nRecovery & Concurrency control\n\n-Recovery subsystem ensures atomicity and durability:\nrestores database to most recent consistent state before failure\none or several transactions abort and all their changes need to be\nrolled back\n-Needs coordination with concurrency control:\nAssume a system crash event that aborts all transactions as a\nmodel type failure\nAssume strict two-phase locking as a model concurrency\ncontrol protocol\n\nShadowing\nrecovery protocol",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 22,
        "page_end": 28,
        "char_len": 2078
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0005",
      "text": "Shadowing\nrecovery protocol\n\nShadowing recovery protocol: Idea\n-flush each modified buffer immediately to a\nnew place on disk, the old (shadow) version is\nkept on disk (and never modified)\nif a transaction commits, the current\nversion is used\nif a transaction aborts, the shadow version\nis used\n-A directory: a table with pointers to\ndisk blocks\n\nShadowing recovery protocol\n\n-Shadowing (shadow paging) recovery protocol\n(for a single transaction):\nat the beginning of a transaction,\na current directory with pointers to all blocks is created,\nand it is copied to shadow directory\nafter each write,\naffected buffers are immediately flushed to a new disk\nblock, and the current directory is updated\nif a transaction commits, the current version is taken\nif a transaction aborts, the shadow version is restored\n\nExample of shadowing\n\nShadowing properties\n\n-no log required in strict (or single-user)\nenvironment\nso, sometimes called NO-UNDO/NO-REDO\n-no checkpoints either\nessentially, a copy of cache on disk\n-results in a lot of overhead and randomordered blocks on disk\n-requires complicated garbage collection\n\nDeferred-update\nprotocol\n\nTransaction picture\n\nDeferred update: Idea\n-Deferred update recovery protocol idea:\nno-steal approach:\npostpone all flushes of (dirty buffers) to disk\nuntil the transaction commits\nREDO- (NEW-) log entries are needed:\nto recover committed transactions after a system\ncrash\nUNDO- (OLD-) log entries are not needed:\nno changes of aborted transactions are on the disk\nthus, NO-UNDO/REDO\n\nProperties and Assumptions\n\n-Deferred update recovery protocol properties:\n\neffective only for short transactions with few changes\ncache size is an issue with longer transactions\n-Simplifying assumptions, some already mentioned\n(can be relaxed but rules and procedures may need adjustments):\n\nstrict two-phase locking with blocks as items\nsystem crash as a fail (i.e., all active transactions\nabort)\nall blocks changed by a transaction fit into cache",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 28,
        "page_end": 36,
        "char_len": 1996
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0006",
      "text": "strict two-phase locking with blocks as items\nsystem crash as a fail (i.e., all active transactions\nabort)\nall blocks changed by a transaction fit into cache\n\nDeferred update rules\n-use log with REDO entries and regular\ncheckpoints\n-all buffers changed by a transaction are pinned\nuntil commit\n(i.e., no-steal approach)\n-transaction does not commit until all its REDOtype log entries are recorded in log and log buffer\nis flushed to disk\n(i.e., the relevant REDO-part of the write-ahead\napproach)\n\nDeferred update: Recovery procedure\n\n-(evaluated by the recovery subsystem after a system crash):\n-find the active (i.e., latest) checkpoint in the log on the disk\nall effects of committed transactions are on the disk by the\ncheckpoint\n-using the checkpoint’s active transactions and the log after\nthe checkpoint, construct the set of all committed\ntransactions between the checkpoint and the crash\ntheir commit log entries are on the disk\n-redo all operations of the committed transactions from the\nset\nall their log entries are on the disk\n\nDeferred update protocol: Example\n\n-T1: already committed and flushed, no action required\n-T2: may have non-flushed buffers,\nbut the log (including begin_transaction) is flushed, so all changes\ncan be recovered from the log (considered from from the checkpoint)\n-T3: may have non-flushed buffers,\nbut the log is flushed and the checkpoint remembers that it is active, so\nall the changes can be recovered from the log\n-T4, T5: aborted and no buffers are flushed, no action required\n\nImmediate-update\nrecovery\n\nImmediate update\n-Immediate update recovery protocol idea:\npossible steal approach:\nallow to flush of dirty buffers\nboth before and after the transaction commits\nREDO- (NEW-) log entries log entries are needed:\nto redo committed transactions after a system crash\nUNDO- (OLD-) log entries log entries are needed: to undo\naborted transactions\nthus, UNDO/REDO (generalises deferred-update)\n-Immediate update recovery protocol properties:\nmore general than deferred-update\nmore difficult",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 36,
        "page_end": 41,
        "char_len": 2054
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0007",
      "text": "assumptions and rules\n-Simplifying assumptions, less than above\nstrict two-phase locking with blocks as items (not essential\nhere)\nsystem crash as a fail (i.e., all active transactions abort)\n-Immediate update protocol rules (just write-ahead logging\nin full):\nuse log with REDO/UNDO entries and regular checkpoints\na dirty buffer does not flush until all relevant log entries are\nrecorded in log and log buffer is flushed to disk\n(i.e., the first part of the write-ahead approach)\ntransaction does not commit until all its log entries are\nrecorded in log and log buffer is flushed to disk\n(i.e., the second part of the write-ahead approach)\n\nImmediate update: Recovery procedure\n\n-(evaluated by the recovery subsystem in case of a crash):\n-find the active (i.e., latest) checkpoint in the log on the disk\nall effects of committed transactions are on the disk by the checkpoint\n-construct the sets of all committed and non-committed (i.e.,\naborted) transactions between the checkpoint and the crash\ntheir commit log entries are on the disk\n-redo all operations of the committed transactions\nall their log entries are on the disk\n-undo all known operations of the non-committed transactions\nsome of their log entries may not be on the disk, but their\ncorresponding changes were not flushed\n\nImmediate update protocol: Example\n\n-T1, T2, T3: same as before\n-T4, T5: aborted, so should be undone;\nsome changes may be already flushed by t2 and\nsome may not, but the log entries of all flushed\nchanges are also flushed\n\nARIES recovery\nprotocol\n\nARIES: Overview\n\n-ARIES protocol main properties:\nAlgorithm for Recovery and Isolation Exploiting Semantics\nan improved version of immediate-update protocol\n(possible steals, REDO/UNDO log entries, write-ahead logging)\nused in practice\nagain, we concentrate on strict schedules\n-ARIES update recovery protocol improvement ideas:\ndifferent checkpoints (main improvement): instead of flushing\neverything,\nremember the table of currently dirty buffers\nother improvements (e.g., logging of recovery): not discussed\nhere\n\nARIES Checkpoints",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 42,
        "page_end": 47,
        "char_len": 2093
      }
    },
    {
      "chunk_id": "ADB_Lec09_c0008",
      "text": "ARIES Checkpoints\n\n-During execution, ARIES maintains (in main memory):\nTransaction table of currently active and committed\ntransactions\n(ID, pointer to the most recent relevant log entry, status)\nDirty buffer table of currently dirty buffers in the cache (ID,\npointer to the earliest update log entry)\n-ARIES checkpoint (fuzzy in general)\nthe begin_checkpoint entry identifies the start point for the\nrecovery, and the state of the transaction table and dirty buffer\ntable to store\nthe end_checkpoint entry has the tables (at the begin-point\nstate) appended in the log\nthe log must be flushed at the end of checkpoint\n\nARIES recovery protocol: Example\n\n(a) The log\n(b) Possible transaction and dirty block table at the begin\ncheckpoint (appended to end_checkpoint)\n(c) Possible tables at the moment of the end of\nthe log (observe that 7 in dirty buffer table\nimplies that C was flushed between Lsn 5 and 7)\n\nARIES recovery main steps\n\n-ARIES recovery procedure (evaluates at the crash event):\n-Analysis step:\nidentifies the earliest update log entry of a dirty buffer\nat the checkpoint (and collects other relevant information)\n-Redo step:\ngo through the log from the identified point to the end and redo the\nnecessary operations (i.e., the writes of dirty blocks in the table before the\nbegin checkpoint and all writes after it)\n-Undo step:\ngo through the log from the end to the beginning of all uncommitted (by the\nlog end) transactions and undo necessary operations\n(i.e., all writes that are not overwritten by a redo)\n-Evaluated undo’s and redo’s are also logged,\n-so that a crash during update does not cause a need to re-evaluate\n\nARIES recovery protocol: Example\n\nLet the log above be in on the disk after a crash. ARIES steps:\n\n1. Analysis step: identifies the earliest log entry: Lsn 1\n2. Redo step: redo the writes with Lsn 1, 6, 7 (no 2)\nUndo step (only T3): undo update with Lsn 6 3.\n\n(Lsn 6 is redone and undone;\n\nthink of advantages and disadvanatages of such strategy)",
      "metadata": {
        "lecture_id": "ADB_Lec09",
        "source_file": "ADB_Lec09.pdf",
        "page_start": 47,
        "page_end": 50,
        "char_len": 1997
      }
    }
  ]
}